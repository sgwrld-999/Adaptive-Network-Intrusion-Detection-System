{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 12:10:24,689 - INFO - Loading dataset...\n",
      "2025-03-22 12:10:24,793 - INFO - Loaded dataset with 150000 samples and 13 features\n",
      "2025-03-22 12:10:24,795 - INFO - Preprocessing data...\n",
      "2025-03-22 12:10:24,849 - INFO - Analyzing features...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import PowerTransformer, MaxAbsScaler\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve, roc_auc_score\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# Setup logging and suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('Layer1_VAE')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "class Layer1AutoencoderVAE:\n",
    "    def __init__(self, input_dim, latent_dim=6, learning_rate=1e-4, layer_sizes=None):\n",
    "        \"\"\"Initialize the VAE model with configurable architecture\"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes or [64, 32]  # Default larger network\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vae = None\n",
    "        self.kde = None\n",
    "        self.threshold = None\n",
    "        self.fallback_threshold = None  # Added for robustness\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.build_model()\n",
    "        \n",
    "    def sampling(self, args):\n",
    "        \"\"\"Reparameterization trick for VAE\"\"\"\n",
    "        z_mean, z_log_var = args\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build the VAE with customizable architecture\"\"\"\n",
    "        # Encoder\n",
    "        encoder_inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = encoder_inputs\n",
    "        \n",
    "        for size in self.layer_sizes:\n",
    "            x = layers.Dense(size, activation=\"relu\", \n",
    "                             kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # VAE latent space\n",
    "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
    "        z = layers.Lambda(self.sampling, output_shape=(self.latent_dim,), name=\"z\")([z_mean, z_log_var])\n",
    "        \n",
    "        # Instantiate encoder\n",
    "        self.encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "        \n",
    "        # Decoder\n",
    "        latent_inputs = layers.Input(shape=(self.latent_dim,))\n",
    "        x = latent_inputs\n",
    "        \n",
    "        for size in reversed(self.layer_sizes):\n",
    "            x = layers.Dense(size, activation=\"relu\", \n",
    "                             kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        decoder_outputs = layers.Dense(self.input_dim, activation=\"sigmoid\")(x)\n",
    "        \n",
    "        # Instantiate decoder\n",
    "        self.decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "        \n",
    "        # Instantiate VAE model\n",
    "        outputs = self.decoder(self.encoder(encoder_inputs)[2])\n",
    "        self.vae = keras.Model(encoder_inputs, outputs, name=\"vae\")\n",
    "        \n",
    "        # Define VAE loss with beta parameter for KL term weighting\n",
    "        beta = 1.0  # Can be adjusted to control KL weight\n",
    "        reconstruction_loss = keras.losses.MeanSquaredError()(encoder_inputs, outputs)\n",
    "        reconstruction_loss *= self.input_dim\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        vae_loss = K.mean(reconstruction_loss + beta * kl_loss)\n",
    "        \n",
    "        self.vae.add_loss(vae_loss)\n",
    "        self.vae.compile(optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "        # Custom metrics\n",
    "        self.vae.metrics_names.append(\"reconstruction_loss\")\n",
    "        self.vae.metrics_names.append(\"kl_loss\")\n",
    "        self.vae.metrics.append(self.reconstruction_loss_tracker)\n",
    "        self.vae.metrics.append(self.kl_loss_tracker)\n",
    "        \n",
    "    def train(self, X_train, X_val, epochs=100, batch_size=32):\n",
    "        \"\"\"Train the VAE model with early stopping and LR reduction\"\"\"\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7\n",
    "        )\n",
    "        \n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "            log_dir=f'./logs/vae_{time.strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "        \n",
    "        class VAECallback(keras.callbacks.Callback):\n",
    "            def __init__(self, parent):\n",
    "                super(VAECallback, self).__init__()\n",
    "                self.parent = parent\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                # Track separate loss components\n",
    "                x_val_reconstructed = self.model.predict(X_val)\n",
    "                reconstruction_loss = np.mean(np.square(X_val - x_val_reconstructed))\n",
    "                z_mean, z_log_var, _ = self.parent.encoder.predict(X_val)\n",
    "                kl_loss = -0.5 * np.mean(np.sum(1 + z_log_var - np.square(z_mean) - np.exp(z_log_var), axis=1))\n",
    "                \n",
    "                # Update metrics\n",
    "                self.parent.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "                self.parent.kl_loss_tracker.update_state(kl_loss)\n",
    "                \n",
    "                logs['reconstruction_loss'] = reconstruction_loss\n",
    "                logs['kl_loss'] = kl_loss\n",
    "        \n",
    "        vae_callback = VAECallback(self)\n",
    "        \n",
    "        logger.info(\"Starting VAE training...\")\n",
    "        history = self.vae.fit(\n",
    "            X_train, X_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val),\n",
    "            callbacks=[early_stopping, reduce_lr, tensorboard_callback, vae_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        logger.info(\"VAE training completed.\")\n",
    "        self._set_dynamic_threshold(X_train)\n",
    "        return history\n",
    "    \n",
    "    def _set_dynamic_threshold(self, X_data):\n",
    "        \"\"\"Set robust thresholds using multiple methods\"\"\"\n",
    "        # Compute reconstruction errors\n",
    "        _, _, z = self.encoder.predict(X_data)\n",
    "        reconstructed = self.decoder.predict(z)\n",
    "        mse = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        \n",
    "        # Set percentile-based fallback threshold (90th-99th percentile)\n",
    "        self.fallback_threshold = np.percentile(mse, 97.5)\n",
    "        \n",
    "        # Optimize KDE bandwidth using grid search\n",
    "        param_grid = {'bandwidth': np.logspace(-2, 1, 10)}\n",
    "        grid_search = GridSearchCV(KernelDensity(kernel='gaussian'), param_grid, cv=5)\n",
    "        grid_search.fit(mse.reshape(-1, 1))\n",
    "        \n",
    "        # Use the optimized bandwidth\n",
    "        best_bandwidth = grid_search.best_params_['bandwidth']\n",
    "        self.kde = KernelDensity(kernel='gaussian', bandwidth=best_bandwidth).fit(mse.reshape(-1, 1))\n",
    "        \n",
    "        log_dens = self.kde.score_samples(mse.reshape(-1, 1))\n",
    "        scores = -log_dens\n",
    "        \n",
    "        # Robust elbow finding\n",
    "        try:\n",
    "            sorted_scores = np.sort(scores)\n",
    "            n_samples = len(sorted_scores)\n",
    "            \n",
    "            if n_samples < 10:  # Not enough samples for reliable elbow detection\n",
    "                self.threshold = self.fallback_threshold\n",
    "            else:\n",
    "                indices = np.arange(n_samples)\n",
    "                \n",
    "                # Use window averaging for more stable angle calculation\n",
    "                window_size = max(3, int(n_samples * 0.02))\n",
    "                angles = []\n",
    "                \n",
    "                for i in range(window_size, n_samples - window_size):\n",
    "                    # Use windowed points for more stability\n",
    "                    p1 = np.array([indices[i-window_size]/n_samples, sorted_scores[i-window_size]])\n",
    "                    p2 = np.array([indices[i]/n_samples, sorted_scores[i]])\n",
    "                    p3 = np.array([indices[i+window_size]/n_samples, sorted_scores[i+window_size]])\n",
    "                    \n",
    "                    # Compute vectors\n",
    "                    v1 = p2 - p1\n",
    "                    v2 = p3 - p2\n",
    "                    \n",
    "                    # Normalize vectors\n",
    "                    v1_norm = np.linalg.norm(v1)\n",
    "                    v2_norm = np.linalg.norm(v2)\n",
    "                    \n",
    "                    if v1_norm > 0 and v2_norm > 0:\n",
    "                        v1 = v1 / v1_norm\n",
    "                        v2 = v2 / v2_norm\n",
    "                        \n",
    "                        # Compute angle using dot product\n",
    "                        dot_product = np.dot(v1, v2)\n",
    "                        angle = np.arccos(np.clip(dot_product, -1.0, 1.0))\n",
    "                        angles.append(angle)\n",
    "                    else:\n",
    "                        angles.append(0)\n",
    "                \n",
    "                if len(angles) > 0 and max(angles) > 0.1:  # Check if we have meaningful angles\n",
    "                    elbow_idx = np.argmax(angles) + window_size\n",
    "                    adaptive_threshold = sorted_scores[elbow_idx]\n",
    "                    \n",
    "                    # Blend with percentile-based threshold for robustness\n",
    "                    self.threshold = 0.7 * adaptive_threshold + 0.3 * self.fallback_threshold\n",
    "                else:\n",
    "                    self.threshold = self.fallback_threshold\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in threshold calculation: {e}. Using fallback threshold.\")\n",
    "            self.threshold = self.fallback_threshold\n",
    "        \n",
    "        logger.info(f\"Dynamic threshold: {self.threshold:.6f} (fallback: {self.fallback_threshold:.6f})\")\n",
    "        \n",
    "        # Visualize the threshold\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(scores, bins=50, alpha=0.6, color='blue')\n",
    "        plt.axvline(x=self.threshold, color='red', linestyle='--', label=f'Threshold: {self.threshold:.6f}')\n",
    "        plt.axvline(x=self.fallback_threshold, color='green', linestyle=':', label=f'Fallback: {self.fallback_threshold:.6f}')\n",
    "        plt.title('Anomaly Score Distribution and Thresholds')\n",
    "        plt.xlabel('Anomaly Score (-log density)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.savefig('plots/anomaly_threshold.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def detect_anomalies(self, X_data):\n",
    "        \"\"\"Detect anomalies in the data\"\"\"\n",
    "        if self.kde is None or self.threshold is None:\n",
    "            raise ValueError(\"Model hasn't been trained yet. Call train() first.\")\n",
    "        \n",
    "        # Get latent representations and reconstructions\n",
    "        _, _, z = self.encoder.predict(X_data)\n",
    "        reconstructed = self.decoder.predict(z)\n",
    "        \n",
    "        # Compute reconstruction error (MSE)\n",
    "        mse = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        \n",
    "        # Compute log density and anomaly scores\n",
    "        log_dens = self.kde.score_samples(mse.reshape(-1, 1))\n",
    "        anomaly_scores = -log_dens\n",
    "        \n",
    "        # Identify anomalies\n",
    "        anomaly_indices = np.where(anomaly_scores > self.threshold)[0]\n",
    "        anomalies = X_data[anomaly_indices]\n",
    "        \n",
    "        # Compute confidence\n",
    "        max_score = np.max(anomaly_scores)\n",
    "        min_score = np.min(anomaly_scores)\n",
    "        confidence = (anomaly_scores - min_score) / (max_score - min_score) if max_score > min_score else np.zeros_like(anomaly_scores)\n",
    "        \n",
    "        return anomalies, anomaly_indices, anomaly_scores, confidence\n",
    "    \n",
    "    def get_encoded_features(self, X_data):\n",
    "        \"\"\"Extract features from the encoder's latent space\"\"\"\n",
    "        _, _, z = self.encoder.predict(X_data)\n",
    "        return z\n",
    "    \n",
    "    def save_model(self, base_path='models'):\n",
    "        \"\"\"Save the model and artifacts\"\"\"\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "        # Save full VAE model\n",
    "        self.vae.save(f'{base_path}/layer1_model_{timestamp}.h5')\n",
    "        self.encoder.save(f'{base_path}/layer1_encoder_{timestamp}.h5')\n",
    "        self.decoder.save(f'{base_path}/layer1_decoder_{timestamp}.h5')\n",
    "        \n",
    "        # Create symlinks to latest models\n",
    "        for model_type in ['model', 'encoder', 'decoder']:\n",
    "            latest_link = f'{base_path}/layer1_{model_type}.h5'\n",
    "            if os.path.exists(latest_link):\n",
    "                os.remove(latest_link)\n",
    "            os.symlink(f'layer1_{model_type}_{timestamp}.h5', latest_link)\n",
    "        \n",
    "        # Save threshold and metadata\n",
    "        model_config = {\n",
    "            'input_dim': self.input_dim,\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'layer_sizes': self.layer_sizes,\n",
    "            'threshold': float(self.threshold),\n",
    "            'fallback_threshold': float(self.fallback_threshold),\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        with open(f'{base_path}/layer1_config_{timestamp}.json', 'w') as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "        \n",
    "        # Save the KDE model\n",
    "        joblib.dump(self.kde, f'{base_path}/layer1_kde_{timestamp}.pkl')\n",
    "        joblib.dump(self.kde, f'{base_path}/layer1_kde.pkl')\n",
    "        \n",
    "        return timestamp\n",
    "\n",
    "def analyze_features(data, save_dir='plots'):\n",
    "    \"\"\"Analyze feature distributions and create visualizations\"\"\"\n",
    "    # Create feature distribution plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    features = data.columns\n",
    "    num_features = len(features)\n",
    "    rows = int(np.ceil(num_features / 3))\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(rows, 3, i+1)\n",
    "        sns.histplot(data[feature], kde=True)\n",
    "        plt.title(f'{feature} Distribution')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{save_dir}/feature_distributions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = data.corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/feature_correlations.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate feature variances\n",
    "    variances = data.var().sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=variances.index, y=variances.values)\n",
    "    plt.title('Feature Variance')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/feature_variances.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Identify low variance features\n",
    "    low_var_threshold = 0.01\n",
    "    low_var_features = variances[variances < low_var_threshold].index.tolist()\n",
    "    \n",
    "    return {\n",
    "        'variances': variances,\n",
    "        'low_var_features': low_var_features,\n",
    "        'correlation_matrix': corr_matrix\n",
    "    }\n",
    "\n",
    "def find_optimal_latent_dim(X_train, X_val, input_dim, min_dim=3, max_dim=15):\n",
    "    \"\"\"Use k-fold cross-validation to find optimal latent dimension\"\"\"\n",
    "    logger.info(\"Finding optimal latent dimension...\")\n",
    "    \n",
    "    # Define candidate dimensions to test\n",
    "    if input_dim <= 10:\n",
    "        candidate_dims = list(range(min_dim, min(max_dim, input_dim) + 1))\n",
    "    else:\n",
    "        # Test a range with more focus on smaller dimensions\n",
    "        candidate_dims = list(range(min_dim, min(8, input_dim // 2) + 1))\n",
    "        candidate_dims += [min(d, input_dim // 2) for d in [10, 12, 15]]\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    candidate_dims = sorted(list(set(candidate_dims)))\n",
    "    \n",
    "    # Combine train and validation for k-fold\n",
    "    X_combined = np.vstack([X_train, X_val])\n",
    "    \n",
    "    results = []\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    for latent_dim in candidate_dims:\n",
    "        fold_losses = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_combined):\n",
    "            X_fold_train, X_fold_val = X_combined[train_idx], X_combined[val_idx]\n",
    "            \n",
    "            # Train a smaller model for quick evaluation\n",
    "            model = Layer1AutoencoderVAE(\n",
    "                input_dim=input_dim, \n",
    "                latent_dim=latent_dim,\n",
    "                layer_sizes=[32, 16],  # Smaller network for quick evaluation\n",
    "                learning_rate=1e-3\n",
    "            )\n",
    "            \n",
    "            # Train with fewer epochs for efficiency\n",
    "            history = model.train(\n",
    "                X_fold_train, X_fold_val, \n",
    "                epochs=30, \n",
    "                batch_size=64\n",
    "            )\n",
    "            \n",
    "            # Get the best validation loss\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "            fold_losses.append(best_val_loss)\n",
    "        \n",
    "        # Average loss across folds\n",
    "        avg_loss = np.mean(fold_losses)\n",
    "        logger.info(f\"Latent dim {latent_dim}: avg validation loss = {avg_loss:.6f}\")\n",
    "        results.append((latent_dim, avg_loss))\n",
    "    \n",
    "    # Find dimension with lowest loss\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    best_dim = results[0][0]\n",
    "    \n",
    "    # Visualize dimension search\n",
    "    dims, losses = zip(*results)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(dims, losses, 'o-')\n",
    "    plt.axvline(x=best_dim, color='red', linestyle='--')\n",
    "    plt.title(f'Latent Dimension Optimization (Best: {best_dim})')\n",
    "    plt.xlabel('Latent Dimension')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('plots/latent_dim_optimization.png')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Optimal latent dimension: {best_dim}\")\n",
    "    return best_dim\n",
    "\n",
    "def generate_evaluation_plots(model, X_normal, X_test=None, y_test=None, save_dir='plots'):\n",
    "    \"\"\"Generate evaluation plots for the model\"\"\"\n",
    "    # Reconstruction error distribution for normal data\n",
    "    _, _, z_normal = model.encoder.predict(X_normal)\n",
    "    X_normal_reconstructed = model.decoder.predict(z_normal)\n",
    "    normal_mse = np.mean(np.square(X_normal - X_normal_reconstructed), axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(normal_mse, kde=True, color='blue', label='Normal')\n",
    "    \n",
    "    # If test data with labels is available\n",
    "    if X_test is not None and y_test is not None:\n",
    "        _, _, z_test = model.encoder.predict(X_test)\n",
    "        X_test_reconstructed = model.decoder.predict(z_test)\n",
    "        test_mse = np.mean(np.square(X_test - X_test_reconstructed), axis=1)\n",
    "        \n",
    "        # Separate normal and anomaly in test set\n",
    "        if np.sum(y_test) > 0:  # If we have anomalies\n",
    "            anomaly_mse = test_mse[y_test == 1]\n",
    "            sns.histplot(anomaly_mse, kde=True, color='red', alpha=0.6, label='Anomaly')\n",
    "    \n",
    "    plt.axvline(x=model.threshold, color='green', linestyle='--', \n",
    "                label=f'Threshold: {model.threshold:.6f}')\n",
    "    plt.title('Reconstruction Error Distribution')\n",
    "    plt.xlabel('Mean Squared Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{save_dir}/reconstruction_error_dist.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # If test data with labels is available, generate ROC and PR curves\n",
    "    if X_test is not None and y_test is not None and np.sum(y_test) > 0:\n",
    "        # Get anomaly scores for test data\n",
    "        _, _, anomaly_scores, _ = model.detect_anomalies(X_test)\n",
    "        \n",
    "        # ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, anomaly_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(f'{save_dir}/roc_curve.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Precision-Recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, anomaly_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(f'{save_dir}/pr_curve.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Latent space visualization (2D projection if latent_dim > 2)\n",
    "        _, _, z = model.encoder.predict(X_test)\n",
    "        \n",
    "        if model.latent_dim >= 2:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            if np.sum(y_test == 0) > 0:\n",
    "                plt.scatter(z[y_test == 0, 0], z[y_test == 0, 1], c='blue', alpha=0.5, label='Normal')\n",
    "            if np.sum(y_test == 1) > 0:\n",
    "                plt.scatter(z[y_test == 1, 0], z[y_test == 1, 1], c='red', alpha=0.5, label='Anomaly')\n",
    "            plt.title('Latent Space Visualization (First 2 Dimensions)')\n",
    "            plt.xlabel('Latent Dim 1')\n",
    "            plt.ylabel('Latent Dim 2')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'{save_dir}/latent_space_2d.png')\n",
    "            plt.close()\n",
    "\n",
    "def generate_report(model, history, feature_analysis, data_info, timestamp, save_dir='reports'):\n",
    "    \"\"\"Generate a summary report of the model training and evaluation\"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"LAYER 1 AUTOENCODER-VAE MODEL SUMMARY REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"Model timestamp: {timestamp}\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Data information\n",
    "    report.append(\"DATA INFORMATION\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"Total samples: {data_info['total_samples']}\")\n",
    "    report.append(f\"Training samples: {data_info['train_samples']}\")\n",
    "    report.append(f\"Validation samples: {data_info['val_samples']}\")\n",
    "    report.append(f\"Input features: {data_info['n_features']}\")\n",
    "    report.append(f\"Feature names: {', '.join(data_info['feature_names'])}\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Feature analysis\n",
    "    report.append(\"FEATURE ANALYSIS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(\"Top 5 features by variance:\")\n",
    "    top_var_features = feature_analysis['variances'].head(5)\n",
    "    for feature, var in top_var_features.items():\n",
    "        report.append(f\"  - {feature}: {var:.6f}\")\n",
    "    \n",
    "    report.append(\"\\nLow variance features:\")\n",
    "    for feature in feature_analysis['low_var_features']:\n",
    "        report.append(f\"  - {feature}: {feature_analysis['variances'][feature]:.6f}\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Model architecture\n",
    "    report.append(\"MODEL ARCHITECTURE\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"Input dimension: {model.input_dim}\")\n",
    "    report.append(f\"Latent dimension: {model.latent_dim}\")\n",
    "    report.append(f\"Hidden layer sizes: {model.layer_sizes}\")\n",
    "    report.append(f\"Learning rate: {model.learning_rate}\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Training performance\n",
    "    report.append(\"TRAINING PERFORMANCE\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "    report.append(f\"Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "    report.append(f\"Final reconstruction loss: {history.history['reconstruction_loss'][-1]:.6f}\")\n",
    "    report.append(f\"Final KL loss: {history.history['kl_loss'][-1]:.6f}\")\n",
    "    report.append(f\"Training epochs: {len(history.history['loss'])}\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Anomaly detection\n",
    "    report.append(\"ANOMALY DETECTION\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"Dynamic threshold: {model.threshold:.6f}\")\n",
    "    report.append(f\"Fallback threshold: {model.fallback_threshold:.6f}\")\n",
    "    report.append(f\"Threshold method: Kernel Density Estimation with robust elbow finding\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Saved artifacts\n",
    "    report.append(\"SAVED ARTIFACTS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"Full model: models/layer1_model_{timestamp}.h5\")\n",
    "    report.append(f\"Encoder model: models/layer1_encoder_{timestamp}.h5\")\n",
    "    report.append(f\"Decoder model: models/layer1_decoder_{timestamp}.h5\")\n",
    "    report.append(f\"KDE model: models/layer1_kde_{timestamp}.pkl\")\n",
    "    report.append(f\"Configuration: models/layer1_config_{timestamp}.json\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    # Write report to file\n",
    "    with open(f\"{save_dir}/layer1_report_{timestamp}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(report))\n",
    "    \n",
    "    # Create symlink to latest report\n",
    "    latest_report = f\"{save_dir}/layer1_report_latest.txt\"\n",
    "    if os.path.exists(latest_report):\n",
    "        os.remove(latest_report)\n",
    "    os.symlink(f\"layer1_report_{timestamp}.txt\", latest_report)\n",
    "    \n",
    "    return report\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    logger.info(\"Loading dataset...\")\n",
    "    dataset_path = \"layer1_training_data.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        logger.info(f\"Loaded dataset with {len(df)} samples and {len(df.columns)} features\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        logger.info(\"Using sample data instead...\")\n",
    "        sample_data = \"\"\"tcp.dstport_category,mbtcp.trans_id,tcp.ack,mqtt.ver,tcp.connection.synack,mbtcp.len,mqtt.conflags,mqtt.conack.flags,tcp.connection.rst,http.tls_port,tcp.srcport,tcp.connection.fin,mqtt.hdrflags\n",
    "0.5,0.0,1.2316824563829088e-09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7660298471022675,0.0,0.8\n",
    "0.5,0.0,1.4780189476594907e-09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9434339426862391,0.0,0.0\n",
    "1.0,0.0,3.1824211308021596e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7660298471022675,0.0,0.8\n",
    "0.5,0.0,1.2316824563829088e-09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7660298471022675,0.0,0.8\"\"\"\n",
    "        df = pd.read_csv(pd.StringIO(sample_data))\n",
    "        \n",
    "    # Preprocess data\n",
    "    logger.info(\"Preprocessing data...\")\n",
    "    \n",
    "    # Drop columns with zero variance\n",
    "    var = df.var()\n",
    "    zero_var_cols = var[var == 0].index.tolist()\n",
    "    if zero_var_cols:\n",
    "        logger.info(f\"Dropping {len(zero_var_cols)} zero-variance columns: {zero_var_cols}\")\n",
    "        df = df.drop(columns=zero_var_cols)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    if df.isna().any().any():\n",
    "        logger.info(\"Filling NaN values with 0\")\n",
    "        df = df.fillna(0)\n",
    "    \n",
    "    # Scale features to [0, 1] range for VAE\n",
    "    scaler = MaxAbsScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # Save scaler for future use\n",
    "    joblib.dump(scaler, 'models/layer1_scaler.pkl')\n",
    "    \n",
    "    # Split data for training\n",
    "    X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Analyze features\n",
    "    logger.info(\"Analyzing features...\")\n",
    "    feature_analysis = analyze_features(df)\n",
    "    \n",
    "    # Find optimal latent dimension\n",
    "    input_dim = X_train.shape[1]\n",
    "    best_latent_dim = find_optimal_latent_dim(X_train, X_val, input_dim)\n",
    "    \n",
    "    # Define model architecture based on input size\n",
    "    if input_dim <= 10:\n",
    "        layer_sizes = [32, 16]\n",
    "    elif input_dim <= 20:\n",
    "        layer_sizes = [64, 32, 16]\n",
    "    else:\n",
    "        layer_sizes = [128, 64, 32]\n",
    "    \n",
    "    # Create and train the model\n",
    "    logger.info(f\"Building VAE model with latent dim {best_latent_dim}...\")\n",
    "    model = Layer1AutoencoderVAE(\n",
    "        input_dim=input_dim,\n",
    "        latent_dim=best_latent_dim,\n",
    "        layer_sizes=layer_sizes\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Training VAE model...\")\n",
    "    history = model.train(X_train, X_val, epochs=200, batch_size=64)\n",
    "    \n",
    "    # Generate evaluation plots\n",
    "    logger.info(\"Generating evaluation plots...\")\n",
    "    generate_evaluation_plots(model, X_train)\n",
    "    \n",
    "    # Save the model\n",
    "    logger.info(\"Saving model...\")\n",
    "    timestamp = model.save_model()\n",
    "    \n",
    "    # Prepare data info for report\n",
    "    data_info = {\n",
    "        'total_samples': len(df),\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'n_features': input_dim,\n",
    "        'feature_names': df.columns.tolist()\n",
    "    }\n",
    "    \n",
    "    # Generate and save report\n",
    "    logger.info(\"Generating report...\")\n",
    "    report = generate_report(model, history, feature_analysis, data_info, timestamp)\n",
    "    \n",
    "    # Extract encoded features for Layer 2\n",
    "    logger.info(\"Extracting encoded features for Layer 2...\")\n",
    "    encoded_features = model.get_encoded_features(X_scaled)\n",
    "    \n",
    "    # Save encoded features for Layer 2\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded_features,\n",
    "        columns=[f'vae_feature_{i}' for i in range(best_latent_dim)]\n",
    "    )\n",
    "    encoded_df.to_csv('layer1_encoded_features.csv', index=False)\n",
    "    \n",
    "    # Perform anomaly detection on training data (to demonstrate)\n",
    "    logger.info(\"Running anomaly detection on training data...\")\n",
    "    anomalies, anomaly_indices, anomaly_scores, confidence = model.detect_anomalies(X_scaled)\n",
    "    \n",
    "    # Save anomaly detection results\n",
    "    anomaly_results = pd.DataFrame({\n",
    "        'anomaly_score': anomaly_scores,\n",
    "        'confidence': confidence,\n",
    "        'is_anomaly': anomaly_scores > model.threshold\n",
    "    })\n",
    "    anomaly_results.to_csv('layer1_anomaly_results.csv', index=False)\n",
    "    \n",
    "    # Print summary of anomalies found\n",
    "    anomaly_count = len(anomaly_indices)\n",
    "    logger.info(f\"Found {anomaly_count} potential anomalies ({(anomaly_count/len(X_scaled))*100:.2f}%)\")\n",
    "    \n",
    "    # Print execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    logger.info(f\"Total execution time: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    logger.info(\"Layer 1 VAE processing completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
