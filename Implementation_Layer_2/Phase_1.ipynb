{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Forward to Layer 2 Implementation  \n",
    "\n",
    "The implementation of Layer 1 (Anomaly Detection & Feature Extraction) is now complete. However, for further modifications and validation, we require more dataset variations or additional traffic patterns. Constructing these datasets will take some time.  \n",
    "\n",
    "In the meantime, I'm now proceeding with the **implementation of Layer 2 (Attack Classification & Adaptive Learning)**.  \n",
    "\n",
    "### Key Next Steps:\n",
    "- **Dataset Construction:** Since Layer 2 relies on anomalous samples detected by Layer 1, we will integrate the Layer 1 code with the Layer 2 pipeline.  \n",
    "- **Feature Extraction:** Extracting CNN-enhanced features from Layer 1 to improve classification performance.  \n",
    "- **Attack Classification Model:** Implementing a CNN-BiLSTM model with Knowledge Distillation for efficient and scalable attack classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing dataset\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Load and preprocess dataset for ANIDS.\n",
    "    - Applies robust scaling\n",
    "    - Removes outliers using IQR\n",
    "    - Splits data into training & validation sets\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    \n",
    "    # Outlier removal using IQR\n",
    "    Q1, Q3 = X.quantile(0.25), X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    X = X[~((X < (Q1 - 3 * IQR)) | (X > (Q3 + 3 * IQR))).any(axis=1)]\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "\n",
    "    return train_test_split(X_scaled, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Load dataset\n",
    "X_train, X_val = preprocess_data(\"/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.2371 - val_loss: 0.2149\n",
      "Epoch 2/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.2108 - val_loss: 0.1954\n",
      "Epoch 3/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1945 - val_loss: 0.1790\n",
      "Epoch 4/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.1813 - val_loss: 0.1649\n",
      "Epoch 5/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1703 - val_loss: 0.1525\n",
      "Epoch 6/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1555 - val_loss: 0.1412\n",
      "Epoch 7/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.1443 - val_loss: 0.1316\n",
      "Epoch 8/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 0.1368 - val_loss: 0.1233\n",
      "Epoch 9/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1273 - val_loss: 0.1163\n",
      "Epoch 10/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1196 - val_loss: 0.1108\n",
      "Epoch 11/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1146 - val_loss: 0.1067\n",
      "Epoch 12/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1153 - val_loss: 0.1038\n",
      "Epoch 13/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.1068 - val_loss: 0.1020\n",
      "Epoch 14/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1080 - val_loss: 0.1011\n",
      "Epoch 15/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1088 - val_loss: 0.1006\n",
      "Epoch 16/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1066 - val_loss: 0.1004\n",
      "Epoch 17/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1050 - val_loss: 0.0999\n",
      "Epoch 18/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1095 - val_loss: 0.0993\n",
      "Epoch 19/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1039 - val_loss: 0.0988\n",
      "Epoch 20/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.1069 - val_loss: 0.0982\n",
      "Epoch 21/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.1082 - val_loss: 0.0977\n",
      "Epoch 22/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1040 - val_loss: 0.0969\n",
      "Epoch 23/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1062 - val_loss: 0.0965\n",
      "Epoch 24/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1049 - val_loss: 0.0962\n",
      "Epoch 25/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1000 - val_loss: 0.0962\n",
      "Epoch 26/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1032 - val_loss: 0.0955\n",
      "Epoch 27/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.1020 - val_loss: 0.0949\n",
      "Epoch 28/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1027 - val_loss: 0.0945\n",
      "Epoch 29/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1074 - val_loss: 0.0945\n",
      "Epoch 30/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1021 - val_loss: 0.0936\n",
      "Epoch 31/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1026 - val_loss: 0.0936\n",
      "Epoch 32/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1016 - val_loss: 0.0935\n",
      "Epoch 33/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1018 - val_loss: 0.0936\n",
      "Epoch 34/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1005 - val_loss: 0.0923\n",
      "Epoch 35/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1017 - val_loss: 0.0925\n",
      "Epoch 36/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1030 - val_loss: 0.0928\n",
      "Epoch 37/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1012 - val_loss: 0.0920\n",
      "Epoch 38/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0999 - val_loss: 0.0917\n",
      "Epoch 39/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1021 - val_loss: 0.0910\n",
      "Epoch 40/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1008 - val_loss: 0.0911\n",
      "Epoch 41/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1020 - val_loss: 0.0905\n",
      "Epoch 42/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1024 - val_loss: 0.0895\n",
      "Epoch 43/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1007 - val_loss: 0.0895\n",
      "Epoch 44/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.1002 - val_loss: 0.0900\n",
      "Epoch 45/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0985 - val_loss: 0.0889\n",
      "Epoch 46/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0980 - val_loss: 0.0886\n",
      "Epoch 47/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0995 - val_loss: 0.0881\n",
      "Epoch 48/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1007 - val_loss: 0.0881\n",
      "Epoch 49/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1012 - val_loss: 0.0877\n",
      "Epoch 50/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0970 - val_loss: 0.0870\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define Layer 1 of the Adaptive NIDS\n",
    "class AdaptiveNIDSLayer1:\n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = self._build_autoencoder()\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.Reshape((-1, 1))(x)\n",
    "\n",
    "        # Feature extraction via Residual CNN\n",
    "        x = layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "        x = layers.Conv1D(32, 3, activation='relu', padding='same')(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        # Latent Representation\n",
    "        x = layers.Dense(64, activation='mish', kernel_regularizer=regularizers.l1(0.0005))(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        encoded = layers.Dense(self.latent_dim, activation='linear')(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(self.latent_dim * 2, return_sequences=True, recurrent_dropout=0.25)(x)\n",
    "        decoded = layers.TimeDistributed(layers.Dense(1, activation='linear'))(x)\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        autoencoder.compile(optimizer=keras.optimizers.Adam(1e-4), loss='mse')\n",
    "        return autoencoder\n",
    "\n",
    "    def train(self, X_train, X_val, epochs=50):\n",
    "        self.model.fit(X_train, X_train, epochs=epochs, batch_size=64, validation_data=(X_val, X_val))\n",
    "\n",
    "    def detect_anomalies(self, X_data, threshold=0.02):\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        return X_data[errors > threshold], np.where(errors > threshold)[0]\n",
    "\n",
    "# Train Layer 1\n",
    "layer1 = AdaptiveNIDSLayer1(input_dim=X_train.shape[1])\n",
    "layer1.train(X_train, X_val)\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies, anomaly_indices = layer1.detect_anomalies(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: global_average_pooling1d. Existing layers are: ['input_layer_3', 'batch_normalization_3', 'reshape_1', 'conv1d_4', 'conv1d_5', 'global_average_pooling1d_1', 'dense_7', 'dropout_3', 'dense_8', 'repeat_vector_1', 'lstm_1', 'time_distributed_1', 'flatten_1'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     feature_extractor = keras.Model(inputs=model.input, outputs=model.get_layer(\u001b[33m\"\u001b[39m\u001b[33mglobal_average_pooling1d\u001b[39m\u001b[33m\"\u001b[39m).output)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_extractor.predict(X_anomalies)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_layer2 = \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manomalies\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(model, X_anomalies)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_features\u001b[39m(model, X_anomalies):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     feature_extractor = keras.Model(inputs=model.input, outputs=\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mglobal_average_pooling1d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.output)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_extractor.predict(X_anomalies)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/models/model.py:210\u001b[39m, in \u001b[36mModel.get_layer\u001b[39m\u001b[34m(self, name, index)\u001b[39m\n\u001b[32m    208\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m layer.name == name:\n\u001b[32m    209\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m layer\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    211\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo such layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Existing layers are: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    212\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(layer.name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlayer\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m     )\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    215\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mProvide either a layer name or layer index at `get_layer`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: No such layer: global_average_pooling1d. Existing layers are: ['input_layer_3', 'batch_normalization_3', 'reshape_1', 'conv1d_4', 'conv1d_5', 'global_average_pooling1d_1', 'dense_7', 'dropout_3', 'dense_8', 'repeat_vector_1', 'lstm_1', 'time_distributed_1', 'flatten_1']."
     ]
    }
   ],
   "source": [
    "# Extracting features from Layer 1 for Layer 2\n",
    "def extract_features(model, X_anomalies):\n",
    "    feature_extractor = keras.Model(inputs=model.input, outputs=model.get_layer(\"global_average_pooling1d\").output)\n",
    "    return feature_extractor.predict(X_anomalies)\n",
    "\n",
    "X_layer2 = extract_features(layer1.model, anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataset for Layer 2\n",
    "def attach_attack_labels(original_df, anomaly_indices):\n",
    "    labeled_anomalies = original_df.iloc[anomaly_indices]\n",
    "    return labeled_anomalies.drop(columns=['Attack_label']), labeled_anomalies['Attack_label']\n",
    "\n",
    "# Load original dataset for labels\n",
    "original_df = pd.read_csv(\"/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv\")\n",
    "X_layer2, y_layer2 = attach_attack_labels(original_df, anomaly_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5838 - loss: 1.0874 - val_accuracy: 0.8537 - val_loss: 0.3313\n",
      "Epoch 2/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9504 - loss: 0.1208 - val_accuracy: 0.9099 - val_loss: 0.2789\n",
      "Epoch 3/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9737 - loss: 0.0649 - val_accuracy: 0.9381 - val_loss: 0.1425\n",
      "Epoch 4/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9821 - loss: 0.0488 - val_accuracy: 0.9587 - val_loss: 0.0755\n",
      "Epoch 5/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9924 - loss: 0.0290 - val_accuracy: 0.9625 - val_loss: 0.0920\n",
      "Epoch 6/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9856 - loss: 0.0318 - val_accuracy: 0.9831 - val_loss: 0.0437\n",
      "Epoch 7/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9794 - loss: 0.0589 - val_accuracy: 0.9550 - val_loss: 0.1045\n",
      "Epoch 8/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9879 - loss: 0.0382 - val_accuracy: 0.9700 - val_loss: 0.0629\n",
      "Epoch 9/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9907 - loss: 0.0233 - val_accuracy: 0.9906 - val_loss: 0.0273\n",
      "Epoch 10/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9906 - loss: 0.0234 - val_accuracy: 0.9869 - val_loss: 0.0303\n",
      "Epoch 11/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9963 - loss: 0.0159 - val_accuracy: 0.9831 - val_loss: 0.0391\n",
      "Epoch 12/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9941 - loss: 0.0137 - val_accuracy: 0.9925 - val_loss: 0.0224\n",
      "Epoch 13/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9937 - loss: 0.0214 - val_accuracy: 0.9906 - val_loss: 0.0198\n",
      "Epoch 14/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9930 - loss: 0.0162 - val_accuracy: 0.9925 - val_loss: 0.0215\n",
      "Epoch 15/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9924 - loss: 0.0192 - val_accuracy: 0.9925 - val_loss: 0.0254\n",
      "Epoch 16/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9971 - loss: 0.0093 - val_accuracy: 0.9887 - val_loss: 0.0234\n",
      "Epoch 17/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9867 - loss: 0.0260 - val_accuracy: 0.9850 - val_loss: 0.0359\n",
      "Epoch 18/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9926 - loss: 0.0186 - val_accuracy: 0.9887 - val_loss: 0.0327\n",
      "Epoch 19/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9946 - loss: 0.0156 - val_accuracy: 0.9700 - val_loss: 0.0897\n",
      "Epoch 20/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9925 - loss: 0.0216 - val_accuracy: 0.9906 - val_loss: 0.0273\n",
      "Epoch 21/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9979 - loss: 0.0074 - val_accuracy: 0.9925 - val_loss: 0.0249\n",
      "Epoch 22/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9969 - loss: 0.0086 - val_accuracy: 0.9887 - val_loss: 0.0280\n",
      "Epoch 23/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9900 - loss: 0.0290 - val_accuracy: 0.9644 - val_loss: 0.0879\n",
      "Epoch 24/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9904 - loss: 0.0196 - val_accuracy: 0.9850 - val_loss: 0.0322\n",
      "Epoch 25/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9960 - loss: 0.0099 - val_accuracy: 0.9906 - val_loss: 0.0298\n",
      "Epoch 26/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9976 - loss: 0.0078 - val_accuracy: 0.9906 - val_loss: 0.0248\n",
      "Epoch 27/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9970 - loss: 0.0069 - val_accuracy: 0.9869 - val_loss: 0.0334\n",
      "Epoch 28/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9953 - loss: 0.0098 - val_accuracy: 0.9812 - val_loss: 0.0401\n",
      "Epoch 29/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9952 - loss: 0.0112 - val_accuracy: 0.9887 - val_loss: 0.0370\n",
      "Epoch 30/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9925 - loss: 0.0180 - val_accuracy: 0.9887 - val_loss: 0.0322\n",
      "Epoch 31/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9979 - loss: 0.0063 - val_accuracy: 0.9887 - val_loss: 0.0322\n",
      "Epoch 32/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 0.9812 - val_loss: 0.0456\n",
      "Epoch 33/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9974 - loss: 0.0055 - val_accuracy: 0.9831 - val_loss: 0.0514\n",
      "Epoch 34/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9955 - loss: 0.0085 - val_accuracy: 0.9831 - val_loss: 0.0552\n",
      "Epoch 35/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9962 - loss: 0.0097 - val_accuracy: 0.9906 - val_loss: 0.0332\n",
      "Epoch 36/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9985 - loss: 0.0070 - val_accuracy: 0.9887 - val_loss: 0.0444\n",
      "Epoch 37/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9985 - loss: 0.0052 - val_accuracy: 0.9812 - val_loss: 0.0690\n",
      "Epoch 38/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9933 - loss: 0.0147 - val_accuracy: 0.9869 - val_loss: 0.0492\n",
      "Epoch 39/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9994 - loss: 0.0046 - val_accuracy: 0.9887 - val_loss: 0.0387\n",
      "Epoch 40/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 0.9850 - val_loss: 0.0469\n",
      "Epoch 41/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9925 - val_loss: 0.0440\n",
      "Epoch 42/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9906 - val_loss: 0.0394\n",
      "Epoch 43/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.9869 - val_loss: 0.0369\n",
      "Epoch 44/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9850 - val_loss: 0.0381\n",
      "Epoch 45/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.6020e-04 - val_accuracy: 0.9869 - val_loss: 0.0417\n",
      "Epoch 46/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.0250e-04 - val_accuracy: 0.9887 - val_loss: 0.0440\n",
      "Epoch 47/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 8.6614e-04 - val_accuracy: 0.9831 - val_loss: 0.0503\n",
      "Epoch 48/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0030 - val_accuracy: 0.9831 - val_loss: 0.0585\n",
      "Epoch 49/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9954 - loss: 0.0193 - val_accuracy: 0.9794 - val_loss: 0.0503\n",
      "Epoch 50/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0049 - val_accuracy: 0.9869 - val_loss: 0.0454\n"
     ]
    }
   ],
   "source": [
    "# Define Layer 2 of the Adaptive NIDS\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        \"\"\"\n",
    "        Initializes Layer 2 for attack classification using CNN-BiLSTM.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features per time step.\n",
    "            num_classes (int): Number of attack classes.\n",
    "            seq_length (int): Number of time steps in sequence.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the CNN-BiLSTM classification model.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim))\n",
    "        \n",
    "        # CNN Feature Extraction\n",
    "        x = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # BiLSTM for Temporal Sequence Learning\n",
    "        x = layers.Bidirectional(layers.GRU(48, return_sequences=False))(x)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Output Layer (Softmax for Classification)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "        # Compile Model\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(1e-3), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=50, batch_size=64):\n",
    "        \"\"\"\n",
    "        Trains the Layer 2 model.\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Input sequences.\n",
    "            y_train (np.array): Attack labels.\n",
    "            epochs (int): Number of training epochs.\n",
    "            batch_size (int): Batch size.\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "\n",
    "def create_sequences(data, labels, seq_length=10):\n",
    "    \"\"\"\n",
    "    Converts feature data into time-series sequences for Layer 2.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): 2D feature matrix.\n",
    "        labels (pd.Series or np.array): Corresponding labels.\n",
    "        seq_length (int): Number of time steps per sequence.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sequential data, adjusted labels)\n",
    "    \"\"\"\n",
    "    sequences, seq_labels = [], []\n",
    "    \n",
    "    # Convert labels to NumPy array to avoid indexing issues\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        seq_labels.append(labels[i + seq_length - 1])  # ✅ Now works correctly\n",
    "\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "# Generate sequences for Layer 2\n",
    "seq_length = 10\n",
    "X_layer2_reshaped, y_layer2_adjusted = create_sequences(X_layer2, y_layer2, seq_length=seq_length)\n",
    "\n",
    "# Train Layer 2\n",
    "layer2 = AdaptiveNIDSLayer2(input_dim=X_layer2_reshaped.shape[2], num_classes=5, seq_length=seq_length)\n",
    "layer2.train(X_layer2_reshaped, y_layer2_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def knowledge_distillation(teacher_model, student_model, X_train, y_train, temperature=3.0, alpha=0.5, epochs=50):\n",
    "    \"\"\"\n",
    "    Implements knowledge distillation by training the student model with soft targets from the teacher.\n",
    "\n",
    "    Args:\n",
    "        teacher_model: Pre-trained teacher model.\n",
    "        student_model: Student model to be trained.\n",
    "        X_train (np.array): Training features.\n",
    "        y_train (np.array): Training labels (should be a NumPy array).\n",
    "        temperature (float): Softmax temperature for distillation.\n",
    "        alpha (float): Weight balance between hard loss and soft loss.\n",
    "        epochs (int): Number of training epochs.\n",
    "    \"\"\"\n",
    "    # ✅ Ensure y_train is a NumPy array\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # ✅ Ensure labels are integers (for sparse categorical crossentropy)\n",
    "    if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n",
    "        y_train = np.argmax(y_train, axis=1)  # Convert one-hot to integer labels\n",
    "\n",
    "    # Get number of classes for one-hot encoding\n",
    "    num_classes = student_model.output_shape[-1]\n",
    "    if isinstance(num_classes, tf.TensorShape):\n",
    "        num_classes = num_classes.as_list()[-1]  # Fix for unknown TensorShape\n",
    "    \n",
    "    # Step 1: Get teacher predictions first\n",
    "    print(\"Getting teacher predictions...\")\n",
    "    teacher_logits = teacher_model.predict(X_train)\n",
    "    teacher_probs = tf.nn.softmax(teacher_logits / temperature).numpy()\n",
    "    \n",
    "    # Step 2: Define custom loss function for distillation\n",
    "    def distillation_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the knowledge distillation loss:\n",
    "        - Hard loss: Student's predictions vs. true labels (Sparse Categorical Crossentropy)\n",
    "        - Soft loss: Student's predictions vs. Teacher's soft probabilities (KL Divergence)\n",
    "        \"\"\"\n",
    "        # Get index of the batch in the dataset\n",
    "        batch_indices = tf.range(tf.shape(y_true)[0])\n",
    "        \n",
    "        # Convert y_true to one-hot format (needed for loss calculation)\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(tf.squeeze(y_true), tf.int32), depth=num_classes)\n",
    "        \n",
    "        # Get teacher soft targets for this batch\n",
    "        # We use a more robust approach that doesn't depend on keeping the teacher predictions in memory\n",
    "        batch_teacher_probs = tf.nn.softmax(teacher_model(tf.cast(tf.gather(X_train, batch_indices), tf.float32), training=False) / temperature)\n",
    "        \n",
    "        # Get student soft predictions\n",
    "        student_logits = y_pred\n",
    "        student_soft_probs = tf.nn.softmax(student_logits / temperature)\n",
    "        \n",
    "        # Compute losses\n",
    "        hard_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, student_logits)\n",
    "        soft_loss = tf.keras.losses.KLDivergence()(batch_teacher_probs, student_soft_probs)\n",
    "        \n",
    "        # Weighted combination of hard and soft loss\n",
    "        return (1 - alpha) * hard_loss + alpha * soft_loss * (temperature ** 2)\n",
    "\n",
    "    # Step 3: Compile the student model with distillation loss\n",
    "    student_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                          loss=distillation_loss,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "    # Step 4: Train the student model using distillation\n",
    "    print(\"Training the student model with knowledge distillation...\")\n",
    "    student_model.fit(X_train, y_train, epochs=epochs, batch_size=64, verbose=1)\n",
    "\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
