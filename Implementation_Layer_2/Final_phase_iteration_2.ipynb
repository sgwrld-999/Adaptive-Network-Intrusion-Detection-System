{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom activation\n",
    "tf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Data Preprocessing\n",
    "# -------------------------------------------------------------------------\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Load and preprocess dataset for ANIDS.\n",
    "    - Applies robust scaling\n",
    "    - Removes outliers using IQR\n",
    "    - Splits data into training & validation sets\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    y = df['Attack_label']\n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    \n",
    "    # Outlier removal using IQR\n",
    "    Q1, Q3 = X.quantile(0.25), X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 3 * IQR)) | (X > (Q3 + 3 * IQR))).any(axis=1)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Layer 1: Anomaly Detection\n",
    "# -------------------------------------------------------------------------\n",
    "class AdaptiveNIDSLayer1:\n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = self._build_autoencoder()\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.Reshape((-1, 1))(x)\n",
    "\n",
    "        # Feature extraction via Residual CNN\n",
    "        conv1 = layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "        conv2 = layers.Conv1D(32, 3, activation='relu', padding='same')(conv1)\n",
    "        # Add residual connection\n",
    "        res = layers.Conv1D(32, 1, padding='same')(conv1)\n",
    "        x = layers.Add()([conv2, res])\n",
    "        x = layers.GlobalAveragePooling1D(name=\"gap_layer\")(x)\n",
    "\n",
    "        # Latent Representation\n",
    "        x = layers.Dense(64, activation=mish, kernel_regularizer=regularizers.l1(0.0005))(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        encoded = layers.Dense(self.latent_dim, activation='linear')(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(self.latent_dim * 2, return_sequences=True, recurrent_dropout=0.25)(x)\n",
    "        decoded = layers.TimeDistributed(layers.Dense(1, activation='linear'))(x)\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        autoencoder.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-4), \n",
    "            loss='mse'\n",
    "        )\n",
    "        return autoencoder\n",
    "\n",
    "    def train(self, X_train, X_val, epochs=50):\n",
    "        # Learning rate schedule\n",
    "        lr_schedule = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6\n",
    "        )\n",
    "        # Early stopping\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train, \n",
    "            epochs=epochs, \n",
    "            batch_size=64, \n",
    "            validation_data=(X_val, X_val),\n",
    "            callbacks=[lr_schedule, early_stopping]\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def detect_anomalies(self, X_data, threshold=0.02):\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        return X_data[errors > threshold], np.where(errors > threshold)[0]\n",
    "        \n",
    "    def extract_features(self, X_anomalies):\n",
    "        # Create a feature extractor model using the named gap_layer\n",
    "        feature_extractor = keras.Model(\n",
    "            inputs=self.model.input, \n",
    "            outputs=self.model.get_layer(\"gap_layer\").output\n",
    "        )\n",
    "        return feature_extractor.predict(X_anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Feature Engineering\n",
    "# -------------------------------------------------------------------------\n",
    "def apply_pca(X_features, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce feature dimensionality while retaining specified variance.\n",
    "    \n",
    "    Args:\n",
    "        X_features: Input feature matrix\n",
    "        variance_threshold: Amount of variance to retain (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        PCA-transformed features and fitted PCA model\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    X_features_pca = pca.fit_transform(X_features)\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "    \n",
    "    print(f\"Original features: {X_features.shape[1]}\")\n",
    "    print(f\"PCA features: {X_features_pca.shape[1]}\")\n",
    "    print(f\"Explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    \n",
    "    return X_features_pca, pca\n",
    "\n",
    "def select_features(X_features, y_labels, n_features=10):\n",
    "    \"\"\"\n",
    "    Select top features based on importance from a Random Forest model.\n",
    "    \n",
    "    Args:\n",
    "        X_features: Input feature matrix\n",
    "        y_labels: Target labels\n",
    "        n_features: Number of top features to select\n",
    "        \n",
    "    Returns:\n",
    "        Selected feature matrix and feature indices\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_features, y_labels)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    feature_indices = np.argsort(importances)[::-1][:n_features]\n",
    "    \n",
    "    # Select top features\n",
    "    X_features_selected = X_features[:, feature_indices]\n",
    "    joblib.dump(feature_indices, 'feature_indices.pkl')\n",
    "    print(f\"Original features: {X_features.shape[1]}\")\n",
    "    print(f\"Selected features: {X_features_selected.shape[1]}\")\n",
    "    \n",
    "    return X_features_selected, feature_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Feature Preprocessing & Sequence Handling\n",
    "# -------------------------------------------------------------------------\n",
    "def create_sequences(data, labels=None, seq_length=10, normalize=True):\n",
    "    \"\"\"\n",
    "    Converts feature data into time-series sequences for Layer 2.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): 2D feature matrix.\n",
    "        labels (pd.Series or np.array): Corresponding labels.\n",
    "        seq_length (int): Number of time steps per sequence.\n",
    "        normalize (bool): Whether to normalize sequence features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sequential data, adjusted labels)\n",
    "    \"\"\"\n",
    "    # Ensure data is numpy array\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Feature normalization\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        joblib.dump(scaler, 'sequence_scaler.pkl')\n",
    "    \n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        seq = data[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Use the label of the last timestep in the sequence\n",
    "            seq_labels.append(labels[i + seq_length - 1])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    sequences = np.array(sequences)\n",
    "    \n",
    "    if labels is not None:\n",
    "        labels = np.array(labels)\n",
    "        seq_labels = np.array(seq_labels)\n",
    "        return sequences, seq_labels\n",
    "    else:\n",
    "        return sequences\n",
    "\n",
    "def balance_sequences_with_smote(X_sequences, y_sequences):\n",
    "    \"\"\"\n",
    "    Balance sequence data using SMOTE.\n",
    "    \n",
    "    Args:\n",
    "        X_sequences: 3D sequence data\n",
    "        y_sequences: Target labels\n",
    "        \n",
    "    Returns:\n",
    "        Balanced sequences and labels\n",
    "    \"\"\"\n",
    "    # Ensure y_sequences is integer type\n",
    "    y_sequences = y_sequences.astype(int)\n",
    "    \n",
    "    # Reshape to 2D for SMOTE\n",
    "    original_shape = X_sequences.shape\n",
    "    X_seq_2d = X_sequences.reshape(X_sequences.shape[0], -1)\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_seq_balanced, y_seq_balanced = smote.fit_resample(X_seq_2d, y_sequences)\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_seq_balanced = X_seq_balanced.reshape(-1, original_shape[1], original_shape[2])\n",
    "    \n",
    "    print(f\"Original class distribution: {np.bincount(y_sequences)}\")\n",
    "    print(f\"Balanced class distribution: {np.bincount(y_seq_balanced)}\")\n",
    "    \n",
    "    return X_seq_balanced, y_seq_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Teacher Model for Knowledge Distillation\n",
    "# -------------------------------------------------------------------------\n",
    "class TeacherModel:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self._build_teacher_model()\n",
    "\n",
    "    def _build_teacher_model(self):\n",
    "        \"\"\"\n",
    "        Build a larger, more complex model to serve as the teacher.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim))\n",
    "        \n",
    "        # Enhanced CNN blocks\n",
    "        x = layers.Conv1D(64, kernel_size=3, padding='same')(inputs)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.SpatialDropout1D(0.3)(x)\n",
    "        \n",
    "        # 2nd CNN block with residual connection\n",
    "        conv2 = layers.Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "        conv2 = layers.LayerNormalization()(conv2)\n",
    "        conv2 = layers.Activation('relu')(conv2)\n",
    "        \n",
    "        # Residual connection\n",
    "        res = layers.Conv1D(128, kernel_size=1, padding='same')(x)\n",
    "        x = layers.Add()([conv2, res])\n",
    "        x = layers.SpatialDropout1D(0.3)(x)\n",
    "        \n",
    "        # Enhanced BiLSTM\n",
    "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, recurrent_dropout=0.3))(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention = layers.Dense(1, activation='tanh')(x)\n",
    "        attention = layers.Flatten()(attention)\n",
    "        attention = layers.Softmax()(attention)\n",
    "        attention = layers.RepeatVector(128)(attention)  # 128 = 64*2 (BiLSTM output dim)\n",
    "        attention = layers.Permute([2, 1])(attention)\n",
    "        \n",
    "        # Apply attention\n",
    "        x = layers.Multiply()([x, attention])\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-3), \n",
    "            loss='sparse_categorical_crossentropy', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the teacher model with adaptive learning rate.\n",
    "        \"\"\"\n",
    "        # Learning rate schedule with warmup and cosine decay\n",
    "        initial_lr = 1e-3\n",
    "        warmup_epochs = 5\n",
    "        \n",
    "        def lr_schedule(epoch):\n",
    "            if epoch < warmup_epochs:\n",
    "                return initial_lr * ((epoch + 1) / warmup_epochs)\n",
    "            else:\n",
    "                decay_epochs = epochs - warmup_epochs\n",
    "                return initial_lr * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / decay_epochs))\n",
    "        \n",
    "        lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "        \n",
    "        # F1 score callback for early stopping\n",
    "        class F1ScoreCallback(keras.callbacks.Callback):\n",
    "            def __init__(self, validation_data, patience=5):\n",
    "                super(F1ScoreCallback, self).__init__()\n",
    "                self.X_val, self.y_val = validation_data\n",
    "                self.patience = patience\n",
    "                self.best_f1 = 0\n",
    "                self.wait = 0\n",
    "                self.best_weights = None\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                y_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "                f1 = f1_score(self.y_val, y_pred, average='weighted')\n",
    "                print(f\" - val_f1: {f1:.4f}\")\n",
    "                \n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.wait = 0\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                else:\n",
    "                    self.wait += 1\n",
    "                    if self.wait >= self.patience:\n",
    "                        self.model.stop_training = True\n",
    "                        print(f\"\\nRestoring best weights with F1 score: {self.best_f1:.4f}\")\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "        f1_callback = F1ScoreCallback(validation_data=(X_val, y_val))\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[lr_scheduler, f1_callback]\n",
    "        )\n",
    "        \n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Focal Loss for Imbalanced Classes\n",
    "# -------------------------------------------------------------------------\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss implementation for class imbalance.\n",
    "    Focuses model training on hard-to-classify examples.\n",
    "    \n",
    "    Args:\n",
    "        gamma: Focusing parameter that reduces the loss contribution from easy examples\n",
    "        alpha: Class weight factor for imbalance\n",
    "        \n",
    "    Returns:\n",
    "        Focal loss function compatible with Keras\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # Convert y_true to integers\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        \n",
    "        # Get logits (predictions before softmax)\n",
    "        if y_pred.shape[-1] > 1:\n",
    "            # Convert probabilities to logits\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "            logits = tf.math.log(y_pred)\n",
    "        else:\n",
    "            logits = y_pred\n",
    "            \n",
    "        # Compute cross entropy loss\n",
    "        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=logits)\n",
    "        \n",
    "        # Compute modulating factor\n",
    "        pt = tf.exp(-ce)\n",
    "        \n",
    "        # Apply focal loss formula: alpha * (1-pt)^gamma * ce\n",
    "        loss = alpha * tf.pow(1-pt, gamma) * ce\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Layer 2: Attack Classification with Attention Mechanism\n",
    "# -------------------------------------------------------------------------\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        \"\"\"\n",
    "        Initializes Layer 2 for attack classification using improved CNN-BiLSTM with attention.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features per time step.\n",
    "            num_classes (int): Number of attack classes.\n",
    "            seq_length (int): Number of time steps in sequence.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the improved CNN-BiLSTM classification model with attention.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim))\n",
    "        \n",
    "        # First CNN block with residual connection\n",
    "        x = layers.Conv1D(32, kernel_size=3, padding='same')(inputs)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.SpatialDropout1D(0.2)(x)\n",
    "        \n",
    "        # Second CNN block with residual connection\n",
    "        conv2 = layers.Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "        conv2 = layers.LayerNormalization()(conv2)\n",
    "        conv2 = layers.Activation('relu')(conv2)\n",
    "        \n",
    "        # Residual connection\n",
    "        res = layers.Conv1D(64, kernel_size=1, padding='same')(x)\n",
    "        x = layers.Add()([conv2, res])\n",
    "        x = layers.SpatialDropout1D(0.2)(x)\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        lstm_units = 32\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True, \n",
    "                                           recurrent_dropout=0.2))(x)\n",
    "                                           \n",
    "        # Attention mechanism\n",
    "        attention = layers.Dense(1, activation='tanh')(x)\n",
    "        attention = layers.Flatten()(attention)\n",
    "        attention = layers.Softmax()(attention)\n",
    "        attention = layers.RepeatVector(lstm_units * 2)(attention)  # BiLSTM doubles the units\n",
    "        attention = layers.Permute([2, 1])(attention)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        x = layers.Multiply()([x, attention])\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = layers.Dense(48, activation='relu')(x)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # Output Layer\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "        # Compile Model with focal loss for class imbalance\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-3), \n",
    "            loss=focal_loss(gamma=2.0, alpha=0.25), \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Trains the Layer 2 model with:\n",
    "        - Adaptive learning rate schedule\n",
    "        - F1-score-based early stopping\n",
    "        \"\"\"\n",
    "        # Learning rate schedule with warmup and cosine decay\n",
    "        initial_lr = 1e-3\n",
    "        warmup_epochs = 5\n",
    "        \n",
    "        def lr_schedule(epoch):\n",
    "            if epoch < warmup_epochs:\n",
    "                return initial_lr * ((epoch + 1) / warmup_epochs)\n",
    "            else:\n",
    "                decay_epochs = epochs - warmup_epochs\n",
    "                return initial_lr * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / decay_epochs))\n",
    "        \n",
    "        lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "        \n",
    "        # Custom F1-score callback for early stopping\n",
    "        class F1ScoreCallback(keras.callbacks.Callback):\n",
    "            def __init__(self, validation_data, patience=5):\n",
    "                super(F1ScoreCallback, self).__init__()\n",
    "                self.X_val, self.y_val = validation_data\n",
    "                self.patience = patience\n",
    "                self.best_f1 = 0\n",
    "                self.wait = 0\n",
    "                self.best_weights = None\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                # Get predictions\n",
    "                y_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "                # Calculate F1 score\n",
    "                f1 = f1_score(self.y_val, y_pred, average='weighted')\n",
    "                print(f\" - val_f1: {f1:.4f}\")\n",
    "                \n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.wait = 0\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                else:\n",
    "                    self.wait += 1\n",
    "                    if self.wait >= self.patience:\n",
    "                        self.model.stop_training = True\n",
    "                        print(f\"\\nRestoring best weights with F1 score: {self.best_f1:.4f}\")\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "        f1_callback = F1ScoreCallback(validation_data=(X_val, y_val))\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[lr_scheduler, f1_callback]\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluates the model and generates detailed performance metrics.\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        y_pred = np.argmax(self.model.predict(X_test), axis=1)\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Calculate detection rate and false positive rate\n",
    "        detection_rate = {}\n",
    "        false_positive_rate = {}\n",
    "        \n",
    "        for class_idx in range(self.num_classes):\n",
    "            true_positives = cm[class_idx, class_idx]\n",
    "            false_negatives = np.sum(cm[class_idx, :]) - true_positives\n",
    "            false_positives = np.sum(cm[:, class_idx]) - true_positives\n",
    "            true_negatives = np.sum(cm) - true_positives - false_negatives - false_positives\n",
    "            \n",
    "            detection_rate[class_idx] = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "            false_positive_rate[class_idx] = false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0\n",
    "        \n",
    "        # Return detailed evaluation results\n",
    "        return {\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm,\n",
    "            'detection_rate': detection_rate,\n",
    "            'false_positive_rate': false_positive_rate\n",
    "        }\n",
    "\n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"\n",
    "        Visualizes training loss and accuracy curves.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Knowledge Distillation with Dynamic Temperature\n",
    "# -------------------------------------------------------------------------\n",
    "def knowledge_distillation(teacher_model, student_model, X_train, y_train, X_val, y_val, \n",
    "                          initial_temp=5.0, final_temp=1.0, alpha=0.7, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Implements enhanced knowledge distillation with:\n",
    "    - Dynamic temperature adjustment\n",
    "    - Balanced KL divergence and cross-entropy loss\n",
    "    - Validation-based early stopping\n",
    "    \n",
    "    Args:\n",
    "        teacher_model: Pre-trained teacher model\n",
    "        student_model: Student model to be trained\n",
    "        X_train: Training data features\n",
    "        y_train: Training data labels\n",
    "        X_val: Validation data features\n",
    "        y_val: Validation data labels\n",
    "        initial_temp: Initial temperature for softmax\n",
    "        final_temp: Final temperature for softmax\n",
    "        alpha: Weight balance between hard loss and soft loss\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "    \"\"\"\n",
    "    # Ensure y_train and y_val are NumPy arrays\n",
    "    y_train = np.array(y_train)\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    # Ensure labels are integers for sparse categorical crossentropy\n",
    "    if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "    if len(y_val.shape) > 1 and y_val.shape[1] > 1:\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Get number of classes\n",
    "    num_classes = student_model.output_shape[-1]\n",
    "    if isinstance(num_classes, tf.TensorShape):\n",
    "        num_classes = num_classes.as_list()[-1]\n",
    "    \n",
    "    # Create a custom model for distillation\n",
    "    class DistillationModel(keras.Model):\n",
    "        def __init__(self, student_model, teacher_model):\n",
    "            super(DistillationModel, self).__init__()\n",
    "            self.student_model = student_model\n",
    "            self.teacher_model = teacher_model\n",
    "            self.teacher_model.trainable = False  # Freeze teacher weights\n",
    "        \n",
    "        def compile(self, optimizer, metrics):\n",
    "            super(DistillationModel, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "            self.temperature = tf.Variable(initial_temp, trainable=False)\n",
    "        \n",
    "        def train_step(self, data):\n",
    "            x, y = data\n",
    "            \n",
    "            # Forward pass through teacher model\n",
    "            teacher_logits = self.teacher_model(x, training=False)\n",
    "            \n",
    "            # Calculate temperature-scaled probabilities\n",
    "            current_temp = self.temperature\n",
    "            teacher_probs = tf.nn.softmax(teacher_logits / current_temp)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass through student model\n",
    "                student_logits = self.student_model(x, training=True)\n",
    "                student_probs = tf.nn.softmax(student_logits / current_temp)\n",
    "                \n",
    "                # Calculate hard loss (student predictions vs true labels)\n",
    "                hard_loss = tf.keras.losses.sparse_categorical_crossentropy(y, student_logits)\n",
    "                \n",
    "                # Calculate soft loss (KL divergence between teacher and student)\n",
    "                soft_loss = tf.keras.losses.kullback_leibler_divergence(teacher_probs, student_probs)\n",
    "                \n",
    "                # Total loss with dynamic temperature scaling\n",
    "                total_loss = (1 - alpha) * hard_loss + alpha * soft_loss * (current_temp ** 2)\n",
    "                \n",
    "            # Get gradients and update student model\n",
    "            gradients = tape.gradient(total_loss, self.student_model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.student_model.trainable_variables))\n",
    "            \n",
    "            # Update metrics\n",
    "            self.compiled_metrics.update_state(y, student_logits)\n",
    "            \n",
    "            # Return a dict mapping metric names to current values\n",
    "            results = {m.name: m.result() for m in self.metrics}\n",
    "            results.update({\"hard_loss\": tf.reduce_mean(hard_loss), \n",
    "                           \"soft_loss\": tf.reduce_mean(soft_loss),\n",
    "                           \"total_loss\": tf.reduce_mean(total_loss),\n",
    "                           \"temperature\": current_temp})\n",
    "            return results\n",
    "    \n",
    "    # Create and compile the distillation model\n",
    "    distillation_model = DistillationModel(student_model, teacher_model)\n",
    "    distillation_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create callbacks for distillation\n",
    "    class TemperatureScheduler(keras.callbacks.Callback):\n",
    "        def __init__(self, initial_temp, final_temp, epochs):\n",
    "            super(TemperatureScheduler, self).__init__()\n",
    "            self.initial_temp = initial_temp\n",
    "            self.final_temp = final_temp\n",
    "            self.epochs = epochs\n",
    "        \n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            # Linear temperature decay\n",
    "            temp = self.initial_temp - (self.initial_temp - self.final_temp) * epoch / self.epochs\n",
    "            self.model.temperature.assign(max(temp, self.final_temp))\n",
    "    \n",
    "    # Create F1 score callback for early stopping\n",
    "    class F1ScoreCallback(keras.callbacks.Callback):\n",
    "        def __init__(self, X_val, y_val, patience=5):\n",
    "            super(F1ScoreCallback, self).__init__()\n",
    "            self.X_val = X_val\n",
    "            self.y_val = y_val\n",
    "            self.patience = patience\n",
    "            self.best_f1 = 0\n",
    "            self.wait = 0\n",
    "            self.best_weights = None\n",
    "        \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            # Get predictions from student model\n",
    "            student_predictions = self.model.student_model.predict(self.X_val)\n",
    "            y_pred = np.argmax(student_predictions, axis=1)\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            f1 = f1_score(self.y_val, y_pred, average='weighted')\n",
    "            print(f\" - val_f1: {f1:.4f}\")\n",
    "            \n",
    "            # Check if F1 score improved\n",
    "            if f1 > self.best_f1:\n",
    "                self.best_f1 = f1\n",
    "                self.wait = 0\n",
    "                # Save best weights\n",
    "                self.best_weights = self.model.student_model.get_weights()\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    # Restore best weights and stop training\n",
    "                    self.model.student_model.set_weights(self.best_weights)\n",
    "                    self.model.stop_training = True\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch+1}. Best val_f1: {self.best_f1:.4f}\")\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = [\n",
    "        TemperatureScheduler(initial_temp, final_temp, epochs),\n",
    "        F1ScoreCallback(X_val, y_val, patience=5)\n",
    "    ]\n",
    "    \n",
    "    # Train the distillation model\n",
    "    history = distillation_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Return the trained student model and training history\n",
    "    return distillation_model.student_model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Model Evaluation and Visualization\n",
    "# -------------------------------------------------------------------------\n",
    "def evaluate_model_comprehensive(model, X_test, y_test, class_names=None):\n",
    "    \"\"\"\n",
    "    Conducts comprehensive evaluation of model performance.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classification model\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "        class_names: List of class names (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Detection rates and false positive rates\n",
    "    num_classes = len(np.unique(y_test))\n",
    "    dr = np.zeros(num_classes)\n",
    "    fpr = np.zeros(num_classes)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i, i]\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        tn = np.sum(cm) - tp - fn - fp\n",
    "        \n",
    "        dr[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr[i] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    result = {\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'detection_rates': dr,\n",
    "        'false_positive_rates': fpr,\n",
    "        'macro_avg_detection_rate': np.mean(dr),\n",
    "        'macro_avg_false_positive_rate': np.mean(fpr),\n",
    "        'weighted_f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def visualize_model_performance(evaluation_results, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualizes model performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary of evaluation metrics\n",
    "        class_names: List of class names (optional)\n",
    "    \"\"\"\n",
    "    cm = evaluation_results['confusion_matrix']\n",
    "    report = evaluation_results['classification_report']\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    if class_names is not None:\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot detection rates and false positive rates\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(evaluation_results['detection_rates']))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, evaluation_results['detection_rates'], width, label='Detection Rate')\n",
    "    plt.bar(x + width/2, evaluation_results['false_positive_rates'], width, label='False Positive Rate')\n",
    "    \n",
    "    plt.ylabel('Rate')\n",
    "    plt.title('Detection Rates and False Positive Rates by Class')\n",
    "    \n",
    "    if class_names is not None:\n",
    "        plt.xticks(x, class_names, rotation=45)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('detection_rates.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Hyperparameter Optimization\n",
    "# -------------------------------------------------------------------------\n",
    "def hyperparameter_optimization(X_train, y_train, X_val, y_val, seq_length, input_dim, num_classes):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter optimization for the NIDS model.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels\n",
    "        seq_length: Sequence length\n",
    "        input_dim: Input dimension\n",
    "        num_classes: Number of classes\n",
    "        \n",
    "    Returns:\n",
    "        Best hyperparameters and best model\n",
    "    \"\"\"\n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "        'dropout_rate': [0.2, 0.3, 0.4],\n",
    "        'lstm_units': [16, 32, 64],\n",
    "        'conv_filters': [32, 64, 128]\n",
    "    }\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_f1 = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Iterate through parameter combinations\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"Trying parameters: {params}\")\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = keras.Sequential([\n",
    "            layers.Input(shape=(seq_length, input_dim)),\n",
    "            layers.Conv1D(params['conv_filters'], kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.SpatialDropout1D(params['dropout_rate']),\n",
    "            layers.Bidirectional(layers.LSTM(params['lstm_units'], return_sequences=True)),\n",
    "            layers.GlobalMaxPooling1D(),\n",
    "            layers.Dense(48, activation='relu'),\n",
    "            layers.Dropout(params['dropout_rate']),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=15,  # Reduced epochs for faster search\n",
    "            batch_size=64,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "        val_f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"Validation F1 score: {val_f1:.4f}\")\n",
    "        \n",
    "        # Check if best so far\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n",
    "    \n",
    "    return best_params, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Main Execution\n",
    "# -------------------------------------------------------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the Adaptive NIDS system.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    X_train, X_val, y_train, y_val = preprocess_data('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv')\n",
    "    \n",
    "    # Layer 1: Anomaly Detection\n",
    "    anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[1], latent_dim=16)\n",
    "    history = anomaly_detector.train(X_train, X_val, epochs=50)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold=0.02)\n",
    "    y_anomalies = y_train.iloc[indices].values\n",
    "    \n",
    "    # Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (12075, 44)\n",
      "Validation data shape: (3019, 44)\n"
     ]
    }
   ],
   "source": [
    "# Replace with your dataset path\n",
    "file_path = \"/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv\"\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_val, y_train, y_val = preprocess_data(file_path)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 0.2305 - val_loss: 0.2211 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 0.2098 - val_loss: 0.2032 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1981 - val_loss: 0.1852 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1782 - val_loss: 0.1699 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1645 - val_loss: 0.1570 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.1523 - val_loss: 0.1458 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1384 - val_loss: 0.1358 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1331 - val_loss: 0.1276 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1245 - val_loss: 0.1204 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1177 - val_loss: 0.1147 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1133 - val_loss: 0.1107 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1103 - val_loss: 0.1078 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1053 - val_loss: 0.1060 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1090 - val_loss: 0.1053 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1027 - val_loss: 0.1047 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1039 - val_loss: 0.1037 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1016 - val_loss: 0.1033 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1044 - val_loss: 0.1025 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - loss: 0.1035 - val_loss: 0.1015 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0984 - val_loss: 0.1007 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1017 - val_loss: 0.1004 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1052 - val_loss: 0.0997 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1036 - val_loss: 0.0990 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1000 - val_loss: 0.0986 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1025 - val_loss: 0.0980 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1027 - val_loss: 0.0977 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1019 - val_loss: 0.0978 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.1011 - val_loss: 0.0973 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0999 - val_loss: 0.0970 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1026 - val_loss: 0.0964 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1007 - val_loss: 0.0959 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1008 - val_loss: 0.0959 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0988 - val_loss: 0.0950 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0959 - val_loss: 0.0952 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0996 - val_loss: 0.0943 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0965 - val_loss: 0.0943 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0951 - val_loss: 0.0938 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0985 - val_loss: 0.0935 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0982 - val_loss: 0.0941 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1004 - val_loss: 0.0938 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0966 - val_loss: 0.0930 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0976 - val_loss: 0.0933 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0973 - val_loss: 0.0923 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0950 - val_loss: 0.0918 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0961 - val_loss: 0.0917 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0942 - val_loss: 0.0919 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0964 - val_loss: 0.0916 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0928 - val_loss: 0.0912 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0962 - val_loss: 0.0901 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0943 - val_loss: 0.0900 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Layer 1\n",
    "anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[1], latent_dim=16)\n",
    "history = anomaly_detector.train(X_train, X_val, epochs=50)\n",
    "\n",
    "# Plot training history (optional)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'feature_indices.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m X_val_pca = pca.transform(X_val_features)  \u001b[38;5;66;03m# Use the same PCA model saved earlier\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Load feature indices from file (assuming they were saved during training)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m feature_indices = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeature_indices.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Step 4: Select features (optional, if you used feature selection earlier)\u001b[39;00m\n\u001b[32m     19\u001b[39m X_val_selected = X_val_pca[:, feature_indices]  \u001b[38;5;66;03m# Use the same feature_indices saved earlier\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode)\u001b[39m\n\u001b[32m    648\u001b[39m         obj = _unpickle(fobj)\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    651\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[32m    652\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    653\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    654\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    655\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'feature_indices.pkl'"
     ]
    }
   ],
   "source": [
    "# Preprocess X_val for Layer 2\n",
    "# Step 1: Detect anomalies in X_val (optional, depending on your use case)\n",
    "X_val_anomalies, val_indices = anomaly_detector.detect_anomalies(X_val, threshold=0.02)\n",
    "y_val_anomalies = y_val.iloc[val_indices].values  # Convert to NumPy array\n",
    "\n",
    "# Ensure y_val_anomalies is integer type\n",
    "y_val_anomalies = y_val_anomalies.astype(int)\n",
    "\n",
    "# Step 2: Extract features for X_val\n",
    "X_val_features = anomaly_detector.extract_features(X_val_anomalies)\n",
    "\n",
    "# Step 3: Apply PCA to X_val\n",
    "X_val_pca = pca.transform(X_val_features)  # Use the same PCA model saved earlier\n",
    "\n",
    "# Load feature indices from file (assuming they were saved during training)\n",
    "feature_indices = joblib.load('feature_indices.pkl')\n",
    "\n",
    "# Step 4: Select features (optional, if you used feature selection earlier)\n",
    "X_val_selected = X_val_pca[:, feature_indices]  # Use the same feature_indices saved earlier\n",
    "# Load feature indices from file\n",
    "feature_indices = joblib.load('feature_indices.pkl')\n",
    "# Step 5: Create sequences for X_val\n",
    "X_val_sequences, y_val_sequences = create_sequences(X_val_selected, y_val_anomalies, seq_length=seq_length)\n",
    "\n",
    "# Initialize Layer 2\n",
    "num_classes = len(np.unique(y_balanced))  # Ensure this matches your dataset\n",
    "layer2_model = AdaptiveNIDSLayer2(\n",
    "    input_dim=X_balanced.shape[2],  # Features per timestep\n",
    "    num_classes=num_classes,\n",
    "    seq_length=seq_length\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = layer2_model.train(\n",
    "    X_balanced, \n",
    "    y_balanced, \n",
    "    X_val_sequences,  # Use preprocessed X_val\n",
    "    y_val_sequences,  # Use preprocessed y_val\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "layer2_model.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
