{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 0.2465 - val_loss: 0.2437 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.2320 - val_loss: 0.2299 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.2125 - val_loss: 0.2155 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.2030 - val_loss: 0.2034 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1906 - val_loss: 0.1928 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1787 - val_loss: 0.1834 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1702 - val_loss: 0.1745 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1622 - val_loss: 0.1665 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1537 - val_loss: 0.1593 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1454 - val_loss: 0.1525 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1391 - val_loss: 0.1465 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1395 - val_loss: 0.1413 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.1282 - val_loss: 0.1369 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1256 - val_loss: 0.1332 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1250 - val_loss: 0.1297 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1172 - val_loss: 0.1271 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1164 - val_loss: 0.1248 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1152 - val_loss: 0.1233 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1136 - val_loss: 0.1223 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1149 - val_loss: 0.1213 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1118 - val_loss: 0.1205 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1059 - val_loss: 0.1201 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1140 - val_loss: 0.1195 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1100 - val_loss: 0.1190 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1128 - val_loss: 0.1187 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1113 - val_loss: 0.1181 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1077 - val_loss: 0.1170 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1093 - val_loss: 0.1169 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1130 - val_loss: 0.1159 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1083 - val_loss: 0.1156 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1084 - val_loss: 0.1155 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1105 - val_loss: 0.1144 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1093 - val_loss: 0.1140 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1070 - val_loss: 0.1141 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1121 - val_loss: 0.1126 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1102 - val_loss: 0.1121 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1091 - val_loss: 0.1112 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1081 - val_loss: 0.1110 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1089 - val_loss: 0.1109 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.1054 - val_loss: 0.1107 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1058 - val_loss: 0.1103 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.1030 - val_loss: 0.1102 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1060 - val_loss: 0.1093 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.1052 - val_loss: 0.1092 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.1049 - val_loss: 0.1086 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1032 - val_loss: 0.1082 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1079 - val_loss: 0.1083 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1064 - val_loss: 0.1076 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1051 - val_loss: 0.1082 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.1026 - val_loss: 0.1066 - learning_rate: 1.0000e-04\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step\n",
      "Epoch 1/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step    - accuracy: 0.5003\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.5003 - loss: 0.0694 - val_accuracy: 0.2647 - val_loss: 0.0441 - learning_rate: 2.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step ep - accuracy: 0.5004 \n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.5004 - loss: 0.0451 - val_accuracy: 0.7356 - val_loss: 0.0423 - learning_rate: 4.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.5153 - l\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.5153 - loss: 0.0435 - val_accuracy: 0.7261 - val_loss: 0.0433 - learning_rate: 6.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.5076 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5076 - loss: 0.0434 - val_accuracy: 0.7356 - val_loss: 0.0433 - learning_rate: 8.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.5077 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5077 - loss: 0.0435 - val_accuracy: 0.2644 - val_loss: 0.0447 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.5169 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5170 - loss: 0.0433 - val_accuracy: 0.7473 - val_loss: 0.0402 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.5833 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5838 - loss: 0.0421 - val_accuracy: 0.6368 - val_loss: 0.0404 - learning_rate: 9.9878e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step ep - accuracy: 0.6873 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6873 - loss: 0.0372 - val_accuracy: 0.6717 - val_loss: 0.0335 - learning_rate: 9.9513e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7129 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7128 - loss: 0.0361 - val_accuracy: 0.6283 - val_loss: 0.0361 - learning_rate: 9.8907e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7189 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7190 - loss: 0.0350 - val_accuracy: 0.6731 - val_loss: 0.0325 - learning_rate: 9.8063e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7452 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7453 - loss: 0.0334 - val_accuracy: 0.6911 - val_loss: 0.0373 - learning_rate: 9.6985e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7577 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7578 - loss: 0.0325 - val_accuracy: 0.7046 - val_loss: 0.0339 - learning_rate: 9.5677e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7734 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7734 - loss: 0.0309 - val_accuracy: 0.7381 - val_loss: 0.0338 - learning_rate: 9.4147e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7834 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7834 - loss: 0.0300 - val_accuracy: 0.7621 - val_loss: 0.0293 - learning_rate: 9.2402e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7882 - l\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7882 - loss: 0.0301 - val_accuracy: 0.7349 - val_loss: 0.0308 - learning_rate: 9.0451e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7911 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7910 - loss: 0.0296 - val_accuracy: 0.6576 - val_loss: 0.0358 - learning_rate: 8.8302e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7855 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7856 - loss: 0.0301 - val_accuracy: 0.7095 - val_loss: 0.0356 - learning_rate: 8.5967e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7934 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7934 - loss: 0.0291 - val_accuracy: 0.7229 - val_loss: 0.0320 - learning_rate: 8.3457e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7927 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7928 - loss: 0.0290 - val_accuracy: 0.7829 - val_loss: 0.0255 - learning_rate: 8.0783e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7930 - l\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7930 - loss: 0.0292 - val_accuracy: 0.7406 - val_loss: 0.0334 - learning_rate: 7.7960e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7957 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7958 - loss: 0.0287 - val_accuracy: 0.7000 - val_loss: 0.0362 - learning_rate: 7.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.7965 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.7965 - loss: 0.0288 - val_accuracy: 0.7624 - val_loss: 0.0283 - learning_rate: 7.1919e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.8112 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8111 - loss: 0.0276 - val_accuracy: 0.7339 - val_loss: 0.0317 - learning_rate: 6.8730e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step ep - accuracy: 0.8104 - los\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8104 - loss: 0.0275 - val_accuracy: 0.7536 - val_loss: 0.0306 - learning_rate: 6.5451e-04\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Final Test Results:\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.79      0.65       583\n",
      "           1       0.91      0.77      0.84      1671\n",
      "\n",
      "    accuracy                           0.78      2254\n",
      "   macro avg       0.73      0.78      0.74      2254\n",
      "weighted avg       0.82      0.78      0.79      2254\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    df = pd.read_csv(file_path)\n",
    "    y = df['Attack_label']\n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    \n",
    "    # Outlier removal\n",
    "    Q1, Q3 = X.quantile(0.25), X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 3 * IQR)) | (X > (Q3 + 3 * IQR))).any(axis=1)\n",
    "    X, y = X[mask], y[mask]\n",
    "    \n",
    "    # Stratified split into train+val and test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    # Stratified split of train_val into train and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size/(1-test_size), \n",
    "        random_state=random_state, stratify=y_train_val)\n",
    "    \n",
    "    # Robust scaling\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "class AdaptiveNIDSLayer1:\n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = self._build_autoencoder()\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.Reshape((-1, 1))(x)\n",
    "        \n",
    "        # Encoder\n",
    "        conv1 = layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "        conv2 = layers.Conv1D(32, 3, activation='relu', padding='same')(conv1)\n",
    "        res = layers.Conv1D(32, 1, padding='same')(conv1)\n",
    "        x = layers.Add()([conv2, res])\n",
    "        x = layers.GlobalAveragePooling1D(name=\"gap_layer\")(x)\n",
    "        x = layers.Dense(64, activation=mish, kernel_regularizer=regularizers.l1(0.0005))(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        encoded = layers.Dense(self.latent_dim, activation='linear')(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(self.latent_dim * 2, return_sequences=True, recurrent_dropout=0.25)(x)\n",
    "        decoded = layers.TimeDistributed(layers.Dense(1, activation='linear'))(x)\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        autoencoder.compile(optimizer=keras.optimizers.Adam(1e-4), loss='mse')\n",
    "        return autoencoder\n",
    "\n",
    "    def train(self, X_train, X_val, epochs=50):\n",
    "        lr_schedule = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        return self.model.fit(\n",
    "            X_train, X_train, epochs=epochs, batch_size=64,\n",
    "            validation_data=(X_val, X_val), callbacks=[lr_schedule, early_stopping])\n",
    "\n",
    "    def detect_anomalies(self, X_data, threshold=0.03):  # Increased threshold\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        return X_data[errors > threshold], np.where(errors > threshold)[0]\n",
    "\n",
    "    def extract_features(self, X_anomalies):\n",
    "        feature_extractor = keras.Model(\n",
    "            inputs=self.model.input, outputs=self.model.get_layer(\"gap_layer\").output)\n",
    "        return feature_extractor.predict(X_anomalies)\n",
    "\n",
    "def apply_pca(X_features, variance_threshold=0.99):  # Increased variance retention\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    X_features_pca = pca.fit_transform(X_features)\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "    return X_features_pca, pca\n",
    "\n",
    "def select_features(X_features, y_labels, n_features=15):  # Increased feature count\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_features, y_labels)\n",
    "    feature_indices = np.argsort(rf.feature_importances_)[::-1][:n_features]\n",
    "    return X_features[:, feature_indices], feature_indices\n",
    "\n",
    "def create_sequences(data, labels=None, seq_length=10, normalize=True):\n",
    "    data = np.array(data)\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        joblib.dump(scaler, 'sequence_scaler.pkl')\n",
    "    sequences, seq_labels = [], []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        if labels is not None:\n",
    "            seq_labels.append(labels[i + seq_length - 1])\n",
    "    sequences = np.array(sequences)\n",
    "    return (sequences, np.array(seq_labels)) if labels is not None else sequences\n",
    "\n",
    "def balance_sequences_with_adasyn(X_sequences, y_sequences):  # Changed to ADASYN\n",
    "    y_sequences = y_sequences.astype(int)\n",
    "    original_shape = X_sequences.shape\n",
    "    X_seq_2d = X_sequences.reshape(X_sequences.shape[0], -1)\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_seq_balanced, y_seq_balanced = adasyn.fit_resample(X_seq_2d, y_sequences)\n",
    "    return X_seq_balanced.reshape(-1, original_shape[1], original_shape[2]), y_seq_balanced\n",
    "\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim))\n",
    "        \n",
    "        # Enhanced Conv blocks\n",
    "        x = layers.Conv1D(32, kernel_size=3, padding='same')(inputs)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.SpatialDropout1D(0.3)(x)  # Increased dropout\n",
    "        \n",
    "        conv2 = layers.Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "        conv2 = layers.LayerNormalization()(conv2)\n",
    "        conv2 = layers.Activation('relu')(conv2)\n",
    "        conv2 = layers.Dropout(0.3)(conv2)  # Added dropout\n",
    "        \n",
    "        res = layers.Conv1D(64, kernel_size=1, padding='same')(x)\n",
    "        x = layers.Add()([conv2, res])\n",
    "        x = layers.SpatialDropout1D(0.3)(x)\n",
    "        \n",
    "        # Increased LSTM units\n",
    "        lstm_units = 64\n",
    "        x = layers.Bidirectional(layers.LSTM(\n",
    "            lstm_units, return_sequences=True, recurrent_dropout=0.2))(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention = layers.Dense(1, activation='tanh')(x)\n",
    "        attention = layers.Flatten()(attention)\n",
    "        attention = layers.Softmax()(attention)\n",
    "        attention = layers.RepeatVector(lstm_units * 2)(attention)\n",
    "        attention = layers.Permute([2, 1])(attention)\n",
    "        x = layers.Multiply()([x, attention])\n",
    "        \n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)  # Increased units\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)  # Increased dropout\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-3), \n",
    "            loss=focal_loss(gamma=2.0, alpha=0.25), \n",
    "            metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "        initial_lr = 1e-3\n",
    "        warmup_epochs = 5\n",
    "        def lr_schedule(epoch):\n",
    "            if epoch < warmup_epochs:\n",
    "                return initial_lr * ((epoch + 1) / warmup_epochs)\n",
    "            else:\n",
    "                decay_epochs = epochs - warmup_epochs\n",
    "                return initial_lr * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / decay_epochs))\n",
    "        \n",
    "        class F1ScoreCallback(keras.callbacks.Callback):\n",
    "            def __init__(self, validation_data, patience=5):\n",
    "                super().__init__()\n",
    "                self.X_val, self.y_val = validation_data\n",
    "                self.patience = patience\n",
    "                self.best_f1 = 0\n",
    "                self.wait = 0\n",
    "                self.best_weights = None\n",
    "            \n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                y_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "                f1 = f1_score(self.y_val, y_pred, average='weighted')\n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.wait = 0\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                else:\n",
    "                    self.wait += 1\n",
    "                    if self.wait >= self.patience:\n",
    "                        self.model.stop_training = True\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "        lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "        f1_callback = F1ScoreCallback(validation_data=(X_val, y_val))\n",
    "        \n",
    "        return self.model.fit(\n",
    "            X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val), \n",
    "            callbacks=[lr_scheduler, f1_callback])\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        y_pred = np.argmax(self.model.predict(X_test), axis=1)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        detection_rate = {}\n",
    "        false_positive_rate = {}\n",
    "        for class_idx in range(self.num_classes):\n",
    "            true_positives = cm[class_idx, class_idx]\n",
    "            false_negatives = np.sum(cm[class_idx, :]) - true_positives\n",
    "            false_positives = np.sum(cm[:, class_idx]) - true_positives\n",
    "            true_negatives = np.sum(cm) - true_positives - false_negatives - false_positives\n",
    "            detection_rate[class_idx] = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "            false_positive_rate[class_idx] = false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0\n",
    "        return {\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm,\n",
    "            'detection_rate': detection_rate,\n",
    "            'false_positive_rate': false_positive_rate\n",
    "        }\n",
    "\n",
    "    def plot_training_history(self, history):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png')\n",
    "        plt.show()\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        if y_pred.shape[-1] > 1:\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "            logits = tf.math.log(y_pred)\n",
    "        else:\n",
    "            logits = y_pred\n",
    "        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=logits)\n",
    "        pt = tf.exp(-ce)\n",
    "        loss = alpha * tf.pow(1-pt, gamma) * ce\n",
    "        return tf.reduce_mean(loss)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def main():\n",
    "    file_path = \"/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv\"\n",
    "    \n",
    "    # Three-way data split\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(file_path)\n",
    "    \n",
    "    # Layer 1: Anomaly Detection\n",
    "    anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[1], latent_dim=16)\n",
    "    layer1_history = anomaly_detector.train(X_train, X_val, epochs=50)\n",
    "    \n",
    "    # Detect anomalies in training data\n",
    "    X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold=0.03)\n",
    "    y_anomalies = y_train.iloc[indices].values.astype(int)\n",
    "    \n",
    "    # Feature engineering pipeline\n",
    "    X_features = anomaly_detector.extract_features(X_anomalies)\n",
    "    X_pca, pca = apply_pca(X_features)\n",
    "    X_selected, feature_indices = select_features(X_pca, y_anomalies, n_features=15)\n",
    "    joblib.dump(feature_indices, 'feature_indices_Iteration_3.pkl')\n",
    "    \n",
    "    # Sequence creation and balancing\n",
    "    X_sequences, y_sequences = create_sequences(X_selected, y_anomalies)\n",
    "    X_balanced, y_balanced = balance_sequences_with_adasyn(X_sequences, y_sequences)\n",
    "    \n",
    "    # Prepare validation sequences\n",
    "    X_val_anomalies, val_indices = anomaly_detector.detect_anomalies(X_val, threshold=0.03)\n",
    "    y_val_anomalies = y_val.iloc[val_indices].values.astype(int)\n",
    "    X_val_features = anomaly_detector.extract_features(X_val_anomalies)\n",
    "    X_val_pca = pca.transform(X_val_features)\n",
    "    X_val_selected = X_val_pca[:, feature_indices]\n",
    "    X_val_sequences, y_val_sequences = create_sequences(X_val_selected, y_val_anomalies)\n",
    "    \n",
    "    # Layer 2: Classification\n",
    "    num_classes = len(np.unique(y_balanced))\n",
    "    layer2_model = AdaptiveNIDSLayer2(\n",
    "        input_dim=X_balanced.shape[2], \n",
    "        num_classes=num_classes, \n",
    "        seq_length=10\n",
    "    )\n",
    "    layer2_history = layer2_model.train(\n",
    "        X_balanced, y_balanced, \n",
    "        X_val_sequences, y_val_sequences, \n",
    "        epochs=50\n",
    "    )\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    X_test_anomalies, test_indices = anomaly_detector.detect_anomalies(X_test, threshold=0.03)\n",
    "    y_test_anomalies = y_test.iloc[test_indices].values.astype(int)\n",
    "    X_test_features = anomaly_detector.extract_features(X_test_anomalies)\n",
    "    X_test_pca = pca.transform(X_test_features)\n",
    "    X_test_selected = X_test_pca[:, feature_indices]\n",
    "    X_test_sequences, y_test_sequences = create_sequences(X_test_selected, y_test_anomalies)\n",
    "    \n",
    "    test_results = layer2_model.evaluate_model(X_test_sequences, y_test_sequences)\n",
    "    print(\"Final Test Results:\")\n",
    "    print(classification_report(y_test_sequences, np.argmax(layer2_model.model.predict(X_test_sequences), axis=1)))\n",
    "    \n",
    "    layer2_model.plot_training_history(layer2_history)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Intrusion Detection System Training Analysis\n",
    "\n",
    "## Model Performance Overview\n",
    "\n",
    "### **Layer 1: Anomaly Detection (Autoencoder)**\n",
    "| Metric              | Start Value | End Value | Improvement |\n",
    "|---------------------|-------------|-----------|-------------|\n",
    "| Training Loss       | 0.2465      | 0.1026    | 58.4% ↓     |\n",
    "| Validation Loss     | 0.2437      | 0.1066    | 56.3% ↓     |\n",
    "\n",
    "- Smooth convergence with no signs of overfitting\n",
    "- Final validation loss: **0.1066** (epoch 50)\n",
    "\n",
    "---\n",
    "\n",
    "### **Layer 2: Classification**\n",
    "#### Training Highlights:\n",
    "- **Best Validation Accuracy:** 78.29% (epoch 19)\n",
    "- **Final Test Accuracy:** 78%\n",
    "- Weighted F1-Score: **0.79**\n",
    "\n",
    "#### Class-Wise Performance:\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| 0     | 0.55      | 0.79   | 0.65     | 583     |\n",
    "| 1     | 0.91      | 0.77   | 0.84     | 1,671   |\n",
    "\n",
    "**Key Observations:**  \n",
    "- Class 1 (Majority): High precision but moderate recall\n",
    "- Class 0 (Minority): High recall but low precision (indicates class imbalance impact)\n",
    "\n",
    "---\n",
    "\n",
    "## Training Dynamics\n",
    "\n",
    "### Loss Curves\n",
    "![Model Loss](image.png)\n",
    "- **Trend:** Steady decrease for both training and validation loss\n",
    "- **Plateau:** Validation loss stabilizes after epoch 30, suggesting:\n",
    "  - Model convergence\n",
    "  - Potential need for learning rate adjustment\n",
    "\n",
    "### Accuracy Curves\n",
    "![Model Accuracy](image.png)\n",
    "- **Training Accuracy:** Stable growth to ~81%\n",
    "- **Validation Accuracy:** Fluctuates between 65-78%, indicating:\n",
    "  - Sensitivity to learning rate changes\n",
    "  - Possible overfitting on minority class\n",
    "\n",
    "---\n",
    "\n",
    "## Critical Insights\n",
    "\n",
    "### **Strengths**\n",
    "- Effective anomaly detection (Layer 1 achieves 58% loss reduction)\n",
    "- Good final test accuracy (78%) despite class imbalance\n",
    "- Smooth learning rate decay implementation\n",
    "\n",
    "### **Weaknesses**\n",
    "- **Class Imbalance Impact:** \n",
    "  - Class 0 recall (79%) vs precision (55%)\n",
    "  - Class 1 precision-recall gap (91% vs 77%)\n",
    "- **Validation Instability:** \n",
    "  - Accuracy swings up to 12% between epochs (e.g., epoch 5: 26.4% → epoch 6: 74.7%)\n",
    "\n",
    "---\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Class Imbalance Mitigation**\n",
    "   - Apply class weights in focal loss\n",
    "   - Test ADASYN instead of SMOTE for sequence balancing\n",
    "\n",
    "2. **Model Optimization**\n",
    "   - Add dropout (0.3-0.4) in Layer 2 LSTM blocks\n",
    "   - Reduce learning rate after validation plateau detection\n",
    "\n",
    "3. **Validation Stability**\n",
    "   - Increase batch size from 32 → 64\n",
    "   - Use label smoothing in focal loss\n",
    "\n",
    "4. **Architecture Tweaks**\n",
    "   - Add skip connections in Layer 2 Conv blocks\n",
    "   - Increase LSTM units from 64 → 128\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "The system demonstrates strong anomaly detection capabilities but requires targeted improvements for classification stability. Prioritize class imbalance handling and architectural adjustments to bridge the precision-recall gap while maintaining overall accuracy. Final test metrics confirm practical usability (F1=0.79), but further tuning could yield 2-3% performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 106ms/step - loss: 107.3843 - val_loss: 179.1667 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m133/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 129.0835"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 637\u001b[39m\n\u001b[32m    634\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_results\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 543\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# Train the anomaly detector (Layer 1)\u001b[39;00m\n\u001b[32m    542\u001b[39m anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[\u001b[32m1\u001b[39m], latent_dim=\u001b[32m16\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m layer1_history = \u001b[43manomaly_detector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# Detect anomalies\u001b[39;00m\n\u001b[32m    546\u001b[39m X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold=\u001b[32m0.03\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mAdaptiveNIDSLayer1.train\u001b[39m\u001b[34m(self, X_train, X_val, epochs)\u001b[39m\n\u001b[32m    115\u001b[39m lr_schedule = keras.callbacks.ReduceLROnPlateau(\n\u001b[32m    116\u001b[39m     monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m5\u001b[39m, min_lr=\u001b[32m1e-6\u001b[39m)\n\u001b[32m    117\u001b[39m early_stopping = keras.callbacks.EarlyStopping(\n\u001b[32m    118\u001b[39m     monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    df = pd.read_csv(file_path)\n",
    "    y = df['Attack_label']\n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    \n",
    "    # Remove outliers using IQR\n",
    "    Q1, Q3 = X.quantile(0.25), X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 3 * IQR)) | (X > (Q3 + 3 * IQR))).any(axis=1)\n",
    "    X, y = X[mask], y[mask]\n",
    "    \n",
    "    # First split: Train + Validation vs Test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    # Second split: Train vs Validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size, \n",
    "        random_state=random_state, stratify=y_train_val)\n",
    "    \n",
    "    # Apply preprocessing steps to all splits\n",
    "    def apply_preprocessing(X_data):\n",
    "        # Apply log1p to all columns and store as new columns\n",
    "        log_features = np.log1p(np.abs(X_data))\n",
    "        log_features.columns = [f'log_{col}' for col in X_data.columns]\n",
    "        X_data = pd.concat([X_data, log_features], axis=1)\n",
    "        \n",
    "        # Interaction features\n",
    "        X_data['interaction_features'] = X_data.iloc[:, :5].mul(X_data.iloc[:, 5:10]).sum(axis=1)\n",
    "        \n",
    "        # Percentile rank for all columns and store as new columns\n",
    "        percentile_rank = X_data.rank(pct=True)\n",
    "        percentile_rank.columns = [f'percentile_rank_{col}' for col in X_data.columns]\n",
    "        X_data = pd.concat([X_data, percentile_rank], axis=1)\n",
    "        \n",
    "        # Statistical features\n",
    "        X_data['mean'] = X_data.mean(axis=1)\n",
    "        X_data['std'] = X_data.std(axis=1)\n",
    "        X_data['skew'] = X_data.skew(axis=1)\n",
    "        X_data['kurt'] = X_data.kurtosis(axis=1)\n",
    "        \n",
    "        # Winsorize data\n",
    "        from scipy.stats import mstats\n",
    "        for col in X_data.columns:\n",
    "            X_data[col] = mstats.winsorize(X_data[col], limits=[0.01, 0.01])\n",
    "        \n",
    "        return X_data\n",
    "    \n",
    "    # Apply preprocessing to all splits\n",
    "    X_train = apply_preprocessing(X_train)\n",
    "    X_val = apply_preprocessing(X_val)\n",
    "    X_test = apply_preprocessing(X_test)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "    \n",
    "    # Return the splits\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "\n",
    "class AdaptiveNIDSLayer1:\n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = self._build_autoencoder()\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.Reshape((-1, 1))(x)\n",
    "        \n",
    "        conv1 = layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "        conv2 = layers.Conv1D(32, 3, activation='relu', padding='same')(conv1)\n",
    "        res = layers.Conv1D(32, 1, padding='same')(conv1)\n",
    "        x = layers.Add()([conv2, res])\n",
    "        x = layers.GlobalAveragePooling1D(name=\"gap_layer\")(x)\n",
    "        x = layers.Dense(64, activation=mish, kernel_regularizer=regularizers.l1(0.0005))(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        encoded = layers.Dense(self.latent_dim, activation='linear')(x)\n",
    "        \n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(self.latent_dim * 2, return_sequences=True, recurrent_dropout=0.25)(x)\n",
    "        decoded = layers.TimeDistributed(layers.Dense(1, activation='linear'))(x)\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        autoencoder.compile(optimizer=keras.optimizers.Adam(1e-4), loss='mse')\n",
    "        return autoencoder\n",
    "\n",
    "    def train(self, X_train, X_val, epochs=50):\n",
    "        lr_schedule = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        return self.model.fit(\n",
    "            X_train, X_train, epochs=epochs, batch_size=64,\n",
    "            validation_data=(X_val, X_val), callbacks=[lr_schedule, early_stopping])\n",
    "\n",
    "    def detect_anomalies(self, X_data, threshold=0.03):\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        return X_data[errors > threshold], np.where(errors > threshold)[0]\n",
    "\n",
    "    def extract_features(self, X_anomalies):\n",
    "        feature_extractor = keras.Model(\n",
    "            inputs=self.model.input, outputs=self.model.get_layer(\"gap_layer\").output)\n",
    "        return feature_extractor.predict(X_anomalies)\n",
    "\n",
    "def apply_pca(X_features, variance_threshold=0.99):\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    X_features_pca = pca.fit_transform(X_features)\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "    return X_features_pca, pca\n",
    "\n",
    "def select_features(X_features, y_labels, n_features=15):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_features, y_labels)\n",
    "    feature_indices = np.argsort(rf.feature_importances_)[::-1][:n_features]\n",
    "    return X_features[:, feature_indices], feature_indices\n",
    "\n",
    "def create_sequences(data, labels=None, seq_length=10, normalize=True):\n",
    "    data = np.array(data)\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        joblib.dump(scaler, 'sequence_scaler.pkl')\n",
    "    sequences, seq_labels = [], []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        if labels is not None:\n",
    "            seq_labels.append(labels[i + seq_length - 1])\n",
    "    sequences = np.array(sequences)\n",
    "    \n",
    "    # Ensure sequences contain more than one class\n",
    "    if labels is not None:\n",
    "        unique_classes = np.unique(seq_labels)\n",
    "        if len(unique_classes) < 2:\n",
    "            raise ValueError(f\"The target 'seq_labels' needs to have more than 1 class. Got {len(unique_classes)} class instead.\")\n",
    "    \n",
    "    return (sequences, np.array(seq_labels)) if labels is not None else sequences\n",
    "\n",
    "def balance_sequences_with_adasyn(X_sequences, y_sequences, sampling_strategy=0.8):\n",
    "    # Ensure y_sequences contains more than one class\n",
    "    unique_classes = np.unique(y_sequences)\n",
    "    if len(unique_classes) < 2:\n",
    "        raise ValueError(f\"The target 'y_sequences' needs to have more than 1 class. Got {len(unique_classes)} class instead.\")\n",
    "    \n",
    "    y_sequences = y_sequences.astype(int)\n",
    "    original_shape = X_sequences.shape\n",
    "    X_seq_2d = X_sequences.reshape(X_sequences.shape[0], -1)\n",
    "    \n",
    "    adasyn = ADASYN(random_state=42, sampling_strategy=sampling_strategy, n_neighbors=5)\n",
    "    X_seq_balanced, y_seq_balanced = adasyn.fit_resample(X_seq_2d, y_sequences)\n",
    "    \n",
    "    return X_seq_balanced.reshape(-1, original_shape[1], original_shape[2]), y_seq_balanced\n",
    "\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim)) \n",
    "        \n",
    "        x = layers.Conv1D(64, kernel_size=3, padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        for _ in range(3):\n",
    "            conv = layers.Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "            conv = layers.BatchNormalization()(conv)\n",
    "            conv = layers.Activation('relu')(conv)\n",
    "            res = layers.Conv1D(128, kernel_size=1, padding='same')(x)\n",
    "            x = layers.Add()([conv, res])\n",
    "            x = layers.SpatialDropout1D(0.3)(x)\n",
    "        \n",
    "        attn_output = layers.MultiHeadAttention(\n",
    "            num_heads=4, key_dim=32, dropout=0.2\n",
    "        )(x, x)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        \n",
    "        lstm_units = 128\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units//2))(x)\n",
    "        \n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-4),\n",
    "            loss=weighted_focal_loss(gamma=2.0, alpha=0.25, class_weights={0: 2.0, 1: 1.0}),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=64):\n",
    "        initial_lr = 5e-4\n",
    "        warmup_epochs = 10\n",
    "        hold_epochs = 5\n",
    "        \n",
    "        def lr_schedule(epoch):\n",
    "            if epoch < warmup_epochs:\n",
    "                return initial_lr * ((epoch + 1) / warmup_epochs)\n",
    "            elif epoch < warmup_epochs + hold_epochs:\n",
    "                return initial_lr\n",
    "            else:\n",
    "                decay_epochs = epochs - warmup_epochs - hold_epochs\n",
    "                return initial_lr * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs - hold_epochs) / decay_epochs))\n",
    "        \n",
    "        class F1ScoreCallback(keras.callbacks.Callback):\n",
    "            def __init__(self, validation_data, patience=8):\n",
    "                super().__init__()\n",
    "                self.X_val, self.y_val = validation_data\n",
    "                self.patience = patience\n",
    "                self.best_f1 = 0\n",
    "                self.wait = 0\n",
    "                self.best_weights = None\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                y_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "                f1 = f1_score(self.y_val, y_pred, average='weighted')\n",
    "                logs['val_f1_score'] = f1\n",
    "                print(f\"\\nEpoch {epoch+1}: val_f1_score: {f1:.4f}\")\n",
    "                \n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.wait = 0\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                    self.model.save_weights('best_nids_model.h5')\n",
    "                    print(f\"Best F1 score improved to {f1:.4f}, saving model\")\n",
    "                else:\n",
    "                    self.wait += 1\n",
    "                    if self.wait >= self.patience:\n",
    "                        print(f\"Early stopping triggered. Best F1: {self.best_f1:.4f}\")\n",
    "                        self.model.stop_training = True\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "        def apply_mixup(x_batch, y_batch, alpha=0.2):\n",
    "            batch_size = x_batch.shape[0]\n",
    "            indices = np.random.permutation(batch_size)\n",
    "            x_shuffled = x_batch[indices]\n",
    "            y_shuffled = y_batch[indices]\n",
    "            \n",
    "            n_classes = self.num_classes\n",
    "            y_batch_onehot = np.eye(n_classes)[y_batch]\n",
    "            y_shuffled_onehot = np.eye(n_classes)[y_shuffled]\n",
    "            \n",
    "            lam = np.random.beta(alpha, alpha, batch_size)\n",
    "            lam = np.maximum(lam, 1-lam)\n",
    "            lam = np.expand_dims(lam, axis=(1, 2))\n",
    "            \n",
    "            x_mixed = lam * x_batch + (1-lam) * x_shuffled\n",
    "            \n",
    "            lam_y = lam[:, 0, 0]\n",
    "            lam_y = np.expand_dims(lam_y, axis=1)\n",
    "            y_mixed = lam_y * y_batch_onehot + (1-lam_y) * y_shuffled_onehot\n",
    "            \n",
    "            return x_mixed, y_mixed\n",
    "        \n",
    "        def custom_train():\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "            loss_fn = self.model.loss\n",
    "            \n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "                policy = mixed_precision.Policy('mixed_float16')\n",
    "                mixed_precision.set_global_policy(policy)\n",
    "            \n",
    "            f1_callback = F1ScoreCallback(validation_data=(X_val, y_val))\n",
    "            lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "            \n",
    "            history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': [], 'val_f1_score': []}\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                current_lr = lr_schedule(epoch)\n",
    "                optimizer.learning_rate.assign(current_lr)\n",
    "                \n",
    "                indices = np.random.permutation(len(X_train))\n",
    "                X_shuffled = X_train[indices]\n",
    "                y_shuffled = y_train[indices]\n",
    "                \n",
    "                train_loss = 0\n",
    "                train_acc = 0\n",
    "                num_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "                \n",
    "                for batch in range(num_batches):\n",
    "                    start_idx = batch * batch_size\n",
    "                    end_idx = min((batch + 1) * batch_size, len(X_train))\n",
    "                    \n",
    "                    x_batch = X_shuffled[start_idx:end_idx]\n",
    "                    y_batch = y_shuffled[start_idx:end_idx]\n",
    "                    \n",
    "                    if np.random.random() < 0.7:\n",
    "                        x_batch, y_batch_onehot = apply_mixup(x_batch, y_batch)\n",
    "                        \n",
    "                        with tf.GradientTape() as tape:\n",
    "                            logits = self.model(x_batch, training=True)\n",
    "                            loss_value = tf.reduce_mean(\n",
    "                                tf.keras.losses.categorical_crossentropy(y_batch_onehot, logits)\n",
    "                            )\n",
    "                            if tf.config.list_physical_devices('GPU'):\n",
    "                                loss_value = optimizer.get_scaled_loss(loss_value)\n",
    "                        \n",
    "                        grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                        if tf.config.list_physical_devices('GPU'):\n",
    "                            grads = optimizer.get_unscaled_gradients(grads)\n",
    "                        \n",
    "                        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                        \n",
    "                        train_loss += loss_value\n",
    "                        preds = tf.argmax(logits, axis=1)\n",
    "                        y_true = tf.argmax(y_batch_onehot, axis=1)\n",
    "                        train_acc += tf.reduce_mean(tf.cast(tf.equal(preds, y_true), tf.float32))\n",
    "                    else:\n",
    "                        self.model.train_on_batch(x_batch, y_batch)\n",
    "                \n",
    "                val_loss, val_acc = self.model.evaluate(X_val, y_val, verbose=0)\n",
    "                y_pred = np.argmax(self.model.predict(X_val), axis=1)\n",
    "                val_f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "                \n",
    "                history['loss'].append(train_loss/num_batches)\n",
    "                history['accuracy'].append(train_acc/num_batches)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_accuracy'].append(val_acc)\n",
    "                history['val_f1_score'].append(val_f1)\n",
    "                \n",
    "                f1_callback.on_epoch_end(epoch, {'val_f1_score': val_f1})\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs} - loss: {train_loss/num_batches:.4f} - accuracy: {train_acc/num_batches:.4f} - val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f} - val_f1: {val_f1:.4f} - lr: {current_lr:.6f}\")\n",
    "                \n",
    "                if f1_callback.wait >= f1_callback.patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "            \n",
    "            self.model.load_weights('best_nids_model.h5')\n",
    "            return keras.callbacks.History().set_model(self.model).set_params({'epochs': epochs, 'metrics': list(history.keys())}).set_model_history(history)\n",
    "        \n",
    "        return custom_train()\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        y_pred_probs = self.model.predict(X_test)\n",
    "        \n",
    "        confidence_threshold = 0.95\n",
    "        low_confidence_mask = np.max(y_pred_probs, axis=1) < confidence_threshold\n",
    "        \n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        metrics = {}\n",
    "        for class_idx in range(self.num_classes):\n",
    "            true_positives = cm[class_idx, class_idx]\n",
    "            false_negatives = np.sum(cm[class_idx, :]) - true_positives\n",
    "            false_positives = np.sum(cm[:, class_idx]) - true_positives\n",
    "            true_negatives = np.sum(cm) - true_positives - false_negatives - false_positives\n",
    "            \n",
    "            metrics[class_idx] = {\n",
    "                'detection_rate': true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0,\n",
    "                'false_positive_rate': false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0,\n",
    "                'precision': true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0,\n",
    "                'f1_score': 2 * true_positives / (2 * true_positives + false_positives + false_negatives) if (2 * true_positives + false_positives + false_negatives) > 0 else 0,\n",
    "                'support': np.sum(y_test == class_idx)\n",
    "            }\n",
    "        \n",
    "        from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "        y_test_bin = np.eye(self.num_classes)[y_test]\n",
    "        roc_auc = roc_auc_score(y_test_bin, y_pred_probs, multi_class='ovr', average='weighted')\n",
    "        pr_auc = average_precision_score(y_test_bin, y_pred_probs, average='weighted')\n",
    "        \n",
    "        self._generate_roc_curves(y_test, y_pred_probs)\n",
    "        \n",
    "        self._plot_confusion_matrix(cm, [f\"Class {i}\" for i in range(self.num_classes)])\n",
    "        \n",
    "        return {\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm,\n",
    "            'class_metrics': metrics,\n",
    "            'low_confidence_predictions': sum(low_confidence_mask),\n",
    "            'overall_metrics': {\n",
    "                'accuracy': report['accuracy'],\n",
    "                'weighted_f1': report['weighted avg']['f1-score'],\n",
    "                'macro_f1': report['macro avg']['f1-score'],\n",
    "                'roc_auc': roc_auc,\n",
    "                'pr_auc': pr_auc\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _generate_roc_curves(self, y_test, y_pred_probs):\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        y_test_bin = np.eye(self.num_classes)[y_test]\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig('roc_curves.png')\n",
    "\n",
    "    def _plot_confusion_matrix(self, cm, class_names):\n",
    "        import seaborn as sns\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, \n",
    "                    yticklabels=class_names)\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "\n",
    "    def plot_training_history(self, history):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss', fontsize=12)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy', fontsize=12)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        if 'val_f1_score' in history.history:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(history.history['val_f1_score'], label='Validation F1 Score', color='green')\n",
    "            plt.title('F1 Score', fontsize=12)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.legend()\n",
    "        \n",
    "        if len(history.epoch) > 0:\n",
    "            plt.subplot(2, 2, 4)\n",
    "            lr_values = [self.model.optimizer.learning_rate(i) for i in range(len(history.epoch))]\n",
    "            plt.plot(lr_values, label='Learning Rate', color='purple')\n",
    "            plt.title('Learning Rate Schedule', fontsize=12)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history_extended.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        pd.DataFrame(history.history).to_csv('training_metrics.csv', index=False)\n",
    "\n",
    "def weighted_focal_loss(gamma=2.0, alpha=0.25, class_weights=None):\n",
    "    \"\"\"\n",
    "    Weighted Focal Loss for imbalanced classification.\n",
    "    \n",
    "    Args:\n",
    "        gamma (float): Focusing parameter for modulating the loss.\n",
    "        alpha (float): Balancing parameter for class imbalance.\n",
    "        class_weights (dict): Dictionary of class weights.\n",
    "    \n",
    "    Returns:\n",
    "        A loss function compatible with Keras.\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            weight_vector = tf.zeros_like(y_true, dtype=tf.float32)\n",
    "            for class_idx, weight in class_weights.items():\n",
    "                weight_vector = tf.where(\n",
    "                    tf.equal(y_true, class_idx),\n",
    "                    tf.ones_like(weight_vector) * weight,\n",
    "                    weight_vector\n",
    "                )\n",
    "            class_weight_factor = tf.expand_dims(weight_vector, axis=-1)\n",
    "        else:\n",
    "            class_weight_factor = 1.0\n",
    "            \n",
    "        ce = -tf.reduce_sum(y_true_one_hot * tf.math.log(y_pred), axis=-1)\n",
    "        pt = tf.exp(-ce)\n",
    "        loss = alpha * tf.pow(1 - pt, gamma) * ce * class_weight_factor\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def main():\n",
    "    file_path = \"/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv\"\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(file_path)\n",
    "    \n",
    "    # Train the anomaly detector (Layer 1)\n",
    "    anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[1], latent_dim=16)\n",
    "    layer1_history = anomaly_detector.train(X_train, X_val, epochs=50)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold=0.03)\n",
    "    y_anomalies = y_train.iloc[indices].values.astype(int)\n",
    "    \n",
    "    # Extract features from anomalies\n",
    "    X_features = anomaly_detector.extract_features(X_anomalies)\n",
    "    X_pca, pca = apply_pca(X_features)\n",
    "    X_selected, feature_indices = select_features(X_pca, y_anomalies, n_features=15)\n",
    "    joblib.dump(feature_indices, 'feature_indices_Iteration_3.pkl')\n",
    "    \n",
    "    # Create sequences\n",
    "    X_sequences, y_sequences = create_sequences(X_selected, y_anomalies)\n",
    "    \n",
    "    # Ensure y_sequences contains more than one class\n",
    "    unique_classes = np.unique(y_sequences)\n",
    "    if len(unique_classes) < 2:\n",
    "        raise ValueError(f\"The target 'y_sequences' needs to have more than 1 class. Got {len(unique_classes)} class instead.\")\n",
    "    \n",
    "    X_balanced, y_balanced = balance_sequences_with_adasyn(X_sequences, y_sequences)\n",
    "    \n",
    "    # Detect anomalies in validation data\n",
    "    X_val_anomalies, val_indices = anomaly_detector.detect_anomalies(X_val, threshold=0.03)\n",
    "    y_val_anomalies = y_val.iloc[val_indices].values.astype(int)\n",
    "    X_val_features = anomaly_detector.extract_features(X_val_anomalies)\n",
    "    X_val_pca = pca.transform(X_val_features)\n",
    "    X_val_selected = X_val_pca[:, feature_indices]\n",
    "    X_val_sequences = create_sequences(X_val_selected)\n",
    "    \n",
    "    # Build the classifier (Layer 2)\n",
    "    num_classes = len(np.unique(y_balanced))\n",
    "    classifier = AdaptiveNIDSLayer2(\n",
    "        input_dim=X_balanced.shape[2], \n",
    "        num_classes=num_classes, \n",
    "        seq_length=X_balanced.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the classifier\n",
    "    layer2_history = classifier.train(X_balanced, y_balanced, X_val_sequences, y_val_anomalies)\n",
    "    \n",
    "    # Detect anomalies in test data\n",
    "    X_test_anomalies, test_indices = anomaly_detector.detect_anomalies(X_test, threshold=0.03)\n",
    "    y_test_anomalies = y_test.iloc[test_indices].values.astype(int)\n",
    "    X_test_features = anomaly_detector.extract_features(X_test_anomalies)\n",
    "    X_test_pca = pca.transform(X_test_features)\n",
    "    X_test_selected = X_test_pca[:, feature_indices]\n",
    "    X_test_sequences = create_sequences(X_test_selected)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = classifier.evaluate_model(X_test_sequences, y_test_anomalies)\n",
    "    \n",
    "    # Plot training history\n",
    "    classifier.plot_training_history(layer2_history)\n",
    "    \n",
    "    # Save the models\n",
    "    anomaly_detector.model.save('anomaly_detector_model.h5')\n",
    "    classifier.model.save('classifier_model.h5')\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(f\"Accuracy: {eval_results['overall_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {eval_results['overall_metrics']['weighted_f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {eval_results['overall_metrics']['roc_auc']:.4f}\")\n",
    "    print(f\"PR AUC: {eval_results['overall_metrics']['pr_auc']:.4f}\")\n",
    "    \n",
    "    # Print per-class metrics\n",
    "    print(\"\\nPer-Class Performance:\")\n",
    "    for class_idx, metrics in eval_results['class_metrics'].items():\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Detection Rate: {metrics['detection_rate']:.4f}\")\n",
    "        print(f\"  False Positive Rate: {metrics['false_positive_rate']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "    \n",
    "    # Generate additional visualizations\n",
    "    plot_feature_importance(classifier, X_test_sequences, y_test_anomalies)\n",
    "    \n",
    "    # Save evaluation results to file\n",
    "    with open('evaluation_results.txt', 'w') as f:\n",
    "        f.write(f\"Model Evaluation Results:\\n\")\n",
    "        f.write(f\"Accuracy: {eval_results['overall_metrics']['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1 Score: {eval_results['overall_metrics']['weighted_f1']:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {eval_results['overall_metrics']['roc_auc']:.4f}\\n\")\n",
    "        f.write(f\"PR AUC: {eval_results['overall_metrics']['pr_auc']:.4f}\\n\")\n",
    "        f.write(\"\\nClassification Report:\\n\")\n",
    "        for class_name, metrics in eval_results['classification_report'].items():\n",
    "            if isinstance(metrics, dict):\n",
    "                f.write(f\"{class_name}: {metrics}\\n\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 95ms/step - loss: 126.0267 - val_loss: 179.1744 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 86ms/step - loss: 128.9724 - val_loss: 179.1212 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 86ms/step - loss: 141.2218 - val_loss: 179.0759 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 131.0692 - val_loss: 179.0498 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 90ms/step - loss: 125.4618 - val_loss: 179.0167 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 88ms/step - loss: 113.8070 - val_loss: 178.9796 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 90ms/step - loss: 99.1769 - val_loss: 178.9274 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 99ms/step - loss: 132.5017 - val_loss: 178.9001 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 91ms/step - loss: 131.0127 - val_loss: 178.8929 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 91ms/step - loss: 139.3523 - val_loss: 178.8715 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 92ms/step - loss: 126.5374 - val_loss: 178.8495 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 125.7809 - val_loss: 178.8288 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 98ms/step - loss: 127.0407 - val_loss: 178.7711 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 134.7936 - val_loss: 178.7426 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 141.6781 - val_loss: 178.7372 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 116.4075 - val_loss: 178.5116 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 114.5399 - val_loss: 178.4474 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 135.4779 - val_loss: 178.5443 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 128.6241 - val_loss: 178.3369 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 117.0058 - val_loss: 178.4220 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 136.2039 - val_loss: 178.3445 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 128.4110 - val_loss: 178.3429 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 118.4245 - val_loss: 178.3142 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 98ms/step - loss: 119.7931 - val_loss: 178.3012 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - loss: 107.6621 - val_loss: 178.2661 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 98ms/step - loss: 111.1868 - val_loss: 178.2692 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 125.4070 - val_loss: 178.2782 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 122.1653 - val_loss: 178.2632 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 131.4187 - val_loss: 178.2190 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 98ms/step - loss: 137.5714 - val_loss: 178.2028 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 118.4001 - val_loss: 178.1662 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - loss: 118.0215 - val_loss: 178.3773 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 123.1608 - val_loss: 178.1803 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 133.9193 - val_loss: 178.1062 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 123.9588 - val_loss: 178.1512 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 95ms/step - loss: 140.0869 - val_loss: 178.1745 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 109.1402 - val_loss: 178.1039 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 118.5351 - val_loss: 178.1283 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 95ms/step - loss: 121.1444 - val_loss: 178.0765 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 118.4227 - val_loss: 178.4850 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 112.8398 - val_loss: 178.1498 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 115.2868 - val_loss: 178.3972 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 117.6418 - val_loss: 177.9850 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 95ms/step - loss: 141.6151 - val_loss: 178.0163 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 97ms/step - loss: 107.1618 - val_loss: 178.3746 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 129.5150 - val_loss: 178.0304 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 129.1754 - val_loss: 178.0199 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 134.2396 - val_loss: 178.1690 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 103.5269 - val_loss: 178.2095 - learning_rate: 5.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 96ms/step - loss: 116.0152 - val_loss: 177.9371 - learning_rate: 5.0000e-05\n",
      "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "property 'model' of 'AdaptiveNIDSLayer2.train.<locals>.F1ScoreCallback' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 779\u001b[39m\n\u001b[32m    776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 566\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    559\u001b[39m classifier = AdaptiveNIDSLayer2(\n\u001b[32m    560\u001b[39m     input_dim=X_balanced.shape[\u001b[32m2\u001b[39m], \n\u001b[32m    561\u001b[39m     num_classes=num_classes, \n\u001b[32m    562\u001b[39m     seq_length=X_balanced.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    563\u001b[39m )\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m layer2_history = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_anomalies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# Detect anomalies in test data\u001b[39;00m\n\u001b[32m    569\u001b[39m X_test_anomalies, test_indices = anomaly_detector.detect_anomalies(X_test, threshold=\u001b[32m0.03\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 398\u001b[39m, in \u001b[36mAdaptiveNIDSLayer2.train\u001b[39m\u001b[34m(self, X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[39m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.load_weights(\u001b[33m'\u001b[39m\u001b[33mbest_nids_model.h5\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m keras.callbacks.History().set_model(\u001b[38;5;28mself\u001b[39m.model).set_params({\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m: epochs, \u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(history.keys())}).set_model_history(history)\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcustom_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 329\u001b[39m, in \u001b[36mAdaptiveNIDSLayer2.train.<locals>.custom_train\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    326\u001b[39m     policy = mixed_precision.Policy(\u001b[33m'\u001b[39m\u001b[33mmixed_float16\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    327\u001b[39m     mixed_precision.set_global_policy(policy)\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m f1_callback = \u001b[43mF1ScoreCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n\u001b[32m    332\u001b[39m history = {\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mval_f1_score\u001b[39m\u001b[33m'\u001b[39m: []}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 273\u001b[39m, in \u001b[36mAdaptiveNIDSLayer2.train.<locals>.F1ScoreCallback.__init__\u001b[39m\u001b[34m(self, validation_data, model, patience)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m.X_val, \u001b[38;5;28mself\u001b[39m.y_val = validation_data\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m = model  \u001b[38;5;66;03m# Pass the model directly to the callback\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28mself\u001b[39m.patience = patience\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m.best_f1 = \u001b[32m0\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: property 'model' of 'AdaptiveNIDSLayer2.train.<locals>.F1ScoreCallback' object has no setter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    df = pd.read_csv(file_path)\n",
    "    y = df['Attack_label']\n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    \n",
    "    Q1, Q3 = X.quantile(0.25), X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 3 * IQR)) | (X > (Q3 + 3 * IQR))).any(axis=1)\n",
    "    X, y = X[mask], y[mask]\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size, \n",
    "        random_state=random_state, stratify=y_train_val)\n",
    "    \n",
    "    def apply_preprocessing(X_data):\n",
    "        log_features = np.log1p(np.abs(X_data))\n",
    "        log_features.columns = [f'log_{col}' for col in X_data.columns]\n",
    "        X_data = pd.concat([X_data, log_features], axis=1)\n",
    "        \n",
    "        X_data['interaction_features'] = X_data.iloc[:, :5].mul(X_data.iloc[:, 5:10]).sum(axis=1)\n",
    "        \n",
    "        percentile_rank = X_data.rank(pct=True)\n",
    "        percentile_rank.columns = [f'percentile_rank_{col}' for col in X_data.columns]\n",
    "        X_data = pd.concat([X_data, percentile_rank], axis=1)\n",
    "        \n",
    "        X_data['mean'] = X_data.mean(axis=1)\n",
    "        X_data['std'] = X_data.std(axis=1)\n",
    "        X_data['skew'] = X_data.skew(axis=1)\n",
    "        X_data['kurt'] = X_data.kurtosis(axis=1)\n",
    "        \n",
    "        from scipy.stats import mstats\n",
    "        for col in X_data.columns:\n",
    "            X_data[col] = mstats.winsorize(X_data[col], limits=[0.01, 0.01])\n",
    "        \n",
    "        return X_data\n",
    "    \n",
    "    X_train = apply_preprocessing(X_train)\n",
    "    X_val = apply_preprocessing(X_val)\n",
    "    X_test = apply_preprocessing(X_test)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "\n",
    "class AdaptiveNIDSLayer1:\n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = self._build_autoencoder()\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.Reshape((-1, 1))(x)\n",
    "        \n",
    "        conv1 = layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "        conv2 = layers.Conv1D(32, 3, activation='relu', padding='same')(conv1)\n",
    "        res = layers.Conv1D(32, 1, padding='same')(conv1)\n",
    "        x = layers.Add()([conv2, res])\n",
    "        x = layers.GlobalAveragePooling1D(name=\"gap_layer\")(x)\n",
    "        x = layers.Dense(64, activation=mish, kernel_regularizer=regularizers.l1(0.0005))(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        encoded = layers.Dense(self.latent_dim, activation='linear')(x)\n",
    "        \n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(self.latent_dim * 2, return_sequences=True, recurrent_dropout=0.25)(x)\n",
    "        decoded = layers.TimeDistributed(layers.Dense(1, activation='linear'))(x)\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        autoencoder.compile(optimizer=keras.optimizers.Adam(1e-4), loss='mse')\n",
    "        return autoencoder\n",
    "\n",
    "    def train(self, X_train, X_val, epochs=50):\n",
    "        lr_schedule = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        return self.model.fit(\n",
    "            X_train, X_train, epochs=epochs, batch_size=64,\n",
    "            validation_data=(X_val, X_val), callbacks=[lr_schedule, early_stopping])\n",
    "\n",
    "    def detect_anomalies(self, X_data, threshold=0.03):\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        return X_data[errors > threshold], np.where(errors > threshold)[0]\n",
    "\n",
    "    def extract_features(self, X_anomalies):\n",
    "        feature_extractor = keras.Model(\n",
    "            inputs=self.model.input, outputs=self.model.get_layer(\"gap_layer\").output)\n",
    "        return feature_extractor.predict(X_anomalies)\n",
    "\n",
    "def apply_pca(X_features, variance_threshold=0.99):\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    X_features_pca = pca.fit_transform(X_features)\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "    return X_features_pca, pca\n",
    "\n",
    "def select_features(X_features, y_labels, n_features=15):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_features, y_labels)\n",
    "    feature_indices = np.argsort(rf.feature_importances_)[::-1][:n_features]\n",
    "    return X_features[:, feature_indices], feature_indices\n",
    "\n",
    "def create_sequences(data, labels=None, seq_length=10, normalize=True):\n",
    "    data = np.array(data)\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        joblib.dump(scaler, 'sequence_scaler.pkl')\n",
    "    sequences, seq_labels = [], []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        if labels is not None:\n",
    "            seq_labels.append(labels[i + seq_length - 1])\n",
    "    sequences = np.array(sequences)\n",
    "    return (sequences, np.array(seq_labels)) if labels is not None else sequences\n",
    "\n",
    "def balance_sequences_with_adasyn(X_sequences, y_sequences, sampling_strategy=0.8):\n",
    "    y_sequences = y_sequences.astype(int)\n",
    "    original_shape = X_sequences.shape\n",
    "    X_seq_2d = X_sequences.reshape(X_sequences.shape[0], -1)\n",
    "    \n",
    "    adasyn = ADASYN(random_state=42, sampling_strategy=sampling_strategy, n_neighbors=5)\n",
    "    X_seq_balanced, y_seq_balanced = adasyn.fit_resample(X_seq_2d, y_sequences)\n",
    "    \n",
    "    return X_seq_balanced.reshape(-1, original_shape[1], original_shape[2]), y_seq_balanced\n",
    "\n",
    "def weighted_focal_loss(gamma=2.0, alpha=0.25, class_weights=None):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            weight_vector = tf.zeros_like(y_true, dtype=tf.float32)\n",
    "            for class_idx, weight in class_weights.items():\n",
    "                weight_vector = tf.where(\n",
    "                    tf.equal(y_true, class_idx),\n",
    "                    tf.ones_like(weight_vector) * weight,\n",
    "                    weight_vector\n",
    "                )\n",
    "            class_weight_factor = tf.expand_dims(weight_vector, axis=-1)\n",
    "        else:\n",
    "            class_weight_factor = 1.0\n",
    "            \n",
    "        ce = -tf.reduce_sum(y_true_one_hot * tf.math.log(y_pred), axis=-1)\n",
    "        pt = tf.exp(-ce)\n",
    "        loss = alpha * tf.pow(1 - pt, gamma) * ce * class_weight_factor\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim)) \n",
    "        \n",
    "        x = layers.Conv1D(64, kernel_size=3, padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        for _ in range(3):\n",
    "            conv = layers.Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "            conv = layers.BatchNormalization()(conv)\n",
    "            conv = layers.Activation('relu')(conv)\n",
    "            res = layers.Conv1D(128, kernel_size=1, padding='same')(x)\n",
    "            x = layers.Add()([conv, res])\n",
    "            x = layers.SpatialDropout1D(0.3)(x)\n",
    "        \n",
    "        attn_output = layers.MultiHeadAttention(\n",
    "            num_heads=4, key_dim=32, dropout=0.2\n",
    "        )(x, x)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        \n",
    "        lstm_units = 128\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units//2))(x)\n",
    "        \n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-4),\n",
    "            loss=weighted_focal_loss(gamma=2.0, alpha=0.25, class_weights={0: 2.0, 1: 1.0}),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=64):\n",
    "        initial_lr = 5e-4\n",
    "        warmup_epochs = 10\n",
    "        hold_epochs = 5\n",
    "        \n",
    "        def lr_schedule(epoch):\n",
    "            if epoch < warmup_epochs:\n",
    "                return initial_lr * ((epoch + 1) / warmup_epochs)\n",
    "            elif epoch < warmup_epochs + hold_epochs:\n",
    "                return initial_lr\n",
    "            else:\n",
    "                decay_epochs = epochs - warmup_epochs - hold_epochs\n",
    "                return initial_lr * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs - hold_epochs) / decay_epochs))\n",
    "        \n",
    "        class F1ScoreCallback(keras.callbacks.Callback):\n",
    "            def __init__(self, validation_data, model, patience=8):\n",
    "                super().__init__()\n",
    "                self.X_val, self.y_val = validation_data\n",
    "                self.model = model  # Pass the model directly to the callback\n",
    "                self.patience = patience\n",
    "                self.best_f1 = 0\n",
    "                self.wait = 0\n",
    "                self.best_weights = None\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                y_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "                f1 = f1_score(self.y_val, y_pred, average='weighted')\n",
    "                logs['val_f1_score'] = f1\n",
    "                print(f\"\\nEpoch {epoch+1}: val_f1_score: {f1:.4f}\")\n",
    "                \n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.wait = 0\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                    self.model.save_weights('best_nids_model.h5')\n",
    "                    print(f\"Best F1 score improved to {f1:.4f}, saving model\")\n",
    "                else:\n",
    "                    self.wait += 1\n",
    "                    if self.wait >= self.patience:\n",
    "                        print(f\"Early stopping triggered. Best F1: {self.best_f1:.4f}\")\n",
    "                        self.model.stop_training = True\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "        def apply_mixup(x_batch, y_batch, alpha=0.2):\n",
    "            batch_size = x_batch.shape[0]\n",
    "            indices = np.random.permutation(batch_size)\n",
    "            x_shuffled = x_batch[indices]\n",
    "            y_shuffled = y_batch[indices]\n",
    "            \n",
    "            n_classes = self.num_classes\n",
    "            y_batch_onehot = np.eye(n_classes)[y_batch]\n",
    "            y_shuffled_onehot = np.eye(n_classes)[y_shuffled]\n",
    "            \n",
    "            lam = np.random.beta(alpha, alpha, batch_size)\n",
    "            lam = np.maximum(lam, 1-lam)\n",
    "            lam = np.expand_dims(lam, axis=(1, 2))\n",
    "            \n",
    "            x_mixed = lam * x_batch + (1-lam) * x_shuffled\n",
    "            \n",
    "            lam_y = lam[:, 0, 0]\n",
    "            lam_y = np.expand_dims(lam_y, axis=1)\n",
    "            y_mixed = lam_y * y_batch_onehot + (1-lam_y) * y_shuffled_onehot\n",
    "            \n",
    "            return x_mixed, y_mixed\n",
    "        \n",
    "        def custom_train():\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "            loss_fn = self.model.loss\n",
    "            \n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "                policy = mixed_precision.Policy('mixed_float16')\n",
    "                mixed_precision.set_global_policy(policy)\n",
    "            \n",
    "            f1_callback = F1ScoreCallback(validation_data=(X_val, y_val), model=self.model)\n",
    "            lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "            \n",
    "            history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': [], 'val_f1_score': []}\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                current_lr = lr_schedule(epoch)\n",
    "                optimizer.learning_rate.assign(current_lr)\n",
    "                \n",
    "                indices = np.random.permutation(len(X_train))\n",
    "                X_shuffled = X_train[indices]\n",
    "                y_shuffled = y_train[indices]\n",
    "                \n",
    "                train_loss = 0\n",
    "                train_acc = 0\n",
    "                num_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "                \n",
    "                for batch in range(num_batches):\n",
    "                    start_idx = batch * batch_size\n",
    "                    end_idx = min((batch + 1) * batch_size, len(X_train))\n",
    "                    \n",
    "                    x_batch = X_shuffled[start_idx:end_idx]\n",
    "                    y_batch = y_shuffled[start_idx:end_idx]\n",
    "                    \n",
    "                    if np.random.random() < 0.7:\n",
    "                        x_batch, y_batch_onehot = apply_mixup(x_batch, y_batch)\n",
    "                        \n",
    "                        with tf.GradientTape() as tape:\n",
    "                            logits = self.model(x_batch, training=True)\n",
    "                            loss_value = tf.reduce_mean(\n",
    "                                tf.keras.losses.categorical_crossentropy(y_batch_onehot, logits)\n",
    "                            )\n",
    "                            if tf.config.list_physical_devices('GPU'):\n",
    "                                loss_value = optimizer.get_scaled_loss(loss_value)\n",
    "                        \n",
    "                        grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                        if tf.config.list_physical_devices('GPU'):\n",
    "                            grads = optimizer.get_unscaled_gradients(grads)\n",
    "                        \n",
    "                        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                        \n",
    "                        train_loss += loss_value\n",
    "                        preds = tf.argmax(logits, axis=1)\n",
    "                        y_true = tf.argmax(y_batch_onehot, axis=1)\n",
    "                        train_acc += tf.reduce_mean(tf.cast(tf.equal(preds, y_true), tf.float32))\n",
    "                    else:\n",
    "                        self.model.train_on_batch(x_batch, y_batch)\n",
    "                \n",
    "                val_loss, val_acc = self.model.evaluate(X_val, y_val, verbose=0)\n",
    "                y_pred = np.argmax(self.model.predict(X_val), axis=1)\n",
    "                val_f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "                \n",
    "                history['loss'].append(train_loss/num_batches)\n",
    "                history['accuracy'].append(train_acc/num_batches)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_accuracy'].append(val_acc)\n",
    "                history['val_f1_score'].append(val_f1)\n",
    "                \n",
    "                f1_callback.on_epoch_end(epoch, {'val_f1_score': val_f1})\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs} - loss: {train_loss/num_batches:.4f} - accuracy: {train_acc/num_batches:.4f} - val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f} - val_f1: {val_f1:.4f} - lr: {current_lr:.6f}\")\n",
    "                \n",
    "                if f1_callback.wait >= f1_callback.patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "            \n",
    "            self.model.load_weights('best_nids_model.h5')\n",
    "            return keras.callbacks.History().set_model(self.model).set_params({'epochs': epochs, 'metrics': list(history.keys())}).set_model_history(history)\n",
    "        \n",
    "        return custom_train()\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        y_pred_probs = self.model.predict(X_test)\n",
    "        \n",
    "        confidence_threshold = 0.95\n",
    "        low_confidence_mask = np.max(y_pred_probs, axis=1) < confidence_threshold\n",
    "        \n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        metrics = {}\n",
    "        for class_idx in range(self.num_classes):\n",
    "            true_positives = cm[class_idx, class_idx]\n",
    "            false_negatives = np.sum(cm[class_idx, :]) - true_positives\n",
    "            false_positives = np.sum(cm[:, class_idx]) - true_positives\n",
    "            true_negatives = np.sum(cm) - true_positives - false_negatives - false_positives\n",
    "            \n",
    "            metrics[class_idx] = {\n",
    "                'detection_rate': true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0,\n",
    "                'false_positive_rate': false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0,\n",
    "                'precision': true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0,\n",
    "                'f1_score': 2 * true_positives / (2 * true_positives + false_positives + false_negatives) if (2 * true_positives + false_positives + false_negatives) > 0 else 0,\n",
    "                'support': np.sum(y_test == class_idx)\n",
    "            }\n",
    "        \n",
    "        from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "        y_test_bin = np.eye(self.num_classes)[y_test]\n",
    "        roc_auc = roc_auc_score(y_test_bin, y_pred_probs, multi_class='ovr', average='weighted')\n",
    "        pr_auc = average_precision_score(y_test_bin, y_pred_probs, average='weighted')\n",
    "        \n",
    "        self._generate_roc_curves(y_test, y_pred_probs)\n",
    "        \n",
    "        self._plot_confusion_matrix(cm, [f\"Class {i}\" for i in range(self.num_classes)])\n",
    "        \n",
    "        return {\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm,\n",
    "            'class_metrics': metrics,\n",
    "            'low_confidence_predictions': sum(low_confidence_mask),\n",
    "            'overall_metrics': {\n",
    "                'accuracy': report['accuracy'],\n",
    "                'weighted_f1': report['weighted avg']['f1-score'],\n",
    "                'macro_f1': report['macro avg']['f1-score'],\n",
    "                'roc_auc': roc_auc,\n",
    "                'pr_auc': pr_auc\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _generate_roc_curves(self, y_test, y_pred_probs):\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        y_test_bin = np.eye(self.num_classes)[y_test]\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig('roc_curves.png')\n",
    "\n",
    "    def _plot_confusion_matrix(self, cm, class_names):\n",
    "        import seaborn as sns\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, \n",
    "                    yticklabels=class_names)\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "\n",
    "    def plot_training_history(self, history):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss', fontsize=12)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy', fontsize=12)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        if 'val_f1_score' in history.history:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(history.history['val_f1_score'], label='Validation F1 Score', color='green')\n",
    "            plt.title('F1 Score', fontsize=12)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.legend()\n",
    "        \n",
    "        if len(history.epoch) > 0:\n",
    "            plt.subplot(2, 2, 4)\n",
    "            lr_values = [self.model.optimizer.learning_rate(i) for i in range(len(history.epoch))]\n",
    "            plt.plot(lr_values, label='Learning Rate', color='purple')\n",
    "            plt.title('Learning Rate Schedule', fontsize=12)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history_extended.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        pd.DataFrame(history.history).to_csv('training_metrics.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    file_path = \"/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv\"\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(file_path)\n",
    "    \n",
    "    # Train the anomaly detector (Layer 1)\n",
    "    anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[1], latent_dim=16)\n",
    "    layer1_history = anomaly_detector.train(X_train, X_val, epochs=50)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold=0.03)\n",
    "    y_anomalies = y_train.iloc[indices].values.astype(int)\n",
    "    \n",
    "    # Extract features from anomalies\n",
    "    X_features = anomaly_detector.extract_features(X_anomalies)\n",
    "    X_pca, pca = apply_pca(X_features)\n",
    "    X_selected, feature_indices = select_features(X_pca, y_anomalies, n_features=15)\n",
    "    joblib.dump(feature_indices, 'feature_indices_Iteration_3.pkl')\n",
    "    \n",
    "    # Create sequences\n",
    "    X_sequences, y_sequences = create_sequences(X_selected, y_anomalies)\n",
    "    X_balanced, y_balanced = balance_sequences_with_adasyn(X_sequences, y_sequences)\n",
    "    \n",
    "    # Detect anomalies in validation data\n",
    "    X_val_anomalies, val_indices = anomaly_detector.detect_anomalies(X_val, threshold=0.03)\n",
    "    y_val_anomalies = y_val.iloc[val_indices].values.astype(int)\n",
    "    X_val_features = anomaly_detector.extract_features(X_val_anomalies)\n",
    "    X_val_pca = pca.transform(X_val_features)\n",
    "    X_val_selected = X_val_pca[:, feature_indices]\n",
    "    X_val_sequences = create_sequences(X_val_selected)\n",
    "    \n",
    "    # Build the classifier (Layer 2)\n",
    "    num_classes = len(np.unique(y_balanced))\n",
    "    classifier = AdaptiveNIDSLayer2(\n",
    "        input_dim=X_balanced.shape[2], \n",
    "        num_classes=num_classes, \n",
    "        seq_length=X_balanced.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the classifier\n",
    "    layer2_history = classifier.train(X_balanced, y_balanced, X_val_sequences, y_val_anomalies)\n",
    "    \n",
    "    # Detect anomalies in test data\n",
    "    X_test_anomalies, test_indices = anomaly_detector.detect_anomalies(X_test, threshold=0.03)\n",
    "    y_test_anomalies = y_test.iloc[test_indices].values.astype(int)\n",
    "    X_test_features = anomaly_detector.extract_features(X_test_anomalies)\n",
    "    X_test_pca = pca.transform(X_test_features)\n",
    "    X_test_selected = X_test_pca[:, feature_indices]\n",
    "    X_test_sequences = create_sequences(X_test_selected)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = classifier.evaluate_model(X_test_sequences, y_test_anomalies)\n",
    "    \n",
    "    # Plot training history\n",
    "    classifier.plot_training_history(layer2_history)\n",
    "    \n",
    "    # Save the models\n",
    "    anomaly_detector.model.save('anomaly_detector_model.h5')\n",
    "    classifier.model.save('classifier_model.h5')\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(f\"Accuracy: {eval_results['overall_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {eval_results['overall_metrics']['weighted_f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {eval_results['overall_metrics']['roc_auc']:.4f}\")\n",
    "    print(f\"PR AUC: {eval_results['overall_metrics']['pr_auc']:.4f}\")\n",
    "    \n",
    "    # Print per-class metrics\n",
    "    print(\"\\nPer-Class Performance:\")\n",
    "    for class_idx, metrics in eval_results['class_metrics'].items():\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Detection Rate: {metrics['detection_rate']:.4f}\")\n",
    "        print(f\"  False Positive Rate: {metrics['false_positive_rate']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "    \n",
    "    # Generate additional visualizations\n",
    "    plot_feature_importance(classifier, X_test_sequences, y_test_anomalies)\n",
    "    \n",
    "    # Save evaluation results to file\n",
    "    with open('evaluation_results.txt', 'w') as f:\n",
    "        f.write(f\"Model Evaluation Results:\\n\")\n",
    "        f.write(f\"Accuracy: {eval_results['overall_metrics']['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1 Score: {eval_results['overall_metrics']['weighted_f1']:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {eval_results['overall_metrics']['roc_auc']:.4f}\\n\")\n",
    "        f.write(f\"PR AUC: {eval_results['overall_metrics']['pr_auc']:.4f}\\n\")\n",
    "        f.write(\"\\nClassification Report:\\n\")\n",
    "        for class_name, metrics in eval_results['classification_report'].items():\n",
    "            if isinstance(metrics, dict):\n",
    "                f.write(f\"{class_name}: {metrics}\\n\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "def plot_feature_importance(classifier, X_test, y_test):\n",
    "    # Use Integrated Gradients for feature attribution\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.models import Model\n",
    "        \n",
    "        # Create a model that returns both predictions and gradients\n",
    "        grad_model = Model(\n",
    "            inputs=classifier.model.inputs,\n",
    "            outputs=[classifier.model.outputs[0], \n",
    "                     tf.gradients(classifier.model.outputs[0], classifier.model.inputs)[0]]\n",
    "        )\n",
    "        \n",
    "        # Sample subset for analysis\n",
    "        sample_indices = np.random.choice(len(X_test), min(100, len(X_test)), replace=False)\n",
    "        X_sample = X_test[sample_indices]\n",
    "        y_sample = y_test[sample_indices]\n",
    "        \n",
    "        # Calculate integrated gradients\n",
    "        predictions, grads = grad_model.predict(X_sample)\n",
    "        feature_importance = np.mean(np.abs(grads), axis=0)\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(feature_importance.T, aspect='auto', cmap='viridis')\n",
    "        plt.colorbar(label='Feature Importance')\n",
    "        plt.xlabel('Sequence Position')\n",
    "        plt.ylabel('Feature Index')\n",
    "        plt.title('Feature Importance Across Sequence')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300)\n",
    "        \n",
    "        # Plot average importance by position in sequence\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        avg_importance = np.mean(feature_importance, axis=1)\n",
    "        plt.bar(range(len(avg_importance)), avg_importance)\n",
    "        plt.xlabel('Sequence Position')\n",
    "        plt.ylabel('Average Importance')\n",
    "        plt.title('Feature Importance by Sequence Position')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('position_importance.png', dpi=300)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate feature importance visualization: {e}\")\n",
    "\n",
    "def predict_single_sample(anomaly_detector, classifier, sample, pca, feature_indices, threshold=0.03):\n",
    "    \n",
    "    # Reshape sample if needed\n",
    "    if len(sample.shape) == 1:\n",
    "        sample = sample.reshape(1, -1)\n",
    "    \n",
    "    # Check if sample is anomalous\n",
    "    reconstructed = anomaly_detector.model.predict(sample)\n",
    "    error = np.mean(np.square(sample - reconstructed), axis=1)[0]\n",
    "    \n",
    "    if error <= threshold:\n",
    "        return {\n",
    "            'anomaly': False,\n",
    "            'error': error,\n",
    "            'class': None,\n",
    "            'confidence': None\n",
    "        }\n",
    "    \n",
    "    # Extract features and apply transformations\n",
    "    features = anomaly_detector.extract_features(sample)\n",
    "    features_pca = pca.transform(features)\n",
    "    selected_features = features_pca[:, feature_indices]\n",
    "    \n",
    "    # Create sequence\n",
    "    sequence = create_sequences(selected_features)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = classifier.model.predict(sequence)[0]\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    confidence = predictions[predicted_class]\n",
    "    \n",
    "    return {\n",
    "        'anomaly': True,\n",
    "        'error': error,\n",
    "        'class': int(predicted_class),\n",
    "        'confidence': float(confidence),\n",
    "        'class_probabilities': {i: float(p) for i, p in enumerate(predictions)}\n",
    "    }\n",
    "\n",
    "def batch_prediction(anomaly_detector, classifier, samples, pca, feature_indices, threshold=0.03):\n",
    "    \n",
    "    # Detect anomalies\n",
    "    reconstructed = anomaly_detector.model.predict(samples)\n",
    "    errors = np.mean(np.square(samples - reconstructed), axis=1)\n",
    "    anomaly_indices = np.where(errors > threshold)[0]\n",
    "    \n",
    "    results = {\n",
    "        'total_samples': len(samples),\n",
    "        'anomalies_detected': len(anomaly_indices),\n",
    "        'anomaly_indices': anomaly_indices.tolist(),\n",
    "        'errors': errors.tolist(),\n",
    "        'predictions': [None] * len(samples)\n",
    "    }\n",
    "    \n",
    "    if len(anomaly_indices) > 0:\n",
    "        # Extract features from anomalies\n",
    "        anomalies = samples[anomaly_indices]\n",
    "        features = anomaly_detector.extract_features(anomalies)\n",
    "        features_pca = pca.transform(features)\n",
    "        selected_features = features_pca[:, feature_indices]\n",
    "        \n",
    "        # Create sequences\n",
    "        sequences = create_sequences(selected_features)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = classifier.model.predict(sequences)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        confidences = np.max(predictions, axis=1)\n",
    "        \n",
    "        # Store predictions\n",
    "        for i, idx in enumerate(anomaly_indices):\n",
    "            results['predictions'][idx] = {\n",
    "                'class': int(predicted_classes[i]),\n",
    "                'confidence': float(confidences[i]),\n",
    "                'class_probabilities': {j: float(p) for j, p in enumerate(predictions[i])}\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 902\u001b[39m\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 680\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;66;03m# Train the anomaly detector (Layer 1)\u001b[39;00m\n\u001b[32m    679\u001b[39m anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[\u001b[32m1\u001b[39m], latent_dim=\u001b[32m16\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m layer1_history = \u001b[43manomaly_detector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[38;5;66;03m# Detect anomalies\u001b[39;00m\n\u001b[32m    683\u001b[39m X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold=\u001b[32m0.03\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mAdaptiveNIDSLayer1.train\u001b[39m\u001b[34m(self, X_train, X_val, epochs, batch_size)\u001b[39m\n\u001b[32m    208\u001b[39m early_stopping = keras.callbacks.EarlyStopping(\n\u001b[32m    209\u001b[39m     monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    210\u001b[39m     patience=\u001b[32m10\u001b[39m,\n\u001b[32m    211\u001b[39m     restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    212\u001b[39m     min_delta=\u001b[32m0.0001\u001b[39m\n\u001b[32m    213\u001b[39m )\n\u001b[32m    215\u001b[39m reduce_lr = keras.callbacks.ReduceLROnPlateau(\n\u001b[32m    216\u001b[39m     monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    217\u001b[39m     factor=\u001b[32m0.3\u001b[39m,\n\u001b[32m    218\u001b[39m     patience=\u001b[32m5\u001b[39m,\n\u001b[32m    219\u001b[39m     min_lr=\u001b[32m1e-6\u001b[39m\n\u001b[32m    220\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    229\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    913\u001b[39m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[32m    914\u001b[39m   filtered_flat_args = (\n\u001b[32m    915\u001b[39m       \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.unpack_inputs(\n\u001b[32m    916\u001b[39m           bound_args\n\u001b[32m    917\u001b[39m       )\n\u001b[32m    918\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    920\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[32m    925\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    df = pd.read_csv(file_path)\n",
    "    y = df['Attack_label']\n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    \n",
    "    Q1, Q3 = X.quantile(0.25), X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 3 * IQR)) | (X > (Q3 + 3 * IQR))).any(axis=1)\n",
    "    X, y = X[mask], y[mask]\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size, \n",
    "        random_state=random_state, stratify=y_train_val)\n",
    "    \n",
    "    def apply_preprocessing(X_data):\n",
    "        log_features = np.log1p(np.abs(X_data))\n",
    "        log_features.columns = [f'log_{col}' for col in X_data.columns]\n",
    "        X_data = pd.concat([X_data, log_features], axis=1)\n",
    "        \n",
    "        X_data['interaction_features'] = X_data.iloc[:, :5].mul(X_data.iloc[:, 5:10]).sum(axis=1)\n",
    "        \n",
    "        percentile_rank = X_data.rank(pct=True)\n",
    "        percentile_rank.columns = [f'percentile_rank_{col}' for col in X_data.columns]\n",
    "        X_data = pd.concat([X_data, percentile_rank], axis=1)\n",
    "        \n",
    "        X_data['mean'] = X_data.mean(axis=1)\n",
    "        X_data['std'] = X_data.std(axis=1)\n",
    "        X_data['skew'] = X_data.skew(axis=1)\n",
    "        X_data['kurt'] = X_data.kurtosis(axis=1)\n",
    "        \n",
    "        from scipy.stats import mstats\n",
    "        for col in X_data.columns:\n",
    "            X_data[col] = mstats.winsorize(X_data[col], limits=[0.01, 0.01])\n",
    "        \n",
    "        return X_data\n",
    "    \n",
    "    X_train = apply_preprocessing(X_train)\n",
    "    X_val = apply_preprocessing(X_val)\n",
    "    X_test = apply_preprocessing(X_test)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "\n",
    "class AdaptiveNIDSLayer1:\n",
    "    def __init__(self, input_dim, latent_dim=16, learning_rate=5e-4):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_enhanced_autoencoder()\n",
    "\n",
    "    def _build_enhanced_autoencoder(self):\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Advanced Preprocessing\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.Reshape((-1, 1))(x)\n",
    "        \n",
    "        # Enhanced Feature Extraction with Residual Connections\n",
    "        x = self._residual_conv_block(x, filters=16, kernel_size=3)\n",
    "        x = self._residual_conv_block(x, filters=32, kernel_size=3)\n",
    "        \n",
    "        # Self-Attention Mechanism\n",
    "        x = self._self_attention_block(x)\n",
    "        \n",
    "        # Global Feature Aggregation\n",
    "        x = layers.GlobalAveragePooling1D(name=\"gap_layer\")(x)\n",
    "        \n",
    "        # Structured Latent Space with Normalization\n",
    "        x = layers.Dense(64, activation=mish, \n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=0.0005, l2=0.00075))(x)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='linear',\n",
    "            kernel_regularizer=regularizers.l1(0.0005),\n",
    "            activity_regularizer=regularizers.l2(0.0005)\n",
    "        )(x)\n",
    "        \n",
    "        # Decoder with Attention-Guided Reconstruction\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim * 2,  \n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=0.25\n",
    "        )(x)\n",
    "        \n",
    "        x = self._attention_decoder(x, encoded)\n",
    "        \n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Use direct learning rate value instead of a schedule\n",
    "        # This allows ReduceLROnPlateau to work properly\n",
    "        autoencoder.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                learning_rate=self.learning_rate, \n",
    "                clipnorm=1.0\n",
    "            ),\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mae', keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def _residual_conv_block(self, x, filters, kernel_size):\n",
    "        shortcut = x\n",
    "        \n",
    "        x = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=mish,\n",
    "            padding='same',\n",
    "            kernel_regularizer=regularizers.l2(0.00075)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.SpatialDropout1D(0.2)(x)\n",
    "        \n",
    "        x = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=mish,\n",
    "            padding='same',\n",
    "            kernel_regularizer=regularizers.l2(0.00075)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Match dimensions if needed\n",
    "        if K.int_shape(shortcut)[-1] != K.int_shape(x)[-1]:\n",
    "            shortcut = layers.Conv1D(\n",
    "                filters=filters,\n",
    "                kernel_size=1,\n",
    "                activation=mish,\n",
    "                padding='same'\n",
    "            )(shortcut)\n",
    "            \n",
    "        x = layers.Add()([x, shortcut])\n",
    "        return x\n",
    "    \n",
    "    def _self_attention_block(self, x):\n",
    "        attention = layers.Attention()([x, x])\n",
    "        return layers.Add()([x, attention])\n",
    "    \n",
    "    def _attention_decoder(self, x, encoded):\n",
    "        attention = layers.Dense(x.shape[-1], activation='softmax')(encoded)\n",
    "        attention = layers.RepeatVector(self.input_dim)(attention)\n",
    "        return layers.Multiply()([x, attention])\n",
    "    \n",
    "    # This method is no longer used since we're using a fixed learning rate\n",
    "    # But kept for reference\n",
    "    def _warmup_schedule(self, lr_schedule, warmup_steps):\n",
    "        def warmup_fn(step):\n",
    "            return tf.cond(\n",
    "                step < warmup_steps,\n",
    "                lambda: tf.cast(step, tf.float32) / warmup_steps * lr_schedule(step),\n",
    "                lambda: lr_schedule(step - warmup_steps)\n",
    "            )\n",
    "        \n",
    "        class WarmupSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            def __init__(self, warmup_function):\n",
    "                super(WarmupSchedule, self).__init__()\n",
    "                self.warmup_function = warmup_function\n",
    "                \n",
    "            def __call__(self, step):\n",
    "                return self.warmup_function(step)\n",
    "            \n",
    "            def get_config(self):\n",
    "                return {}\n",
    "                \n",
    "        return WarmupSchedule(warmup_fn)\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=64):\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.0001\n",
    "        )\n",
    "        \n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.3,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, method='ensemble'):\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        \n",
    "        if method == 'ensemble':\n",
    "            percentile_threshold = np.percentile(reconstruction_errors, 90)\n",
    "            iqr_threshold = np.median(reconstruction_errors) + 1.5 * (np.percentile(reconstruction_errors, 75) - np.percentile(reconstruction_errors, 25))\n",
    "            std_threshold = np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors)\n",
    "            \n",
    "            thresholds = np.array([percentile_threshold, iqr_threshold, std_threshold])\n",
    "            weights = np.array([0.4, 0.3, 0.3])\n",
    "            threshold = np.dot(thresholds, weights)\n",
    "        elif method == 'percentile':\n",
    "            threshold = np.percentile(reconstruction_errors, 90)\n",
    "        elif method == 'median_plus_iqr':\n",
    "            median = np.median(reconstruction_errors)\n",
    "            Q1 = np.percentile(reconstruction_errors, 25)\n",
    "            Q3 = np.percentile(reconstruction_errors, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            threshold = median + 1.5 * IQR\n",
    "        elif method == 'mean_plus_std':\n",
    "            threshold = np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid threshold calculation method\")\n",
    "        \n",
    "        return threshold\n",
    "    \n",
    "    def detect_anomalies(self, X_data, threshold=None):\n",
    "        if threshold is None:\n",
    "            threshold = self.calculate_threshold(X_data)\n",
    "            \n",
    "        reconstructions = self.model.predict(X_data)\n",
    "        feature_importance = self.calculate_feature_importance(X_data)\n",
    "        weighted_errors = np.mean(np.square(X_data - reconstructions) * feature_importance, axis=1)\n",
    "        \n",
    "        anomalies = weighted_errors > threshold\n",
    "        return X_data[anomalies], np.where(anomalies)[0]\n",
    "    \n",
    "    def calculate_feature_importance(self, X_test):\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        errors = np.square(X_test - reconstructions)\n",
    "        \n",
    "        feature_errors = np.mean(errors, axis=0)\n",
    "        feature_importance = feature_errors / np.sum(feature_errors)\n",
    "        return feature_importance.reshape(1, -1)\n",
    "    \n",
    "    def extract_features(self, X_anomalies):\n",
    "        feature_extractor = keras.Model(\n",
    "            inputs=self.model.input, outputs=self.model.get_layer(\"gap_layer\").output)\n",
    "        return feature_extractor.predict(X_anomalies)\n",
    "        \n",
    "    def save_model(self, model_path='enhanced_autoencoder_nids.h5'):\n",
    "        self.model.save(model_path)\n",
    "\n",
    "def apply_pca(X_features, variance_threshold=0.99):\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    X_features_pca = pca.fit_transform(X_features)\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "    return X_features_pca, pca\n",
    "\n",
    "def select_features(X_features, y_labels, n_features=15):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_features, y_labels)\n",
    "    feature_indices = np.argsort(rf.feature_importances_)[::-1][:n_features]\n",
    "    return X_features[:, feature_indices], feature_indices\n",
    "\n",
    "def create_sequences(data, labels=None, seq_length=10, normalize=True):\n",
    "    data = np.array(data)\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        joblib.dump(scaler, 'sequence_scaler.pkl')\n",
    "    sequences, seq_labels = [], []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        if labels is not None:\n",
    "            seq_labels.append(labels[i + seq_length - 1])\n",
    "    sequences = np.array(sequences)\n",
    "    return (sequences, np.array(seq_labels)) if labels is not None else sequences\n",
    "\n",
    "def balance_sequences_with_adasyn(X_sequences, y_sequences, sampling_strategy=0.8):\n",
    "    unique_classes = np.unique(y_sequences)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(\"Only one class present in y_sequences. Skipping ADASYN.\")\n",
    "        return X_sequences, y_sequences\n",
    "\n",
    "    y_sequences = y_sequences.astype(int)\n",
    "    original_shape = X_sequences.shape\n",
    "    X_seq_2d = X_sequences.reshape(X_sequences.shape[0], -1)\n",
    "    \n",
    "    adasyn = ADASYN(random_state=42, sampling_strategy=sampling_strategy, n_neighbors=5)\n",
    "    X_seq_balanced, y_seq_balanced = adasyn.fit_resample(X_seq_2d, y_sequences)\n",
    "    \n",
    "    return X_seq_balanced.reshape(-1, original_shape[1], original_shape[2]), y_seq_balanced\n",
    "\n",
    "def weighted_focal_loss(gamma=2.0, alpha=0.25, class_weights=None):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            weight_vector = tf.zeros_like(y_true, dtype=tf.float32)\n",
    "            for class_idx, weight in class_weights.items():\n",
    "                weight_vector = tf.where(\n",
    "                    tf.equal(y_true, class_idx),\n",
    "                    tf.ones_like(weight_vector) * weight,\n",
    "                    weight_vector\n",
    "                )\n",
    "            class_weight_factor = tf.expand_dims(weight_vector, axis=-1)\n",
    "        else:\n",
    "            class_weight_factor = 1.0\n",
    "            \n",
    "        ce = -tf.reduce_sum(y_true_one_hot * tf.math.log(y_pred), axis=-1)\n",
    "        pt = tf.exp(-ce)\n",
    "        loss = alpha * tf.pow(1 - pt, gamma) * ce * class_weight_factor\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim)) \n",
    "        \n",
    "        x = layers.Conv1D(64, kernel_size=3, padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        for _ in range(3):\n",
    "            conv = layers.Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "            conv = layers.BatchNormalization()(conv)\n",
    "            conv = layers.Activation('relu')(conv)\n",
    "            res = layers.Conv1D(128, kernel_size=1, padding='same')(x)\n",
    "            x = layers.Add()([conv, res])\n",
    "            x = layers.SpatialDropout1D(0.3)(x)\n",
    "        \n",
    "        attn_output = layers.MultiHeadAttention(\n",
    "            num_heads=4, key_dim=32, dropout=0.2\n",
    "        )(x, x)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        \n",
    "        lstm_units = 128\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(lstm_units//2))(x)\n",
    "        \n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-4),\n",
    "            loss=weighted_focal_loss(gamma=2.0, alpha=0.25, class_weights={0: 2.0, 1: 1.0}),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=64):\n",
    "        initial_lr = 5e-4\n",
    "        warmup_epochs = 10\n",
    "        hold_epochs = 5\n",
    "        \n",
    "        def lr_schedule(epoch):\n",
    "            if epoch < warmup_epochs:\n",
    "                return initial_lr * ((epoch + 1) / warmup_epochs)\n",
    "            elif epoch < warmup_epochs + hold_epochs:\n",
    "                return initial_lr\n",
    "            else:\n",
    "                decay_epochs = epochs - warmup_epochs - hold_epochs\n",
    "                return initial_lr * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs - hold_epochs) / decay_epochs))\n",
    "        \n",
    "        class F1ScoreCallback(keras.callbacks.Callback):\n",
    "            def __init__(self, validation_data, model, patience=8):\n",
    "                super().__init__()\n",
    "                self.X_val, self.y_val = validation_data\n",
    "                self.model = model  # This will use the setter\n",
    "                self.patience = patience\n",
    "                self.best_f1 = 0\n",
    "                self.wait = 0\n",
    "                self.best_weights = None\n",
    "\n",
    "            @property\n",
    "            def model(self):\n",
    "                return self._model\n",
    "\n",
    "            @model.setter\n",
    "            def model(self, value):\n",
    "                self._model = value\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                y_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "                f1 = f1_score(self.y_val, y_pred, average='weighted')\n",
    "                logs['val_f1_score'] = f1\n",
    "                print(f\"\\nEpoch {epoch+1}: val_f1_score: {f1:.4f}\")\n",
    "                \n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.wait = 0\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                    self.model.save_weights('best_nids_model.h5')\n",
    "                    print(f\"Best F1 score improved to {f1:.4f}, saving model\")\n",
    "                else:\n",
    "                    self.wait += 1\n",
    "                    if self.wait >= self.patience:\n",
    "                        print(f\"Early stopping triggered. Best F1: {self.best_f1:.4f}\")\n",
    "                        self.model.stop_training = True\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "        def apply_mixup(x_batch, y_batch, alpha=0.2):\n",
    "            batch_size = x_batch.shape[0]\n",
    "            indices = np.random.permutation(batch_size)\n",
    "            x_shuffled = x_batch[indices]\n",
    "            y_shuffled = y_batch[indices]\n",
    "            \n",
    "            n_classes = self.num_classes\n",
    "            y_batch_onehot = np.eye(n_classes)[y_batch]\n",
    "            y_shuffled_onehot = np.eye(n_classes)[y_shuffled]\n",
    "            \n",
    "            lam = np.random.beta(alpha, alpha, batch_size)\n",
    "            lam = np.maximum(lam, 1-lam)\n",
    "            lam = np.expand_dims(lam, axis=(1, 2))\n",
    "            \n",
    "            x_mixed = lam * x_batch + (1-lam) * x_shuffled\n",
    "            \n",
    "            lam_y = lam[:, 0, 0]\n",
    "            lam_y = np.expand_dims(lam_y, axis=1)\n",
    "            y_mixed = lam_y * y_batch_onehot + (1-lam_y) * y_shuffled_onehot\n",
    "            \n",
    "            return x_mixed, y_mixed\n",
    "        \n",
    "        def custom_train():\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "            loss_fn = self.model.loss\n",
    "            \n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "                policy = mixed_precision.Policy('mixed_float16')\n",
    "                mixed_precision.set_global_policy(policy)\n",
    "            \n",
    "            f1_callback = F1ScoreCallback(validation_data=(X_val, y_val), model=self.model)\n",
    "            lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "            \n",
    "            history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': [], 'val_f1_score': []}\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                current_lr = lr_schedule(epoch)\n",
    "                optimizer.learning_rate.assign(current_lr)\n",
    "                \n",
    "                indices = np.random.permutation(len(X_train))\n",
    "                X_shuffled = X_train[indices]\n",
    "                y_shuffled = y_train[indices]\n",
    "                \n",
    "                train_loss = 0\n",
    "                train_acc = 0\n",
    "                num_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "                \n",
    "                for batch in range(num_batches):\n",
    "                    start_idx = batch * batch_size\n",
    "                    end_idx = min((batch + 1) * batch_size, len(X_train))\n",
    "                    \n",
    "                    x_batch = X_shuffled[start_idx:end_idx]\n",
    "                    y_batch = y_shuffled[start_idx:end_idx]\n",
    "                    \n",
    "                    if np.random.random() < 0.7:\n",
    "                        x_batch, y_batch_onehot = apply_mixup(x_batch, y_batch)\n",
    "                        \n",
    "                        with tf.GradientTape() as tape:\n",
    "                            logits = self.model(x_batch, training=True)\n",
    "                            loss_value = tf.reduce_mean(\n",
    "                                tf.keras.losses.categorical_crossentropy(y_batch_onehot, logits)\n",
    "                            )\n",
    "                            if tf.config.list_physical_devices('GPU'):\n",
    "                                loss_value = optimizer.get_scaled_loss(loss_value)\n",
    "                        \n",
    "                        grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                        if tf.config.list_physical_devices('GPU'):\n",
    "                            grads = optimizer.get_unscaled_gradients(grads)\n",
    "                        \n",
    "                        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                        \n",
    "                        train_loss += loss_value\n",
    "                        preds = tf.argmax(logits, axis=1)\n",
    "                        y_true = tf.argmax(y_batch_onehot, axis=1)\n",
    "                        train_acc += tf.reduce_mean(tf.cast(tf.equal(preds, y_true), tf.float32))\n",
    "                    else:\n",
    "                        self.model.train_on_batch(x_batch, y_batch)\n",
    "                \n",
    "                val_loss, val_acc = self.model.evaluate(X_val, y_val, verbose=0)\n",
    "                y_pred = np.argmax(self.model.predict(X_val), axis=1)\n",
    "                val_f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "                \n",
    "                history['loss'].append(train_loss/num_batches)\n",
    "                history['accuracy'].append(train_acc/num_batches)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_accuracy'].append(val_acc)\n",
    "                history['val_f1_score'].append(val_f1)\n",
    "                \n",
    "                f1_callback.on_epoch_end(epoch, {'val_f1_score': val_f1})\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs} - loss: {train_loss/num_batches:.4f} - accuracy: {train_acc/num_batches:.4f} - val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f} - val_f1: {val_f1:.4f} - lr: {current_lr:.6f}\")\n",
    "                \n",
    "                if f1_callback.wait >= f1_callback.patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "            \n",
    "            self.model.load_weights('best_nids_model.h5')\n",
    "            return keras.callbacks.History().set_model(self.model).set_params({'epochs': epochs, 'metrics': list(history.keys())}).set_model_history(history)\n",
    "        \n",
    "        return custom_train()\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        y_pred_probs = self.model.predict(X_test)\n",
    "        \n",
    "        confidence_threshold = 0.95\n",
    "        low_confidence_mask = np.max(y_pred_probs, axis=1) < confidence_threshold\n",
    "        \n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        metrics = {}\n",
    "        for class_idx in range(self.num_classes):\n",
    "            true_positives = cm[class_idx, class_idx]\n",
    "            false_negatives = np.sum(cm[class_idx, :]) - true_positives\n",
    "            false_positives = np.sum(cm[:, class_idx]) - true_positives\n",
    "            true_negatives = np.sum(cm) - true_positives - false_negatives - false_positives\n",
    "            \n",
    "            metrics[class_idx] = {\n",
    "                'detection_rate': true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0,\n",
    "                'false_positive_rate': false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0,\n",
    "                'precision': true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0,\n",
    "                'f1_score': 2 * true_positives / (2 * true_positives + false_positives + false_negatives) if (2 * true_positives + false_positives + false_negatives) > 0 else 0,\n",
    "                'support': np.sum(y_test == class_idx)\n",
    "            }\n",
    "        \n",
    "        from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "        y_test_bin = np.eye(self.num_classes)[y_test]\n",
    "        roc_auc = roc_auc_score(y_test_bin, y_pred_probs, multi_class='ovr', average='weighted')\n",
    "        pr_auc = average_precision_score(y_test_bin, y_pred_probs, average='weighted')\n",
    "        \n",
    "        self._generate_roc_curves(y_test, y_pred_probs)\n",
    "        \n",
    "        self._plot_confusion_matrix(cm, [f\"Class {i}\" for i in range(self.num_classes)])\n",
    "        \n",
    "        return {\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm,\n",
    "            'class_metrics': metrics,\n",
    "            'low_confidence_predictions': sum(low_confidence_mask),\n",
    "            'overall_metrics': {\n",
    "                'accuracy': report['accuracy'],\n",
    "                'weighted_f1': report['weighted avg']['f1-score'],\n",
    "                'macro_f1': report['macro avg']['f1-score'],\n",
    "                'roc_auc': roc_auc,\n",
    "                'pr_auc': pr_auc\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _generate_roc_curves(self, y_test, y_pred_probs):\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        y_test_bin = np.eye(self.num_classes)[y_test]\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig('roc_curves.png')\n",
    "\n",
    "    def _plot_confusion_matrix(self, cm, class_names):\n",
    "        import seaborn as sns\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, \n",
    "                    yticklabels=class_names)\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "\n",
    "    def plot_training_history(self, history):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss', fontsize=12)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy', fontsize=12)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        if 'val_f1_score' in history.history:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(history.history['val_f1_score'], label='Validation F1 Score', color='green')\n",
    "            plt.title('F1 Score', fontsize=12)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.legend()\n",
    "        \n",
    "        if len(history.epoch) > 0:\n",
    "            plt.subplot(2, 2, 4)\n",
    "            lr_values = [self.model.optimizer.learning_rate(i) for i in range(len(history.epoch))]\n",
    "            plt.plot(lr_values, label='Learning Rate', color='purple')\n",
    "            plt.title('Learning Rate Schedule', fontsize=12)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history_extended.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        pd.DataFrame(history.history).to_csv('training_metrics.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    file_path = \"/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv\"\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(file_path)\n",
    "    \n",
    "    # Train the anomaly detector (Layer 1)\n",
    "    anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[1], latent_dim=16)\n",
    "    layer1_history = anomaly_detector.train(X_train, X_val, epochs=50)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold=0.03)\n",
    "    y_anomalies = y_train.iloc[indices].values.astype(int)\n",
    "    \n",
    "    # Extract features from anomalies\n",
    "    X_features = anomaly_detector.extract_features(X_anomalies)\n",
    "    X_pca, pca = apply_pca(X_features)\n",
    "    X_selected, feature_indices = select_features(X_pca, y_anomalies, n_features=15)\n",
    "    joblib.dump(feature_indices, 'feature_indices_Iteration_3.pkl')\n",
    "    \n",
    "   \n",
    "    # Create sequences\n",
    "    X_sequences, y_sequences = create_sequences(X_selected, y_anomalies)\n",
    "\n",
    "    # Check if y_sequences has only one class\n",
    "    unique_classes = np.unique(y_sequences)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(\"Only one class present in y_sequences. Skipping ADASYN.\")\n",
    "        X_balanced, y_balanced = X_sequences, y_sequences\n",
    "    else:\n",
    "        # Balance sequences using ADASYN\n",
    "        X_balanced, y_balanced = balance_sequences_with_adasyn(X_sequences, y_sequences)\n",
    "    X_balanced, y_balanced = balance_sequences_with_adasyn(X_sequences, y_sequences)\n",
    "    \n",
    "    # Detect anomalies in validation data\n",
    "    X_val_anomalies, val_indices = anomaly_detector.detect_anomalies(X_val, threshold=0.03)\n",
    "    y_val_anomalies = y_val.iloc[val_indices].values.astype(int)\n",
    "    X_val_features = anomaly_detector.extract_features(X_val_anomalies)\n",
    "    X_val_pca = pca.transform(X_val_features)\n",
    "    X_val_selected = X_val_pca[:, feature_indices]\n",
    "    X_val_sequences = create_sequences(X_val_selected)\n",
    "    \n",
    "    # Build the classifier (Layer 2)\n",
    "    num_classes = len(np.unique(y_balanced))\n",
    "    classifier = AdaptiveNIDSLayer2(\n",
    "        input_dim=X_balanced.shape[2], \n",
    "        num_classes=num_classes, \n",
    "        seq_length=X_balanced.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the classifier\n",
    "    layer2_history = classifier.train(X_balanced, y_balanced, X_val_sequences, y_val_anomalies)\n",
    "    \n",
    "    # Detect anomalies in test data\n",
    "    X_test_anomalies, test_indices = anomaly_detector.detect_anomalies(X_test, threshold=0.03)\n",
    "    y_test_anomalies = y_test.iloc[test_indices].values.astype(int)\n",
    "    X_test_features = anomaly_detector.extract_features(X_test_anomalies)\n",
    "    X_test_pca = pca.transform(X_test_features)\n",
    "    X_test_selected = X_test_pca[:, feature_indices]\n",
    "    X_test_sequences = create_sequences(X_test_selected)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = classifier.evaluate_model(X_test_sequences, y_test_anomalies)\n",
    "    \n",
    "    # Plot training history\n",
    "    classifier.plot_training_history(layer2_history)\n",
    "    \n",
    "    # Save the models\n",
    "    anomaly_detector.model.save('anomaly_detector_model.h5')\n",
    "    classifier.model.save('classifier_model.h5')\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(f\"Accuracy: {eval_results['overall_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {eval_results['overall_metrics']['weighted_f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {eval_results['overall_metrics']['roc_auc']:.4f}\")\n",
    "    print(f\"PR AUC: {eval_results['overall_metrics']['pr_auc']:.4f}\")\n",
    "    \n",
    "    # Print per-class metrics\n",
    "    print(\"\\nPer-Class Performance:\")\n",
    "    for class_idx, metrics in eval_results['class_metrics'].items():\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Detection Rate: {metrics['detection_rate']:.4f}\")\n",
    "        print(f\"  False Positive Rate: {metrics['false_positive_rate']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "    \n",
    "    # Generate additional visualizations\n",
    "    plot_feature_importance(classifier, X_test_sequences, y_test_anomalies)\n",
    "    \n",
    "    # Save evaluation results to file\n",
    "    with open('evaluation_results.txt', 'w') as f:\n",
    "        f.write(f\"Model Evaluation Results:\\n\")\n",
    "        f.write(f\"Accuracy: {eval_results['overall_metrics']['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1 Score: {eval_results['overall_metrics']['weighted_f1']:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {eval_results['overall_metrics']['roc_auc']:.4f}\\n\")\n",
    "        f.write(f\"PR AUC: {eval_results['overall_metrics']['pr_auc']:.4f}\\n\")\n",
    "        f.write(\"\\nClassification Report:\\n\")\n",
    "        for class_name, metrics in eval_results['classification_report'].items():\n",
    "            if isinstance(metrics, dict):\n",
    "                f.write(f\"{class_name}: {metrics}\\n\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "def plot_feature_importance(classifier, X_test, y_test):\n",
    "    # Use Integrated Gradients for feature attribution\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.models import Model\n",
    "        \n",
    "        # Create a model that returns both predictions and gradients\n",
    "        grad_model = Model(\n",
    "            inputs=classifier.model.inputs,\n",
    "            outputs=[classifier.model.outputs[0], \n",
    "                     tf.gradients(classifier.model.outputs[0], classifier.model.inputs)[0]]\n",
    "        )\n",
    "        \n",
    "        # Sample subset for analysis\n",
    "        sample_indices = np.random.choice(len(X_test), min(100, len(X_test)), replace=False)\n",
    "        X_sample = X_test[sample_indices]\n",
    "        y_sample = y_test[sample_indices]\n",
    "        \n",
    "        # Calculate integrated gradients\n",
    "        predictions, grads = grad_model.predict(X_sample)\n",
    "        feature_importance = np.mean(np.abs(grads), axis=0)\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(feature_importance.T, aspect='auto', cmap='viridis')\n",
    "        plt.colorbar(label='Feature Importance')\n",
    "        plt.xlabel('Sequence Position')\n",
    "        plt.ylabel('Feature Index')\n",
    "        plt.title('Feature Importance Across Sequence')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300)\n",
    "        \n",
    "        # Plot average importance by position in sequence\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        avg_importance = np.mean(feature_importance, axis=1)\n",
    "        plt.bar(range(len(avg_importance)), avg_importance)\n",
    "        plt.xlabel('Sequence Position')\n",
    "        plt.ylabel('Average Importance')\n",
    "        plt.title('Feature Importance by Sequence Position')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('position_importance.png', dpi=300)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate feature importance visualization: {e}\")\n",
    "\n",
    "def predict_single_sample(anomaly_detector, classifier, sample, pca, feature_indices, threshold=0.03):\n",
    "    \n",
    "    # Reshape sample if needed\n",
    "    if len(sample.shape) == 1:\n",
    "        sample = sample.reshape(1, -1)\n",
    "    \n",
    "    # Check if sample is anomalous\n",
    "    reconstructed = anomaly_detector.model.predict(sample)\n",
    "    error = np.mean(np.square(sample - reconstructed), axis=1)[0]\n",
    "    \n",
    "    if error <= threshold:\n",
    "        return {\n",
    "            'anomaly': False,\n",
    "            'error': error,\n",
    "            'class': None,\n",
    "            'confidence': None\n",
    "        }\n",
    "    \n",
    "    # Extract features and apply transformations\n",
    "    features = anomaly_detector.extract_features(sample)\n",
    "    features_pca = pca.transform(features)\n",
    "    selected_features = features_pca[:, feature_indices]\n",
    "    \n",
    "    # Create sequence\n",
    "    sequence = create_sequences(selected_features)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = classifier.model.predict(sequence)[0]\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    confidence = predictions[predicted_class]\n",
    "    \n",
    "    return {\n",
    "        'anomaly': True,\n",
    "        'error': error,\n",
    "        'class': int(predicted_class),\n",
    "        'confidence': float(confidence),\n",
    "        'class_probabilities': {i: float(p) for i, p in enumerate(predictions)}\n",
    "    }\n",
    "\n",
    "def batch_prediction(anomaly_detector, classifier, samples, pca, feature_indices, threshold=0.03):\n",
    "    \n",
    "    # Detect anomalies\n",
    "    reconstructed = anomaly_detector.model.predict(samples)\n",
    "    errors = np.mean(np.square(samples - reconstructed), axis=1)\n",
    "    anomaly_indices = np.where(errors > threshold)[0]\n",
    "    \n",
    "    results = {\n",
    "        'total_samples': len(samples),\n",
    "        'anomalies_detected': len(anomaly_indices),\n",
    "        'anomaly_indices': anomaly_indices.tolist(),\n",
    "        'errors': errors.tolist(),\n",
    "        'predictions': [None] * len(samples)\n",
    "    }\n",
    "    \n",
    "    if len(anomaly_indices) > 0:\n",
    "        # Extract features from anomalies\n",
    "        anomalies = samples[anomaly_indices]\n",
    "        features = anomaly_detector.extract_features(anomalies)\n",
    "        features_pca = pca.transform(features)\n",
    "        selected_features = features_pca[:, feature_indices]\n",
    "        \n",
    "        # Create sequences\n",
    "        sequences = create_sequences(selected_features)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = classifier.model.predict(sequences)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        confidences = np.max(predictions, axis=1)\n",
    "        \n",
    "        # Store predictions\n",
    "        for i, idx in enumerate(anomaly_indices):\n",
    "            results['predictions'][idx] = {\n",
    "                'class': int(predicted_classes[i]),\n",
    "                'confidence': float(confidences[i]),\n",
    "                'class_probabilities': {j: float(p) for j, p in enumerate(predictions[i])}\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found. Using CPU strategy.\n",
      "Epoch 1/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: nan - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: nan - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: nan - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: nan - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: nan - val_loss: nan - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: nan - val_loss: nan - learning_rate: 5.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: nan - val_loss: nan - learning_rate: 5.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: nan - val_loss: nan - learning_rate: 5.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: nan - val_loss: nan - learning_rate: 5.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: nan - val_loss: nan - learning_rate: 5.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: nan - val_loss: nan - learning_rate: 2.5000e-05\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m1339/1339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Warning: No anomalies detected with the current threshold. Adjusting threshold...\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Warning: NaN values detected in extracted features. Replacing with zeros.\n",
      "ADASYN failed: The target 'y' needs to have more than 1 class. Got 1 class instead. Using original data.\n",
      "Warning: Only 1 class detected. Setting minimum of 2 classes.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:648: RuntimeWarning: invalid value encountered in divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/9\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5917 - loss: 0.6919   "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_9\" is incompatible with the layer: expected shape=(None, 10, 1), found shape=(None, 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 398\u001b[39m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining and evaluation completed successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 336\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m strategy.scope():\n\u001b[32m    331\u001b[39m         classifier = AdaptiveNIDSLayer2(\n\u001b[32m    332\u001b[39m             input_dim=X_balanced.shape[\u001b[32m2\u001b[39m],\n\u001b[32m    333\u001b[39m             num_classes=num_classes,\n\u001b[32m    334\u001b[39m             seq_length=X_balanced.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    335\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m         layer2_history = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mX_balanced\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43my_val_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mX_balanced\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    341\u001b[39m     classifier = AdaptiveNIDSLayer2(\n\u001b[32m    342\u001b[39m         input_dim=X_balanced.shape[\u001b[32m2\u001b[39m],\n\u001b[32m    343\u001b[39m         num_classes=num_classes,\n\u001b[32m    344\u001b[39m         seq_length=X_balanced.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    345\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 217\u001b[39m, in \u001b[36mAdaptiveNIDSLayer2.train\u001b[39m\u001b[34m(self, X_train, y_train, X_val, y_val, epochs)\u001b[39m\n\u001b[32m    212\u001b[39m lr_schedule = keras.callbacks.ReduceLROnPlateau(\n\u001b[32m    213\u001b[39m     monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m5\u001b[39m, min_lr=\u001b[32m1e-6\u001b[39m)\n\u001b[32m    214\u001b[39m early_stopping = keras.callbacks.EarlyStopping(\n\u001b[32m    215\u001b[39m     monitor=\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/input_spec.py:245\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim != dim:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    246\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m is \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mincompatible with the layer: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Input 0 of layer \"functional_9\" is incompatible with the layer: expected shape=(None, 10, 1), found shape=(None, 44)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import joblib\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Initialize strategy as None by default\n",
    "strategy = None\n",
    "\n",
    "# GPU Configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Try to use multiple GPUs if available\n",
    "        tf.config.experimental.set_visible_devices(gpus[:3], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        if len(gpus) > 1:\n",
    "            strategy = tf.distribute.MirroredStrategy(devices=[f\"/gpu:{i}\" for i in range(min(3, len(gpus)))])\n",
    "        else:\n",
    "            strategy = tf.distribute.get_strategy()  # Default strategy for single GPU\n",
    "        print(f'Number of GPUs available: {len(gpus)}')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        strategy = tf.distribute.get_strategy()  # Default strategy if GPU setup fails\n",
    "else:\n",
    "    print(\"No GPUs found. Using CPU strategy.\")\n",
    "    strategy = tf.distribute.get_strategy()  # Default strategy for CPU\n",
    "\n",
    "# Set mixed precision policy\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Custom mish activation\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "tf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    df = pd.read_csv(file_path)\n",
    "    y = df['Attack_label']\n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    \n",
    "    # Simplified preprocessing\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size, \n",
    "        random_state=random_state, stratify=y_train_val)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "class AdaptiveNIDSLayer1:\n",
    "    def __init__(self, input_dim, latent_dim=8):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        if strategy:\n",
    "            with strategy.scope():\n",
    "                self.model = self._build_autoencoder()\n",
    "        else:\n",
    "            self.model = self._build_autoencoder()\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # Encoder\n",
    "        x = layers.Conv1D(8, 3, activation='selu', padding='same')(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(32, activation=mish)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        encoded = layers.Dense(self.latent_dim, activation='linear')(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(16, return_sequences=True)(x)\n",
    "        x = layers.TimeDistributed(layers.Dense(1, activation='linear'))(x)\n",
    "        decoded = layers.Flatten()(x)\n",
    "        \n",
    "        autoencoder = keras.Model(inputs, decoded)\n",
    "        optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        if mixed_precision.global_policy().name == 'mixed_float16':\n",
    "            optimizer = tf.keras.optimizers.LossScaleOptimizer(optimizer)\n",
    "        \n",
    "        autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "        return autoencoder\n",
    "\n",
    "    def train(self, X_train, X_val, epochs=50):\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train)) \\\n",
    "            .shuffle(1024).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, X_val)) \\\n",
    "            .batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        lr_schedule = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        \n",
    "        return self.model.fit(\n",
    "            train_dataset, epochs=epochs,\n",
    "            validation_data=val_dataset, callbacks=[lr_schedule, early_stopping])\n",
    "\n",
    "    def detect_anomalies(self, X_data, threshold):\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        \n",
    "        # Find indices of anomalies\n",
    "        anomaly_indices = np.where(errors > threshold)[0]\n",
    "        \n",
    "        # Check if any anomalies were found\n",
    "        if len(anomaly_indices) == 0:\n",
    "            print(\"Warning: No anomalies detected with the current threshold. Adjusting threshold...\")\n",
    "            # Fallback: take top 5% highest reconstruction errors\n",
    "            new_threshold = np.percentile(errors, 95)\n",
    "            anomaly_indices = np.where(errors > new_threshold)[0]\n",
    "            if len(anomaly_indices) == 0:\n",
    "                # If still no anomalies, use at least some samples\n",
    "                anomaly_indices = np.argsort(errors)[-int(len(X_data)*0.05):]\n",
    "        \n",
    "        return X_data[anomaly_indices], anomaly_indices\n",
    "\n",
    "    def extract_features(self, X_anomalies):\n",
    "        # Check if X_anomalies is empty\n",
    "        if len(X_anomalies) == 0:\n",
    "            raise ValueError(\"No anomalies detected to extract features from\")\n",
    "        \n",
    "        # Get the LSTM layer by name or more reliably by layer type\n",
    "        lstm_layers = [layer for layer in self.model.layers if 'lstm' in layer.name.lower()]\n",
    "        if lstm_layers:\n",
    "            lstm_layer = lstm_layers[0]\n",
    "            feature_extractor = keras.Model(\n",
    "                inputs=self.model.input, \n",
    "                outputs=lstm_layer.output)\n",
    "        else:\n",
    "            # Fallback to using a different layer if LSTM is not found\n",
    "            feature_extractor = keras.Model(\n",
    "                inputs=self.model.input, \n",
    "                outputs=self.model.layers[-3].output)\n",
    "        \n",
    "        # Extract features\n",
    "        features = feature_extractor.predict(X_anomalies)\n",
    "        \n",
    "        # Reshape to 2D if needed\n",
    "        if features.ndim > 2:\n",
    "            # Either flatten or take mean across sequence dimension\n",
    "            features = np.mean(features, axis=1) if features.ndim == 3 else features.reshape(features.shape[0], -1)\n",
    "        \n",
    "        # Check for NaN values and replace them\n",
    "        if np.isnan(features).any():\n",
    "            print(\"Warning: NaN values detected in extracted features. Replacing with zeros.\")\n",
    "            features = np.nan_to_num(features, nan=0.0)\n",
    "        \n",
    "        return features\n",
    "\n",
    "def create_sequences(data, seq_length=10):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "    return np.array(sequences)\n",
    "\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        if strategy:\n",
    "            with strategy.scope():\n",
    "                self.model = self._build_model()\n",
    "        else:\n",
    "            self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim))\n",
    "        x = layers.Conv1D(32, 3, activation='selu', padding='same')(inputs)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        x = layers.Dense(128, activation=mish)(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        if mixed_precision.global_policy().name == 'mixed_float16':\n",
    "            optimizer = tf.keras.optimizers.LossScaleOptimizer(optimizer)\n",
    "        \n",
    "        model.compile(optimizer=optimizer, \n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100):\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
    "            .shuffle(1024).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \\\n",
    "            .batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        lr_schedule = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "        \n",
    "        return self.model.fit(\n",
    "            train_dataset, epochs=epochs,\n",
    "            validation_data=val_dataset, callbacks=[lr_schedule, early_stopping])\n",
    "\n",
    "def main():\n",
    "    file_path = \"/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Implementaiton/training_dataset.csv\"\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(file_path)\n",
    "    \n",
    "    # Convert y_train to numpy array if it's a pandas Series\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train_array = y_train.values\n",
    "    else:\n",
    "        y_train_array = y_train\n",
    "        \n",
    "    # Do the same for test and validation sets\n",
    "    if isinstance(y_val, pd.Series):\n",
    "        y_val_array = y_val.values\n",
    "    else:\n",
    "        y_val_array = y_val\n",
    "        \n",
    "    if isinstance(y_test, pd.Series):\n",
    "        y_test_array = y_test.values\n",
    "    else:\n",
    "        y_test_array = y_test\n",
    "    \n",
    "    # Layer 1 training\n",
    "    if strategy:\n",
    "        with strategy.scope():\n",
    "            anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[1])\n",
    "            layer1_history = anomaly_detector.train(X_train, X_val, epochs=50)\n",
    "            \n",
    "            # Calculate threshold using validation data\n",
    "            val_errors = np.mean(np.square(\n",
    "                X_val - anomaly_detector.model.predict(X_val)), axis=1)\n",
    "            threshold = np.percentile(val_errors, 95)\n",
    "            \n",
    "            # Detect anomalies\n",
    "            X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold)\n",
    "            # Use numpy array indexing instead of pandas Series indexing\n",
    "            y_anomalies = y_train_array[indices]\n",
    "            \n",
    "            # Feature extraction\n",
    "            X_features = anomaly_detector.extract_features(X_anomalies)\n",
    "    else:\n",
    "        anomaly_detector = AdaptiveNIDSLayer1(input_dim=X_train.shape[1])\n",
    "        layer1_history = anomaly_detector.train(X_train, X_val, epochs=50)\n",
    "        \n",
    "        val_errors = np.mean(np.square(\n",
    "            X_val - anomaly_detector.model.predict(X_val)), axis=1)\n",
    "        threshold = np.percentile(val_errors, 95)\n",
    "        \n",
    "        X_anomalies, indices = anomaly_detector.detect_anomalies(X_train, threshold)\n",
    "        # Use numpy array indexing instead of pandas Series indexing\n",
    "        y_anomalies = y_train_array[indices]\n",
    "        \n",
    "        X_features = anomaly_detector.extract_features(X_anomalies)\n",
    "    \n",
    "    # Ensure features are 2D for PCA and handle NaN values\n",
    "    if X_features.ndim > 2:\n",
    "        X_features = np.mean(X_features, axis=1) if X_features.ndim == 3 else X_features.reshape(X_features.shape[0], -1)\n",
    "    \n",
    "    # Check and handle NaN values again before PCA\n",
    "    if np.isnan(X_features).any():\n",
    "        print(\"Warning: NaN values detected before PCA. Replacing with zeros.\")\n",
    "        X_features = np.nan_to_num(X_features, nan=0.0)\n",
    "    \n",
    "    # Check for inf values as well\n",
    "    if np.isinf(X_features).any():\n",
    "        print(\"Warning: Infinite values detected before PCA. Replacing with large finite values.\")\n",
    "        X_features = np.nan_to_num(X_features, posinf=1e10, neginf=-1e10)\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    pca = PCA(n_components=0.99)\n",
    "    X_pca = pca.fit_transform(X_features)\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "    \n",
    "    # Create sequences for Layer 2\n",
    "    X_sequences = create_sequences(X_pca)\n",
    "    \n",
    "    # Make sure X_sequences is properly shaped\n",
    "    # If not enough samples, adjust sequence length\n",
    "    if len(X_sequences) < 10:\n",
    "        print(\"Warning: Not enough samples for sequences. Adjusting sequence length.\")\n",
    "        seq_length = max(1, min(3, len(X_pca) - 1))\n",
    "        X_sequences = create_sequences(X_pca, seq_length=seq_length)\n",
    "    \n",
    "    # Reshape for ADASYN if needed\n",
    "    X_reshaped = X_sequences.reshape(X_sequences.shape[0], -1)\n",
    "    \n",
    "    # Check if we have enough samples for each class\n",
    "    unique_classes, class_counts = np.unique(y_anomalies[:len(X_reshaped)], return_counts=True)\n",
    "    if np.any(class_counts < 6):  # ADASYN needs at least 6 samples per class\n",
    "        print(\"Warning: Some classes have too few samples for ADASYN. Using original data.\")\n",
    "        X_balanced = X_reshaped\n",
    "        y_balanced = y_anomalies[:len(X_reshaped)]\n",
    "    else:\n",
    "        try:\n",
    "            # Apply ADASYN for balancing\n",
    "            X_balanced, y_balanced = ADASYN().fit_resample(X_reshaped, y_anomalies[:len(X_reshaped)])\n",
    "        except Exception as e:\n",
    "            print(f\"ADASYN failed: {e}. Using original data.\")\n",
    "            X_balanced = X_reshaped\n",
    "            y_balanced = y_anomalies[:len(X_reshaped)]\n",
    "    \n",
    "    # Reshape back to 3D for Layer 2\n",
    "    X_balanced = X_balanced.reshape(-1, X_sequences.shape[1], X_sequences.shape[2])\n",
    "    \n",
    "    # Layer 2 training\n",
    "    num_classes = len(np.unique(y_balanced))\n",
    "    if num_classes <= 1:\n",
    "        print(f\"Warning: Only {num_classes} class detected. Setting minimum of 2 classes.\") \n",
    "    num_classes = 2\n",
    "    if strategy:\n",
    "        with strategy.scope():\n",
    "            classifier = AdaptiveNIDSLayer2(\n",
    "                input_dim=X_balanced.shape[2],\n",
    "                num_classes=num_classes,\n",
    "                seq_length=X_balanced.shape[1]\n",
    "            )\n",
    "            layer2_history = classifier.train(X_balanced, y_balanced, \n",
    "                                             X_val[:X_balanced.shape[1]], \n",
    "                                             y_val_array[:X_balanced.shape[1]], \n",
    "                                             epochs=100)\n",
    "    else:\n",
    "        classifier = AdaptiveNIDSLayer2(\n",
    "            input_dim=X_balanced.shape[2],\n",
    "            num_classes=num_classes,\n",
    "            seq_length=X_balanced.shape[1]\n",
    "        )\n",
    "        layer2_history = classifier.train(X_balanced, y_balanced, \n",
    "                                         X_val[:X_balanced.shape[1]], \n",
    "                                         y_val_array[:X_balanced.shape[1]], \n",
    "                                         epochs=100)\n",
    "    \n",
    "    # Evaluation\n",
    "    X_test_anomalies, test_indices = anomaly_detector.detect_anomalies(X_test, threshold)\n",
    "    y_test_anomalies = y_test_array[test_indices]\n",
    "    \n",
    "    # Extract features and ensure 2D shape\n",
    "    X_test_features = anomaly_detector.extract_features(X_test_anomalies)\n",
    "    if X_test_features.ndim > 2:\n",
    "        X_test_features = np.mean(X_test_features, axis=1) if X_test_features.ndim == 3 else X_test_features.reshape(X_test_features.shape[0], -1)\n",
    "    \n",
    "    # Handle NaN and inf values in test features\n",
    "    if np.isnan(X_test_features).any():\n",
    "        print(\"Warning: NaN values detected in test features. Replacing with zeros.\")\n",
    "        X_test_features = np.nan_to_num(X_test_features, nan=0.0)\n",
    "    \n",
    "    if np.isinf(X_test_features).any():\n",
    "        print(\"Warning: Infinite values detected in test features. Replacing with large finite values.\")\n",
    "        X_test_features = np.nan_to_num(X_test_features, posinf=1e10, neginf=-1e10)\n",
    "    \n",
    "    # Apply PCA transformation\n",
    "    X_test_features_pca = pca.transform(X_test_features)\n",
    "    \n",
    "    # Create sequences with the same sequence length as training\n",
    "    X_test_sequences = create_sequences(X_test_features_pca, seq_length=X_balanced.shape[1])\n",
    "    \n",
    "    # Check if we have test sequences\n",
    "    if len(X_test_sequences) > 0:\n",
    "        # Evaluate model\n",
    "        try:\n",
    "            eval_results = classifier.model.evaluate(X_test_sequences, \n",
    "                                                   y_test_anomalies[:len(X_test_sequences)])\n",
    "            print(f\"Test Accuracy: {eval_results[1]:.4f}\")\n",
    "            \n",
    "            # Generate predictions\n",
    "            y_pred = np.argmax(classifier.model.predict(X_test_sequences), axis=1)\n",
    "            print(classification_report(y_test_anomalies[:len(X_test_sequences)], y_pred))\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"No test sequences available for evaluation.\")\n",
    "    \n",
    "    # Save models\n",
    "    anomaly_detector.model.save('anomaly_detector_model')\n",
    "    classifier.model.save('classifier_model')\n",
    "    \n",
    "    print(\"Training and evaluation completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
