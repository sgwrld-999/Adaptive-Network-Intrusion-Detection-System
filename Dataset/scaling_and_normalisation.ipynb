{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/fzpcz2t93td2j063ysbwphbr0000gn/T/ipykernel_10664/2206728934.py:1: DtypeWarning: Columns (5,30,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Dataset/Normal_dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Adaptive-Network-Intrusion-Detection-System/Dataset/Normal_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().to_latex('output.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  frame.time frame_year          frame_date\n",
      "0   2022 00:51:59.554403000        2022  00:51:59.554403000\n",
      "1   2022 05:54:06.889467000        2022  05:54:06.889467000\n",
      "2   2021 19:15:21.234421000        2021  19:15:21.234421000\n",
      "3   2021 04:58:55.411604000        2021  04:58:55.411604000\n",
      "4   2021 05:20:28.623968000        2021  05:20:28.623968000\n"
     ]
    }
   ],
   "source": [
    "# First split the frame.time column into temporary columns\n",
    "df['year'] = df['frame.time'].str.strip().str.split(n=1, expand=True)[0]\n",
    "df['time'] = df['frame.time'].str.strip().str.split(n=1, expand=True)[1]\n",
    "\n",
    "# Now rename the columns to desired names\n",
    "df = df.rename(columns={'year': 'frame_year', 'time': 'frame_date'})\n",
    "\n",
    "# Apply conversion only for valid year entries (optional step)\n",
    "df['frame_year'] = df['frame_year'].apply(lambda x: int(x) if x.isdigit() and len(x) == 4 else x)\n",
    "\n",
    "# Display the result to verify\n",
    "print(df[['frame.time', 'frame_year', 'frame_date']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('frame.time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 64 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   ip.src_host                150000 non-null  object \n",
      " 1   ip.dst_host                150000 non-null  object \n",
      " 2   arp.dst.proto_ipv4         150000 non-null  object \n",
      " 3   arp.opcode                 150000 non-null  float64\n",
      " 4   arp.hw.size                150000 non-null  object \n",
      " 5   arp.src.proto_ipv4         150000 non-null  object \n",
      " 6   icmp.checksum              150000 non-null  float64\n",
      " 7   icmp.seq_le                150000 non-null  float64\n",
      " 8   icmp.transmit_timestamp    150000 non-null  float64\n",
      " 9   icmp.unused                150000 non-null  float64\n",
      " 10  http.file_data             150000 non-null  float64\n",
      " 11  http.content_length        150000 non-null  float64\n",
      " 12  http.request.uri.query     150000 non-null  float64\n",
      " 13  http.request.method        150000 non-null  float64\n",
      " 14  http.referer               150000 non-null  float64\n",
      " 15  http.request.full_uri      150000 non-null  float64\n",
      " 16  http.request.version       150000 non-null  float64\n",
      " 17  http.response              150000 non-null  float64\n",
      " 18  http.tls_port              150000 non-null  float64\n",
      " 19  tcp.ack                    150000 non-null  float64\n",
      " 20  tcp.ack_raw                150000 non-null  object \n",
      " 21  tcp.checksum               150000 non-null  object \n",
      " 22  tcp.connection.fin         150000 non-null  float64\n",
      " 23  tcp.connection.rst         150000 non-null  float64\n",
      " 24  tcp.connection.syn         150000 non-null  float64\n",
      " 25  tcp.connection.synack      150000 non-null  float64\n",
      " 26  tcp.dstport                150000 non-null  object \n",
      " 27  tcp.flags                  150000 non-null  object \n",
      " 28  tcp.flags.ack              150000 non-null  float64\n",
      " 29  tcp.len                    150000 non-null  object \n",
      " 30  tcp.options                150000 non-null  object \n",
      " 31  tcp.payload                150000 non-null  object \n",
      " 32  tcp.seq                    150000 non-null  float64\n",
      " 33  tcp.srcport                150000 non-null  float64\n",
      " 34  udp.port                   150000 non-null  float64\n",
      " 35  udp.stream                 150000 non-null  float64\n",
      " 36  udp.time_delta             150000 non-null  float64\n",
      " 37  dns.qry.name               150000 non-null  object \n",
      " 38  dns.qry.name.len           150000 non-null  object \n",
      " 39  dns.qry.qu                 150000 non-null  float64\n",
      " 40  dns.qry.type               150000 non-null  float64\n",
      " 41  dns.retransmission         150000 non-null  float64\n",
      " 42  dns.retransmit_request     150000 non-null  float64\n",
      " 43  dns.retransmit_request_in  150000 non-null  float64\n",
      " 44  mqtt.conack.flags          150000 non-null  object \n",
      " 45  mqtt.conflag.cleansess     150000 non-null  float64\n",
      " 46  mqtt.conflags              150000 non-null  object \n",
      " 47  mqtt.hdrflags              150000 non-null  object \n",
      " 48  mqtt.len                   150000 non-null  float64\n",
      " 49  mqtt.msg_decoded_as        150000 non-null  float64\n",
      " 50  mqtt.msg                   150000 non-null  object \n",
      " 51  mqtt.msgtype               150000 non-null  float64\n",
      " 52  mqtt.proto_len             150000 non-null  float64\n",
      " 53  mqtt.protoname             150000 non-null  object \n",
      " 54  mqtt.topic                 150000 non-null  object \n",
      " 55  mqtt.topic_len             150000 non-null  float64\n",
      " 56  mqtt.ver                   150000 non-null  float64\n",
      " 57  mbtcp.len                  150000 non-null  float64\n",
      " 58  mbtcp.trans_id             150000 non-null  float64\n",
      " 59  mbtcp.unit_id              150000 non-null  float64\n",
      " 60  Attack_label               150000 non-null  int64  \n",
      " 61  Attack_type                150000 non-null  object \n",
      " 62  frame_year                 150000 non-null  object \n",
      " 63  frame_date                 135000 non-null  object \n",
      "dtypes: float64(40), int64(1), object(23)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Frame_year', axis=1, inplace=True)\n",
    "df.drop('Frame_date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 64 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   ip.src_host                150000 non-null  object \n",
      " 1   ip.dst_host                150000 non-null  object \n",
      " 2   arp.dst.proto_ipv4         150000 non-null  object \n",
      " 3   arp.opcode                 150000 non-null  float64\n",
      " 4   arp.hw.size                150000 non-null  object \n",
      " 5   arp.src.proto_ipv4         150000 non-null  object \n",
      " 6   icmp.checksum              150000 non-null  float64\n",
      " 7   icmp.seq_le                150000 non-null  float64\n",
      " 8   icmp.transmit_timestamp    150000 non-null  float64\n",
      " 9   icmp.unused                150000 non-null  float64\n",
      " 10  http.file_data             150000 non-null  float64\n",
      " 11  http.content_length        150000 non-null  float64\n",
      " 12  http.request.uri.query     150000 non-null  float64\n",
      " 13  http.request.method        150000 non-null  float64\n",
      " 14  http.referer               150000 non-null  float64\n",
      " 15  http.request.full_uri      150000 non-null  float64\n",
      " 16  http.request.version       150000 non-null  float64\n",
      " 17  http.response              150000 non-null  float64\n",
      " 18  http.tls_port              150000 non-null  float64\n",
      " 19  tcp.ack                    150000 non-null  float64\n",
      " 20  tcp.ack_raw                150000 non-null  object \n",
      " 21  tcp.checksum               150000 non-null  object \n",
      " 22  tcp.connection.fin         150000 non-null  float64\n",
      " 23  tcp.connection.rst         150000 non-null  float64\n",
      " 24  tcp.connection.syn         150000 non-null  float64\n",
      " 25  tcp.connection.synack      150000 non-null  float64\n",
      " 26  tcp.dstport                150000 non-null  object \n",
      " 27  tcp.flags                  150000 non-null  object \n",
      " 28  tcp.flags.ack              150000 non-null  float64\n",
      " 29  tcp.len                    150000 non-null  object \n",
      " 30  tcp.options                150000 non-null  object \n",
      " 31  tcp.payload                150000 non-null  object \n",
      " 32  tcp.seq                    150000 non-null  float64\n",
      " 33  tcp.srcport                150000 non-null  float64\n",
      " 34  udp.port                   150000 non-null  float64\n",
      " 35  udp.stream                 150000 non-null  float64\n",
      " 36  udp.time_delta             150000 non-null  float64\n",
      " 37  dns.qry.name               150000 non-null  object \n",
      " 38  dns.qry.name.len           150000 non-null  object \n",
      " 39  dns.qry.qu                 150000 non-null  float64\n",
      " 40  dns.qry.type               150000 non-null  float64\n",
      " 41  dns.retransmission         150000 non-null  float64\n",
      " 42  dns.retransmit_request     150000 non-null  float64\n",
      " 43  dns.retransmit_request_in  150000 non-null  float64\n",
      " 44  mqtt.conack.flags          150000 non-null  object \n",
      " 45  mqtt.conflag.cleansess     150000 non-null  float64\n",
      " 46  mqtt.conflags              150000 non-null  object \n",
      " 47  mqtt.hdrflags              150000 non-null  object \n",
      " 48  mqtt.len                   150000 non-null  float64\n",
      " 49  mqtt.msg_decoded_as        150000 non-null  float64\n",
      " 50  mqtt.msg                   150000 non-null  object \n",
      " 51  mqtt.msgtype               150000 non-null  float64\n",
      " 52  mqtt.proto_len             150000 non-null  float64\n",
      " 53  mqtt.protoname             150000 non-null  object \n",
      " 54  mqtt.topic                 150000 non-null  object \n",
      " 55  mqtt.topic_len             150000 non-null  float64\n",
      " 56  mqtt.ver                   150000 non-null  float64\n",
      " 57  mbtcp.len                  150000 non-null  float64\n",
      " 58  mbtcp.trans_id             150000 non-null  float64\n",
      " 59  mbtcp.unit_id              150000 non-null  float64\n",
      " 60  Attack_label               150000 non-null  int64  \n",
      " 61  Attack_type                150000 non-null  object \n",
      " 62  frame_year                 150000 non-null  object \n",
      " 63  frame_date                 135000 non-null  object \n",
      "dtypes: float64(40), int64(1), object(23)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of features: 45\n",
      "Preprocessing complete. Processed dataset saved as 'processed_training_dataset.csv'.\n",
      "Scaler saved as 'scaler.pkl' and Label Encoders saved as 'label_encoders.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Step 1: Load Dataset (Assuming `td.csv` Contains Only Normal Samples)\n",
    "df_normal = pd.read_csv(\"td.csv\")\n",
    "\n",
    "# Step 2: Drop Unnecessary Columns (Labels, High-Cardinality, Non-Informative)\n",
    "columns_to_drop = [\n",
    "    'ip.src_host', 'ip.dst_host',            # IP addresses (Not generalizable)\n",
    "    'arp.dst.proto_ipv4', 'arp.src.proto_ipv4',  # IP addresses\n",
    "    'tcp.payload', 'dns.qry.name', 'mqtt.msg', 'mqtt.topic',  # High-cardinality text features\n",
    "    'frame_date', 'frame_year',              # Timestamps (Redundant)\n",
    "    'tcp.checksum', 'tcp.ack_raw', 'tcp.options'  # Redundant/Verification fields\n",
    "]\n",
    "df_normal.drop(columns=columns_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Step 3: Convert Object Columns to Numeric Format\n",
    "df_normal['tcp.len'] = pd.to_numeric(df_normal['tcp.len'], errors='coerce')\n",
    "df_normal['dns.qry.name.len'] = pd.to_numeric(df_normal['dns.qry.name.len'], errors='coerce')\n",
    "\n",
    "# Step 4: Convert `tcp.dstport` to Meaningful Categories (Only If Present)\n",
    "if 'tcp.dstport' in df_normal.columns:\n",
    "    df_normal['tcp.dstport'] = pd.to_numeric(df_normal['tcp.dstport'], errors='coerce')\n",
    "\n",
    "    def categorize_port(port):\n",
    "        if pd.isna(port):\n",
    "            return np.nan\n",
    "        elif port <= 1023:\n",
    "            return 0  # Well-known ports\n",
    "        elif port <= 49151:\n",
    "            return 1  # Registered ports\n",
    "        else:\n",
    "            return 2  # Dynamic ports\n",
    "\n",
    "    df_normal['tcp.dstport_category'] = df_normal['tcp.dstport'].apply(categorize_port)\n",
    "    df_normal.drop('tcp.dstport', axis=1, inplace=True)\n",
    "\n",
    "# Step 5: Apply Label Encoding for Categorical Columns (Only Low-Cardinality)\n",
    "categorical_cols = df_normal.select_dtypes(include=['object']).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df_normal[col].nunique() < 10:  # Safe for Label Encoding\n",
    "        le = LabelEncoder()\n",
    "        df_normal[col] = le.fit_transform(df_normal[col].astype(str))\n",
    "        label_encoders[col] = le  # Store encoder for consistency\n",
    "    else:\n",
    "        df_normal.drop(columns=[col], inplace=True)  # Drop high-cardinality categorical features\n",
    "\n",
    "# Step 6: Handle Missing Values (Instead of Dropping Rows)\n",
    "df_normal.fillna(df_normal.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Step 7: Remove Highly Correlated Columns (Correlation > 0.95)\n",
    "corr_matrix = df_normal.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "df_normal.drop(columns=to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Step 8: Scale the Data Using MinMaxScaler (Recommended for Autoencoder)\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df_normal)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df_normal.columns)\n",
    "\n",
    "# Step 9: Save Preprocessed Dataset & Artifacts\n",
    "df_scaled.to_csv('processed_training_dataset.csv', index=False)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(label_encoders, \"label_encoders.pkl\")\n",
    "\n",
    "# Print Summary\n",
    "print(f\"Final number of features: {df_scaled.shape[1]}\")\n",
    "print(\"Preprocessing complete. Processed dataset saved as 'processed_training_dataset.csv'.\")\n",
    "print(\"Scaler saved as 'scaler.pkl' and Label Encoders saved as 'label_encoders.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 45 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   arp.opcode               150000 non-null  float64\n",
      " 1   icmp.checksum            150000 non-null  float64\n",
      " 2   icmp.seq_le              150000 non-null  float64\n",
      " 3   icmp.transmit_timestamp  150000 non-null  float64\n",
      " 4   icmp.unused              150000 non-null  float64\n",
      " 5   http.file_data           150000 non-null  float64\n",
      " 6   http.content_length      150000 non-null  float64\n",
      " 7   http.request.uri.query   150000 non-null  float64\n",
      " 8   http.request.method      150000 non-null  float64\n",
      " 9   http.referer             150000 non-null  float64\n",
      " 10  http.request.full_uri    150000 non-null  float64\n",
      " 11  http.request.version     150000 non-null  float64\n",
      " 12  http.response            150000 non-null  float64\n",
      " 13  http.tls_port            150000 non-null  float64\n",
      " 14  tcp.ack                  150000 non-null  float64\n",
      " 15  tcp.connection.fin       150000 non-null  float64\n",
      " 16  tcp.connection.rst       150000 non-null  float64\n",
      " 17  tcp.connection.syn       150000 non-null  float64\n",
      " 18  tcp.connection.synack    150000 non-null  float64\n",
      " 19  tcp.flags.ack            150000 non-null  float64\n",
      " 20  tcp.len                  150000 non-null  float64\n",
      " 21  tcp.seq                  150000 non-null  float64\n",
      " 22  tcp.srcport              150000 non-null  float64\n",
      " 23  udp.port                 150000 non-null  float64\n",
      " 24  udp.stream               150000 non-null  float64\n",
      " 25  udp.time_delta           150000 non-null  float64\n",
      " 26  dns.qry.name.len         150000 non-null  float64\n",
      " 27  dns.qry.qu               150000 non-null  float64\n",
      " 28  dns.qry.type             150000 non-null  float64\n",
      " 29  dns.retransmission       150000 non-null  float64\n",
      " 30  dns.retransmit_request   150000 non-null  float64\n",
      " 31  mqtt.conack.flags        150000 non-null  float64\n",
      " 32  mqtt.conflag.cleansess   150000 non-null  float64\n",
      " 33  mqtt.conflags            150000 non-null  float64\n",
      " 34  mqtt.hdrflags            150000 non-null  float64\n",
      " 35  mqtt.len                 150000 non-null  float64\n",
      " 36  mqtt.msg_decoded_as      150000 non-null  float64\n",
      " 37  mqtt.msgtype             150000 non-null  float64\n",
      " 38  mqtt.topic_len           150000 non-null  float64\n",
      " 39  mqtt.ver                 150000 non-null  float64\n",
      " 40  mbtcp.len                150000 non-null  float64\n",
      " 41  mbtcp.trans_id           150000 non-null  float64\n",
      " 42  mbtcp.unit_id            150000 non-null  float64\n",
      " 43  Attack_label             150000 non-null  float64\n",
      " 44  tcp.dstport_category     150000 non-null  float64\n",
      "dtypes: float64(45)\n",
      "memory usage: 51.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_scaled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Calculating Mutual Information scores...\n",
      "Selected 10 features based on MI scores (threshold: 0.000470)\n",
      "Performing PCA analysis...\n",
      "Number of components needed for 95% variance: 20\n",
      "Selected 13 features based on combined scores\n",
      "Top 10 selected features:\n",
      "1. tcp.dstport_category\n",
      "2. mbtcp.trans_id\n",
      "3. tcp.ack\n",
      "4. mqtt.ver\n",
      "5. tcp.connection.synack\n",
      "6. mbtcp.len\n",
      "7. mqtt.conflags\n",
      "8. mqtt.conack.flags\n",
      "9. tcp.connection.rst\n",
      "10. http.tls_port\n",
      "Generating LaTeX report...\n",
      "Feature importance analysis complete. Results saved to 'feature_importance.tex'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Create output directory for plots if it doesn't exist\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('processed_training_dataset.csv')\n",
    "\n",
    "# Check if 'Attack_label' exists in the dataset\n",
    "# If not, we'll need to simulate it for demonstration purposes\n",
    "if 'Attack_label' not in df.columns:\n",
    "    print(\"Warning: 'Attack_label' not found in dataset. Creating a simulated label for demonstration.\")\n",
    "    # This is just a placeholder - in a real scenario, you would use the actual label\n",
    "    # For demonstration, we'll assume 5% of data points are attacks\n",
    "    np.random.seed(42)\n",
    "    df['Attack_label'] = np.random.choice([0, 1], size=len(df), p=[0.95, 0.05])\n",
    "\n",
    "# Get features and target\n",
    "X = df.drop('Attack_label', axis=1) if 'Attack_label' in df.columns else df\n",
    "y = df['Attack_label'] if 'Attack_label' in df.columns else None\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# 1. Calculate Mutual Information scores\n",
    "print(\"Calculating Mutual Information scores...\")\n",
    "if y is not None:\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    mi_scores = pd.Series(mi_scores, index=feature_names)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Plot Mutual Information scores\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mi_scores.plot(kind='bar')\n",
    "    plt.title('Feature Importance Based on Mutual Information')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Mutual Information Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/mutual_information_scores.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Identify important features based on MI scores\n",
    "    # Keep features that have MI scores above the mean or a threshold\n",
    "    threshold = mi_scores.mean()\n",
    "    important_features_mi = mi_scores[mi_scores > threshold].index.tolist()\n",
    "    print(f\"Selected {len(important_features_mi)} features based on MI scores (threshold: {threshold:.6f})\")\n",
    "else:\n",
    "    print(\"Skipping Mutual Information calculation as 'Attack_label' is not available\")\n",
    "    mi_scores = pd.Series([0] * len(feature_names), index=feature_names)  # Placeholder\n",
    "    important_features_mi = feature_names  # Keep all features\n",
    "\n",
    "# 2. Perform PCA to determine variance contribution\n",
    "print(\"Performing PCA analysis...\")\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the scree plot (explained variance)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.savefig('plots/pca_scree_plot.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), \n",
    "         cumulative_variance, 'o-', linewidth=2, color='blue')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('plots/pca_cumulative_variance.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Determine number of components needed for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Number of components needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "# Get feature importance from PCA (using the first component as simplification)\n",
    "pca_importance = np.abs(pca.components_[0])\n",
    "pca_importance = pd.Series(pca_importance, index=feature_names)\n",
    "pca_importance = pca_importance.sort_values(ascending=False)\n",
    "\n",
    "# Plot PCA-based feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "pca_importance.plot(kind='bar')\n",
    "plt.title('Feature Importance Based on First PCA Component')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('PCA Loading (absolute value)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/pca_feature_importance.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 3. Combine feature selection methods\n",
    "# If we have MI scores, use both methods; otherwise, just use PCA\n",
    "if y is not None:\n",
    "    # Scale importance scores to 0-1 range for both methods\n",
    "    mi_scores_scaled = (mi_scores - mi_scores.min()) / (mi_scores.max() - mi_scores.min())\n",
    "    pca_importance_scaled = (pca_importance - pca_importance.min()) / (pca_importance.max() - pca_importance.min())\n",
    "    \n",
    "    # Calculate combined score (equal weight to MI and PCA)\n",
    "    combined_scores = pd.DataFrame({\n",
    "        'MI_Score': mi_scores,\n",
    "        'MI_Score_Scaled': mi_scores_scaled,\n",
    "        'PCA_Loading': pca_importance,\n",
    "        'PCA_Loading_Scaled': pca_importance_scaled,\n",
    "    })\n",
    "    \n",
    "    combined_scores['Combined_Score'] = combined_scores['MI_Score_Scaled'] * 0.7 + combined_scores['PCA_Loading_Scaled'] * 0.3\n",
    "    combined_scores = combined_scores.sort_values('Combined_Score', ascending=False)\n",
    "    \n",
    "    # Select features above threshold (e.g., mean of combined scores)\n",
    "    threshold = combined_scores['Combined_Score'].mean()\n",
    "    selected_features = combined_scores[combined_scores['Combined_Score'] > threshold].index.tolist()\n",
    "else:\n",
    "    # If no MI scores, just use PCA-based importance\n",
    "    combined_scores = pd.DataFrame({\n",
    "        'PCA_Loading': pca_importance,\n",
    "    })\n",
    "    combined_scores = combined_scores.sort_values('PCA_Loading', ascending=False)\n",
    "    \n",
    "    # Select top features based on PCA loading\n",
    "    threshold = pca_importance.mean()\n",
    "    selected_features = pca_importance[pca_importance > threshold].index.tolist()\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features based on combined scores\")\n",
    "print(\"Top 10 selected features:\")\n",
    "for i, feature in enumerate(selected_features[:10]):\n",
    "    print(f\"{i+1}. {feature}\")\n",
    "\n",
    "# 4. Generate LaTeX file with results\n",
    "print(\"Generating LaTeX report...\")\n",
    "with open('feature_importance.tex', 'w') as f:\n",
    "    f.write(\"\\\\documentclass{article}\\n\")\n",
    "    f.write(\"\\\\usepackage{graphicx}\\n\")\n",
    "    f.write(\"\\\\usepackage{booktabs}\\n\")\n",
    "    f.write(\"\\\\usepackage{longtable}\\n\")\n",
    "    f.write(\"\\\\usepackage{geometry}\\n\")\n",
    "    f.write(\"\\\\geometry{margin=1in}\\n\")\n",
    "    f.write(\"\\\\title{Feature Importance Analysis for Anomaly Detection}\\n\")\n",
    "    f.write(\"\\\\author{GORK AI System}\\n\")\n",
    "    f.write(\"\\\\date{\\\\today}\\n\")\n",
    "    f.write(\"\\\\begin{document}\\n\")\n",
    "    f.write(\"\\\\maketitle\\n\")\n",
    "    \n",
    "    f.write(\"\\\\section{Feature Importance Analysis}\\n\")\n",
    "    \n",
    "    # 1. Introduction\n",
    "    f.write(\"\\\\subsection{Introduction}\\n\")\n",
    "    f.write(\"This report presents the feature importance analysis for the dataset used in the Layer 1 (Autoencoder-VAE) of the GORK AI system. \")\n",
    "    f.write(\"We analyzed a total of \" + str(len(feature_names)) + \" features to determine their significance in anomaly detection.\\n\")\n",
    "    \n",
    "    # 2. Mutual Information Analysis\n",
    "    f.write(\"\\\\subsection{Mutual Information Analysis}\\n\")\n",
    "    if y is not None:\n",
    "        f.write(\"Mutual Information (MI) measures the dependency between features and the target variable (Attack\\\\_label). \")\n",
    "        f.write(\"Higher MI scores indicate more informative features for detecting anomalies.\\n\")\n",
    "        \n",
    "        f.write(\"\\\\begin{figure}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\includegraphics[width=\\\\textwidth]{plots/mutual_information_scores.png}\\n\")\n",
    "        f.write(\"\\\\caption{Feature Importance Based on Mutual Information Scores}\\n\")\n",
    "        f.write(\"\\\\label{fig:mi_scores}\\n\")\n",
    "        f.write(\"\\\\end{figure}\\n\")\n",
    "        \n",
    "        # MI scores table\n",
    "        f.write(\"\\\\begin{longtable}{lrr}\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Feature & MI Score & Normalized Score \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        \n",
    "        for feature, score in mi_scores.items():\n",
    "            normalized_score = (score - mi_scores.min()) / (mi_scores.max() - mi_scores.min())\n",
    "            f.write(f\"{feature} & {score:.6f} & {normalized_score:.6f} \\\\\\\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\caption{Mutual Information Scores for All Features}\\n\")\n",
    "        f.write(\"\\\\end{longtable}\\n\")\n",
    "    else:\n",
    "        f.write(\"Mutual Information analysis was skipped as the 'Attack\\\\_label' was not available in the dataset.\\n\")\n",
    "    \n",
    "    # 3. PCA Analysis\n",
    "    f.write(\"\\\\subsection{Principal Component Analysis}\\n\")\n",
    "    f.write(\"Principal Component Analysis (PCA) was used to understand the variance contribution of each feature and dimensionality reduction potential.\\n\")\n",
    "    \n",
    "    f.write(\"\\\\begin{figure}[ht]\\n\")\n",
    "    f.write(\"\\\\centering\\n\")\n",
    "    f.write(\"\\\\includegraphics[width=0.8\\\\textwidth]{plots/pca_scree_plot.png}\\n\")\n",
    "    f.write(\"\\\\caption{Scree Plot Showing Variance Explained by Each Principal Component}\\n\")\n",
    "    f.write(\"\\\\label{fig:pca_scree}\\n\")\n",
    "    f.write(\"\\\\end{figure}\\n\")\n",
    "    \n",
    "    f.write(\"\\\\begin{figure}[ht]\\n\")\n",
    "    f.write(\"\\\\centering\\n\")\n",
    "    f.write(\"\\\\includegraphics[width=0.8\\\\textwidth]{plots/pca_cumulative_variance.png}\\n\")\n",
    "    f.write(\"\\\\caption{Cumulative Explained Variance by Principal Components}\\n\")\n",
    "    f.write(\"\\\\label{fig:pca_cumulative}\\n\")\n",
    "    f.write(\"\\\\end{figure}\\n\")\n",
    "    \n",
    "    f.write(f\"Based on the PCA analysis, {n_components_95} components are needed to explain 95\\\\% of the variance in the data.\\n\")\n",
    "    \n",
    "    f.write(\"\\\\begin{figure}[ht]\\n\")\n",
    "    f.write(\"\\\\centering\\n\")\n",
    "    f.write(\"\\\\includegraphics[width=\\\\textwidth]{plots/pca_feature_importance.png}\\n\")\n",
    "    f.write(\"\\\\caption{Feature Importance Based on First PCA Component}\\n\")\n",
    "    f.write(\"\\\\label{fig:pca_importance}\\n\")\n",
    "    f.write(\"\\\\end{figure}\\n\")\n",
    "    \n",
    "    # PCA importance table\n",
    "    f.write(\"\\\\begin{longtable}{lr}\\n\")\n",
    "    f.write(\"\\\\toprule\\n\")\n",
    "    f.write(\"Feature & PCA Loading (abs) \\\\\\\\\\n\")\n",
    "    f.write(\"\\\\midrule\\n\")\n",
    "    \n",
    "    for feature, importance in pca_importance.items():\n",
    "        f.write(f\"{feature} & {importance:.6f} \\\\\\\\\\n\")\n",
    "    \n",
    "    f.write(\"\\\\bottomrule\\n\")\n",
    "    f.write(\"\\\\caption{PCA-based Feature Importance Scores}\\n\")\n",
    "    f.write(\"\\\\end{longtable}\\n\")\n",
    "    \n",
    "    # 4. Combined Feature Selection\n",
    "    f.write(\"\\\\subsection{Combined Feature Selection}\\n\")\n",
    "    \n",
    "    if y is not None:\n",
    "        f.write(\"We combined the results from Mutual Information and PCA to obtain a more robust feature selection. \")\n",
    "        f.write(\"Features were selected based on a combined score that gives 70\\\\% weight to MI and 30\\\\% weight to PCA importance.\\n\")\n",
    "        \n",
    "        # Combined scores table\n",
    "        f.write(\"\\\\begin{longtable}{lrrr}\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Feature & MI Score & PCA Loading & Combined Score \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        \n",
    "        for feature in combined_scores.index[:30]:  # Show top 30 features\n",
    "            row = combined_scores.loc[feature]\n",
    "            f.write(f\"{feature} & {row['MI_Score']:.6f} & {row['PCA_Loading']:.6f} & {row['Combined_Score']:.6f} \\\\\\\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\caption{Top 30 Features Based on Combined Importance Score}\\n\")\n",
    "        f.write(\"\\\\end{longtable}\\n\")\n",
    "    else:\n",
    "        f.write(\"Since the 'Attack\\\\_label' was not available, feature selection was based solely on PCA loadings.\\n\")\n",
    "        \n",
    "        # PCA-based selection table\n",
    "        f.write(\"\\\\begin{longtable}{lr}\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Feature & PCA Loading \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        \n",
    "        for feature in combined_scores.index[:30]:  # Show top 30 features\n",
    "            importance = combined_scores.loc[feature, 'PCA_Loading']\n",
    "            f.write(f\"{feature} & {importance:.6f} \\\\\\\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\caption{Top 30 Features Based on PCA Importance}\\n\")\n",
    "        f.write(\"\\\\end{longtable}\\n\")\n",
    "    \n",
    "    # 5. Conclusion\n",
    "    f.write(\"\\\\subsection{Conclusion}\\n\")\n",
    "    f.write(f\"Based on our analysis, we identified {len(selected_features)} features as important for anomaly detection. \")\n",
    "    f.write(\"These features will be used in the Layer 1 (Autoencoder-VAE) of the GORK AI system.\\n\")\n",
    "    \n",
    "    f.write(\"\\\\paragraph{Selected Features:}\\n\")\n",
    "    f.write(\"\\\\begin{itemize}\\n\")\n",
    "    for feature in selected_features[:min(20, len(selected_features))]:  # List top 20 features\n",
    "        f.write(f\"\\\\item {feature}\\n\")\n",
    "    \n",
    "    if len(selected_features) > 20:\n",
    "        f.write(f\"\\\\item ... {len(selected_features) - 20} more features\\n\")\n",
    "    \n",
    "    f.write(\"\\\\end{itemize}\\n\")\n",
    "    \n",
    "    f.write(\"\\\\end{document}\\n\")\n",
    "\n",
    "print(\"Feature importance analysis complete. Results saved to 'feature_importance.tex'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 45\n",
      "Feature importance visualization complete. Plots saved in 'plots/' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"processed_training_dataset.csv\")\n",
    "\n",
    "# Check feature names\n",
    "feature_names = df.columns.tolist()\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "\n",
    "# Compute feature correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=False, fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.savefig(\"plots/feature_correlation_heatmap.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Compute Mutual Information scores (if Attack_label is available)\n",
    "if 'Attack_label' in df.columns:\n",
    "    X = df.drop('Attack_label', axis=1)\n",
    "    y = df['Attack_label']\n",
    "    \n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "    # Plot MI scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    mi_scores.plot(kind='bar', color='blue')\n",
    "    plt.title('Feature Importance Based on Mutual Information')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Mutual Information Score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/mutual_information_distribution.png\", dpi=300)\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Warning: 'Attack_label' not found, skipping MI computation.\")\n",
    "\n",
    "# PCA Analysis\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(df_scaled)\n",
    "\n",
    "# Scree Plot (Explained Variance)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.grid()\n",
    "plt.savefig(\"plots/pca_scree_plot.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Cumulative Explained Variance Plot\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2, color='blue')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"plots/pca_cumulative_variance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# PCA Feature Importance\n",
    "pca_importance = np.abs(pca.components_[0])\n",
    "pca_importance = pd.Series(pca_importance, index=feature_names)\n",
    "pca_importance = pca_importance.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "pca_importance.plot(kind='bar', color='red')\n",
    "plt.title('Feature Importance Based on First PCA Component')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('PCA Loading (absolute value)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/pca_feature_importance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Histogram for Feature Distributions\n",
    "df.hist(figsize=(15, 12), bins=50, edgecolor=\"black\")\n",
    "plt.suptitle(\"Feature Distribution Histograms\")\n",
    "plt.savefig(\"plots/feature_histograms.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Feature importance visualization complete. Plots saved in 'plots/' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 45\n",
      "Feature importance visualization complete. Plots saved in 'plots/' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure output directory exists after kernel restart\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Reload dataset\n",
    "df = pd.read_csv(\"processed_training_dataset.csv\")\n",
    "\n",
    "# Check feature names\n",
    "feature_names = df.columns.tolist()\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "\n",
    "# Compute feature correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=False, fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.savefig(\"plots/feature_correlation_heatmap.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Compute Mutual Information scores (if Attack_label is available)\n",
    "if 'Attack_label' in df.columns:\n",
    "    X = df.drop('Attack_label', axis=1)\n",
    "    y = df['Attack_label']\n",
    "    \n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "    # Plot MI scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    mi_scores.plot(kind='bar', color='blue')\n",
    "    plt.title('Feature Importance Based on Mutual Information')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Mutual Information Score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/mutual_information_distribution.png\", dpi=300)\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Warning: 'Attack_label' not found, skipping MI computation.\")\n",
    "\n",
    "# PCA Analysis\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(df_scaled)\n",
    "\n",
    "# Scree Plot (Explained Variance)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.grid()\n",
    "plt.savefig(\"plots/pca_scree_plot.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Cumulative Explained Variance Plot\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2, color='blue')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"plots/pca_cumulative_variance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# PCA Feature Importance\n",
    "pca_importance = np.abs(pca.components_[0])\n",
    "pca_importance = pd.Series(pca_importance, index=feature_names)\n",
    "pca_importance = pca_importance.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "pca_importance.plot(kind='bar', color='red')\n",
    "plt.title('Feature Importance Based on First PCA Component')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('PCA Loading (absolute value)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/pca_feature_importance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Histogram for Feature Distributions\n",
    "df.hist(figsize=(15, 12), bins=50, edgecolor=\"black\")\n",
    "plt.suptitle(\"Feature Distribution Histograms\")\n",
    "plt.savefig(\"plots/feature_histograms.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Feature importance visualization complete. Plots saved in 'plots/' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance visualization complete. Plots saved in 'plots/' directory.\n",
      "Text data saved in 'text_outputs/' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('text_outputs', exist_ok=True)\n",
    "\n",
    "# Reload dataset\n",
    "df = pd.read_csv(\"processed_training_dataset.csv\")\n",
    "\n",
    "# Check feature names\n",
    "feature_names = df.columns.tolist()\n",
    "with open(\"text_outputs/feature_list.txt\", \"w\") as f:\n",
    "    f.write(f\"Total features: {len(feature_names)}\\n\")\n",
    "    f.write(\"\\n\".join(feature_names))\n",
    "\n",
    "# Compute feature correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=False, fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.savefig(\"plots/feature_correlation_heatmap.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Save correlation matrix in text format\n",
    "correlation_matrix = df.corr()\n",
    "correlation_matrix.to_csv(\"text_outputs/correlation_matrix.txt\", sep=\"\\t\")\n",
    "\n",
    "# Compute Mutual Information scores (if Attack_label is available)\n",
    "if 'Attack_label' in df.columns:\n",
    "    X = df.drop('Attack_label', axis=1)\n",
    "    y = df['Attack_label']\n",
    "    \n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "    # Save MI scores in text format\n",
    "    mi_scores.to_csv(\"text_outputs/mutual_information_scores.txt\", sep=\"\\t\", header=[\"Mutual Information Score\"])\n",
    "\n",
    "    # Plot MI scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    mi_scores.plot(kind='bar', color='blue')\n",
    "    plt.title('Feature Importance Based on Mutual Information')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Mutual Information Score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/mutual_information_distribution.png\", dpi=300)\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Warning: 'Attack_label' not found, skipping MI computation.\")\n",
    "\n",
    "# PCA Analysis\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(df_scaled)\n",
    "\n",
    "# Scree Plot (Explained Variance)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.grid()\n",
    "plt.savefig(\"plots/pca_scree_plot.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Cumulative Explained Variance Plot\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2, color='blue')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"plots/pca_cumulative_variance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Save PCA variance in text format\n",
    "with open(\"text_outputs/pca_explained_variance.txt\", \"w\") as f:\n",
    "    f.write(\"Principal Component\\tExplained Variance Ratio\\n\")\n",
    "    for i, variance in enumerate(pca.explained_variance_ratio_):\n",
    "        f.write(f\"{i+1}\\t{variance:.6f}\\n\")\n",
    "\n",
    "# PCA Feature Importance\n",
    "pca_importance = np.abs(pca.components_[0])\n",
    "pca_importance = pd.Series(pca_importance, index=feature_names)\n",
    "pca_importance = pca_importance.sort_values(ascending=False)\n",
    "\n",
    "# Save PCA feature importance in text format\n",
    "pca_importance.to_csv(\"text_outputs/pca_feature_importance.txt\", sep=\"\\t\", header=[\"PCA Loading (absolute value)\"])\n",
    "\n",
    "# Plot PCA-based feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "pca_importance.plot(kind='bar', color='red')\n",
    "plt.title('Feature Importance Based on First PCA Component')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('PCA Loading (absolute value)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/pca_feature_importance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Histogram for Feature Distributions\n",
    "df.hist(figsize=(15, 12), bins=50, edgecolor=\"black\")\n",
    "plt.suptitle(\"Feature Distribution Histograms\")\n",
    "plt.savefig(\"plots/feature_histograms.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Feature importance visualization complete. Plots saved in 'plots/' directory.\")\n",
    "print(\"Text data saved in 'text_outputs/' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Selection for Layer 1 Training**  \n",
    "\n",
    "#### **Overview**  \n",
    "To optimize the **Autoencoder-VAE** model for anomaly detection, we performed **feature selection** using:  \n",
    "1. **Mutual Information (MI)**  Measures dependency between each feature and anomalies.  \n",
    "2. **Principal Component Analysis (PCA)**  Identifies features contributing to variance.  \n",
    "3. **Combined Scoring (70% MI + 30% PCA)**  Ensures a balanced feature selection approach.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Selected Features**  \n",
    "Based on the combined MI-PCA scores, the following **13 features** have been selected:  \n",
    "\n",
    "| Feature Name               | Description |\n",
    "|----------------------------|-------------|\n",
    "| `tcp.dstport_category`     | Categorized TCP destination port range |\n",
    "| `mbtcp.trans_id`           | Modbus transaction ID (potential protocol-based anomaly) |\n",
    "| `tcp.ack`                  | TCP acknowledgment flag |\n",
    "| `mqtt.ver`                 | MQTT protocol version |\n",
    "| `tcp.connection.synack`     | SYN-ACK flag in TCP handshake |\n",
    "| `mbtcp.len`                | Length of Modbus TCP payload |\n",
    "| `mqtt.conflags`            | MQTT connection flags |\n",
    "| `mqtt.conack.flags`        | MQTT connection acknowledgment flags |\n",
    "| `tcp.connection.rst`       | TCP Reset flag (can indicate network attacks) |\n",
    "| `http.tls_port`            | Port used for TLS traffic |\n",
    "| `tcp.srcport`              | TCP source port |\n",
    "| `tcp.connection.fin`       | TCP FIN flag (connection termination) |\n",
    "| `mqtt.hdrflags`            | MQTT header flags |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Next Steps**  \n",
    "**Dataset Preparation**  \n",
    "- Filter `processed_training_dataset.csv` to retain only these **13 features**.  \n",
    "- Normalize values using **MinMaxScaler**.  \n",
    "- Save as **`layer1_training_data.csv`** for training.  \n",
    "\n",
    "**Autoencoder-VAE Model Setup**  \n",
    "- **Input Size:** 13  \n",
    "- **Latent Space:** 6 (50% of input)  \n",
    "- **Loss Function:** Reconstruction Loss + KL Divergence  \n",
    "- **Activation:** ReLU for hidden layers, Sigmoid for output  \n",
    "\n",
    "**Training Phase**  \n",
    "- Train using **only normal data** to model normal patterns.  \n",
    "- Implement **Early Stopping & Learning Rate Decay** for stability.  \n",
    "- Save the trained **encoder model** for feature extraction in Layer 2.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data_preparation for Layer one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Layer 1 data preparation...\n",
      "Created output directory: layer1_output\n",
      "Loading processed_training_dataset.csv...\n",
      "Original dataset loaded: 150000 rows and 45 columns\n",
      "Filtered dataset: 150000 rows and 13 columns\n",
      "Handling missing values...\n",
      "Missing values: 0 before, 0 after replacement\n",
      "Applying MinMaxScaler...\n",
      "Saving processed dataset to layer1_output/layer1_training_data.csv...\n",
      "Saving scaler model to layer1_output/scaler_layer1.pkl...\n",
      "\n",
      "========== Layer 1 Dataset Summary ==========\n",
      "Total rows: 150000\n",
      "Total columns: 13\n",
      "\n",
      "Feature statistics after preprocessing:\n",
      "       tcp.dstport_category  mbtcp.trans_id       tcp.ack       mqtt.ver  \\\n",
      "count          150000.00000   150000.000000  1.500000e+05  150000.000000   \n",
      "mean                0.71954        0.009242  5.768431e-02       0.023978   \n",
      "std                 0.25203        0.037664  1.956340e-01       0.110854   \n",
      "min                 0.00000        0.000000  0.000000e+00       0.000000   \n",
      "25%                 0.50000        0.000000  2.463365e-10       0.000000   \n",
      "50%                 0.50000        0.000000  1.478019e-09       0.000000   \n",
      "75%                 1.00000        0.000000  3.695047e-09       0.000000   \n",
      "max                 1.00000        1.000000  1.000000e+00       1.000000   \n",
      "\n",
      "       tcp.connection.synack      mbtcp.len  mqtt.conflags  mqtt.conack.flags  \\\n",
      "count          150000.000000  150000.000000  150000.000000      150000.000000   \n",
      "mean                0.030468       0.028728       0.103660           0.104400   \n",
      "std                 0.161795       0.135965       0.260605           0.261727   \n",
      "min                 0.000000       0.000000       0.000000           0.000000   \n",
      "25%                 0.000000       0.000000       0.000000           0.000000   \n",
      "50%                 0.000000       0.000000       0.000000           0.000000   \n",
      "75%                 0.000000       0.000000       0.000000           0.000000   \n",
      "max                 1.000000       1.000000       1.000000           1.000000   \n",
      "\n",
      "       tcp.connection.rst  http.tls_port    tcp.srcport  tcp.connection.fin  \\\n",
      "count       150000.000000  150000.000000  150000.000000       150000.000000   \n",
      "mean             0.124507       0.006022       0.411375            0.108653   \n",
      "std              0.330160       0.034082       0.424309            0.311205   \n",
      "min              0.000000       0.000000       0.000000            0.000000   \n",
      "25%              0.000000       0.000000       0.028733            0.000000   \n",
      "50%              0.000000       0.000000       0.028733            0.000000   \n",
      "75%              0.000000       0.000000       0.856883            0.000000   \n",
      "max              1.000000       1.000000       1.000000            1.000000   \n",
      "\n",
      "       mqtt.hdrflags  \n",
      "count  150000.000000  \n",
      "mean        0.171175  \n",
      "std         0.302261  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.200000  \n",
      "max         1.000000  \n",
      "\n",
      "First 5 rows of the processed dataset:\n",
      "   tcp.dstport_category  mbtcp.trans_id       tcp.ack  mqtt.ver  \\\n",
      "0                   0.5             0.0  1.231682e-09       0.0   \n",
      "1                   0.5             0.0  1.478019e-09       0.0   \n",
      "2                   1.0             0.0  3.182421e-05       0.0   \n",
      "3                   1.0             0.0  2.463365e-10       0.0   \n",
      "4                   1.0             0.0  0.000000e+00       0.0   \n",
      "\n",
      "   tcp.connection.synack  mbtcp.len  mqtt.conflags  mqtt.conack.flags  \\\n",
      "0               0.000000        0.0            0.0                0.0   \n",
      "1               0.000000        0.0            0.0                0.0   \n",
      "2               0.000000        0.0            0.0                0.0   \n",
      "3               0.000015        0.0            0.0                0.0   \n",
      "4               0.000000        0.0            0.0                0.0   \n",
      "\n",
      "   tcp.connection.rst  http.tls_port  tcp.srcport  tcp.connection.fin  \\\n",
      "0                 0.0            0.0     0.766030                 0.0   \n",
      "1                 0.0            0.0     0.943434                 0.0   \n",
      "2                 0.0            0.0     0.090030                 0.0   \n",
      "3                 0.0            0.0     0.028733                 0.0   \n",
      "4                 1.0            0.0     0.028733                 0.0   \n",
      "\n",
      "   mqtt.hdrflags  \n",
      "0            0.8  \n",
      "1            0.0  \n",
      "2            0.0  \n",
      "3            0.0  \n",
      "4            0.0  \n",
      "\n",
      "Layer 1 data preparation completed successfully.\n",
      "Output files created in the 'layer1_output' directory:\n",
      "  - layer1_training_data.csv\n",
      "  - scaler_layer1.pkl\n",
      "  - layer1_features.txt\n",
      "  - layer1_summary_report.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def prepare_layer1_data():\n",
    "    print(\"Starting Layer 1 data preparation...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"layer1_output\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "    else:\n",
    "        print(f\"Output directory already exists: {output_dir}\")\n",
    "    \n",
    "    # 1. Load the dataset\n",
    "    print(\"Loading processed_training_dataset.csv...\")\n",
    "    try:\n",
    "        df = pd.read_csv('processed_training_dataset.csv')\n",
    "        print(f\"Original dataset loaded: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: processed_training_dataset.csv not found.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Filter to keep only the 13 selected features\n",
    "    selected_features = [\n",
    "        'tcp.dstport_category', 'mbtcp.trans_id', 'tcp.ack', 'mqtt.ver',\n",
    "        'tcp.connection.synack', 'mbtcp.len', 'mqtt.conflags', 'mqtt.conack.flags',\n",
    "        'tcp.connection.rst', 'http.tls_port', 'tcp.srcport', 'tcp.connection.fin',\n",
    "        'mqtt.hdrflags'\n",
    "    ]\n",
    "    \n",
    "    # Check if all selected features exist in the dataset\n",
    "    missing_features = [feature for feature in selected_features if feature not in df.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following features are missing from the dataset: {missing_features}\")\n",
    "        # Continue with available features\n",
    "        selected_features = [feature for feature in selected_features if feature in df.columns]\n",
    "        \n",
    "    # Filter to keep only selected features\n",
    "    df_filtered = df[selected_features]\n",
    "    print(f\"Filtered dataset: {df_filtered.shape[0]} rows and {df_filtered.shape[1]} columns\")\n",
    "    \n",
    "    # 3. Handle missing values with median\n",
    "    print(\"Handling missing values...\")\n",
    "    missing_before = df_filtered.isnull().sum().sum()\n",
    "    \n",
    "    if missing_before > 0:\n",
    "        # Replace missing values with median\n",
    "        for column in df_filtered.columns:\n",
    "            if df_filtered[column].isnull().sum() > 0:\n",
    "                median_value = df_filtered[column].median()\n",
    "                df_filtered[column].fillna(median_value, inplace=True)\n",
    "                \n",
    "    missing_after = df_filtered.isnull().sum().sum()\n",
    "    print(f\"Missing values: {missing_before} before, {missing_after} after replacement\")\n",
    "    \n",
    "    # 4. Apply MinMaxScaler\n",
    "    print(\"Applying MinMaxScaler...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df_filtered)\n",
    "    \n",
    "    # Convert scaled data back to DataFrame\n",
    "    df_scaled = pd.DataFrame(scaled_data, columns=df_filtered.columns)\n",
    "    \n",
    "    # 5. Save the processed dataset to the output directory\n",
    "    output_csv_path = os.path.join(output_dir, 'layer1_training_data.csv')\n",
    "    print(f\"Saving processed dataset to {output_csv_path}...\")\n",
    "    df_scaled.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    # 6. Save the scaler model to the output directory\n",
    "    output_scaler_path = os.path.join(output_dir, 'scaler_layer1.pkl')\n",
    "    print(f\"Saving scaler model to {output_scaler_path}...\")\n",
    "    with open(output_scaler_path, 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    \n",
    "    # Save feature names list for reference\n",
    "    output_features_path = os.path.join(output_dir, 'layer1_features.txt')\n",
    "    with open(output_features_path, 'w') as file:\n",
    "        for feature in df_filtered.columns:\n",
    "            file.write(f\"{feature}\\n\")\n",
    "    \n",
    "    # 7. Print dataset summary\n",
    "    print(\"\\n========== Layer 1 Dataset Summary ==========\")\n",
    "    print(f\"Total rows: {df_scaled.shape[0]}\")\n",
    "    print(f\"Total columns: {df_scaled.shape[1]}\")\n",
    "    print(\"\\nFeature statistics after preprocessing:\")\n",
    "    print(df_scaled.describe())\n",
    "    print(\"\\nFirst 5 rows of the processed dataset:\")\n",
    "    print(df_scaled.head())\n",
    "    \n",
    "    # Save summary statistics to a report file\n",
    "    summary_path = os.path.join(output_dir, 'layer1_summary_report.txt')\n",
    "    with open(summary_path, 'w') as file:\n",
    "        file.write(\"========== Layer 1 Dataset Summary ==========\\n\")\n",
    "        file.write(f\"Total rows: {df_scaled.shape[0]}\\n\")\n",
    "        file.write(f\"Total columns: {df_scaled.shape[1]}\\n\\n\")\n",
    "        file.write(\"Feature statistics after preprocessing:\\n\")\n",
    "        file.write(df_scaled.describe().to_string())\n",
    "        \n",
    "    print(\"\\nLayer 1 data preparation completed successfully.\")\n",
    "    print(f\"Output files created in the '{output_dir}' directory:\")\n",
    "    print(f\"  - {os.path.basename(output_csv_path)}\")\n",
    "    print(f\"  - {os.path.basename(output_scaler_path)}\")\n",
    "    print(f\"  - {os.path.basename(output_features_path)}\")\n",
    "    print(f\"  - {os.path.basename(summary_path)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_layer1_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
