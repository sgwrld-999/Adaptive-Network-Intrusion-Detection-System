{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/fzpcz2t93td2j063ysbwphbr0000gn/T/ipykernel_10883/1196730258.py:1: DtypeWarning: Columns (3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_normal_Distanc = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Distance/Distance.csv')\n"
     ]
    }
   ],
   "source": [
    "df_normal_Distanc = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Distance/Distance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_Flame_sensor = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Flame_sensor/Flame_sensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_Heart_Rate = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Heart_Rate/Heart_Rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/fzpcz2t93td2j063ysbwphbr0000gn/T/ipykernel_10883/4228598068.py:1: DtypeWarning: Columns (3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_normal_IR_Receiver = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/IR_Receiver/IR_Receiver.csv')\n"
     ]
    }
   ],
   "source": [
    "df_normal_IR_Receiver = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/IR_Receiver/IR_Receiver.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/fzpcz2t93td2j063ysbwphbr0000gn/T/ipykernel_10883/2031369843.py:1: DtypeWarning: Columns (2,5,30,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_normal_Modbus = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Modbus/Modbus.csv')\n"
     ]
    }
   ],
   "source": [
    "df_normal_Modbus = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Modbus/Modbus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_phValue = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/phValue/phValue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/fzpcz2t93td2j063ysbwphbr0000gn/T/ipykernel_10883/146392412.py:1: DtypeWarning: Columns (3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_normal_Soil_moisture = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Soil_moisture/Soil_moisture.csv')\n",
      "/var/folders/p0/fzpcz2t93td2j063ysbwphbr0000gn/T/ipykernel_10883/146392412.py:2: DtypeWarning: Columns (3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_normal_Sound_Sensor = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Sound_Sensor/Sound_Sensor.csv')\n",
      "/var/folders/p0/fzpcz2t93td2j063ysbwphbr0000gn/T/ipykernel_10883/146392412.py:3: DtypeWarning: Columns (3,4,6,7,9,39,40,49,53,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_normal_Temperature_and_Humidity = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Temperature_and_Humidity/Temperature_and_Humidity.csv')\n",
      "/var/folders/p0/fzpcz2t93td2j063ysbwphbr0000gn/T/ipykernel_10883/146392412.py:4: DtypeWarning: Columns (3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_normal_Water_level = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Water_level/Water_level.csv')\n"
     ]
    }
   ],
   "source": [
    "df_normal_Soil_moisture = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Soil_moisture/Soil_moisture.csv')\n",
    "df_normal_Sound_Sensor = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Sound_Sensor/Sound_Sensor.csv')\n",
    "df_normal_Temperature_and_Humidity = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Temperature_and_Humidity/Temperature_and_Humidity.csv')\n",
    "df_normal_Water_level = pd.read_csv('/Users/siddhantgond/Desktop/6THSEM/Project_Elective/Edge-IIoTset Cyber Security Dataset of IoT & IIoT/Edge-IIoTset dataset/Normal traffic/Water_level/Water_level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame.time</th>\n",
       "      <th>ip.src_host</th>\n",
       "      <th>ip.dst_host</th>\n",
       "      <th>arp.dst.proto_ipv4</th>\n",
       "      <th>arp.opcode</th>\n",
       "      <th>arp.hw.size</th>\n",
       "      <th>arp.src.proto_ipv4</th>\n",
       "      <th>icmp.checksum</th>\n",
       "      <th>icmp.seq_le</th>\n",
       "      <th>icmp.transmit_timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>mqtt.proto_len</th>\n",
       "      <th>mqtt.protoname</th>\n",
       "      <th>mqtt.topic</th>\n",
       "      <th>mqtt.topic_len</th>\n",
       "      <th>mqtt.ver</th>\n",
       "      <th>mbtcp.len</th>\n",
       "      <th>mbtcp.trans_id</th>\n",
       "      <th>mbtcp.unit_id</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>Attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021 23:58:21.314757000</td>\n",
       "      <td>192.168.1.101</td>\n",
       "      <td>192.168.1.128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021 23:58:21.314960000</td>\n",
       "      <td>192.168.1.101</td>\n",
       "      <td>192.168.1.128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021 23:58:21.315192000</td>\n",
       "      <td>192.168.1.101</td>\n",
       "      <td>192.168.1.128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021 23:58:21.315258000</td>\n",
       "      <td>192.168.1.128</td>\n",
       "      <td>192.168.1.101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021 23:58:21.315294000</td>\n",
       "      <td>192.168.1.101</td>\n",
       "      <td>192.168.1.128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  frame.time    ip.src_host    ip.dst_host arp.dst.proto_ipv4  \\\n",
       "0   2021 23:58:21.314757000   192.168.1.101  192.168.1.128                  0   \n",
       "1   2021 23:58:21.314960000   192.168.1.101  192.168.1.128                  0   \n",
       "2   2021 23:58:21.315192000   192.168.1.101  192.168.1.128                  0   \n",
       "3   2021 23:58:21.315258000   192.168.1.128  192.168.1.101                  0   \n",
       "4   2021 23:58:21.315294000   192.168.1.101  192.168.1.128                  0   \n",
       "\n",
       "   arp.opcode  arp.hw.size arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
       "0         0.0          0.0                  0            0.0          0.0   \n",
       "1         0.0          0.0                  0            0.0          0.0   \n",
       "2         0.0          0.0                  0            0.0          0.0   \n",
       "3         0.0          0.0                  0            0.0          0.0   \n",
       "4         0.0          0.0                  0            0.0          0.0   \n",
       "\n",
       "   icmp.transmit_timestamp  ...  mqtt.proto_len  mqtt.protoname  mqtt.topic  \\\n",
       "0                      0.0  ...             0.0               0           0   \n",
       "1                      0.0  ...             0.0               0           0   \n",
       "2                      0.0  ...             0.0               0           0   \n",
       "3                      0.0  ...             0.0               0           0   \n",
       "4                      0.0  ...             0.0               0           0   \n",
       "\n",
       "   mqtt.topic_len  mqtt.ver  mbtcp.len  mbtcp.trans_id  mbtcp.unit_id  \\\n",
       "0             0.0       0.0        0.0             0.0            0.0   \n",
       "1             0.0       0.0        0.0             0.0            0.0   \n",
       "2             0.0       0.0        0.0             0.0            0.0   \n",
       "3             0.0       0.0        0.0             0.0            0.0   \n",
       "4             0.0       0.0        0.0             0.0            0.0   \n",
       "\n",
       "   Attack_label  Attack_type  \n",
       "0             0       Normal  \n",
       "1             0       Normal  \n",
       "2             0       Normal  \n",
       "3             0       Normal  \n",
       "4             0       Normal  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normal_Distanc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_normal_Distanc: (1143540, 63)\n",
      "Shape of df_normal_Flame_sensor: (1070196, 63)\n",
      "Shape of df_normal_Heart_Rate: (165319, 63)\n",
      "Shape of df_normal_IR_Receiver: (1307778, 63)\n",
      "Shape of df_normal_Modbus: (159502, 63)\n",
      "Shape of df_normal_phValue: (746908, 63)\n",
      "Shape of df_normal_Soil_moisture: (1192777, 63)\n",
      "Shape of df_normal_Sound_Sensor: (1512883, 63)\n",
      "Shape of df_normal_Temperature_and_Humidity: (1615722, 63)\n",
      "Shape of df_normal_Water_level: (2295288, 63)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of df_normal_Distanc:\", df_normal_Distanc.shape)\n",
    "print(\"Shape of df_normal_Flame_sensor:\", df_normal_Flame_sensor.shape)\n",
    "print(\"Shape of df_normal_Heart_Rate:\", df_normal_Heart_Rate.shape)\n",
    "print(\"Shape of df_normal_IR_Receiver:\", df_normal_IR_Receiver.shape)\n",
    "print(\"Shape of df_normal_Modbus:\", df_normal_Modbus.shape)\n",
    "print(\"Shape of df_normal_phValue:\", df_normal_phValue.shape)\n",
    "print(\"Shape of df_normal_Soil_moisture:\", df_normal_Soil_moisture.shape)\n",
    "print(\"Shape of df_normal_Sound_Sensor:\", df_normal_Sound_Sensor.shape)\n",
    "print(\"Shape of df_normal_Temperature_and_Humidity:\", df_normal_Temperature_and_Humidity.shape)\n",
    "print(\"Shape of df_normal_Water_level:\", df_normal_Water_level.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference columns from df_normal_Distanc: ['frame.time', 'ip.src_host', 'ip.dst_host', 'arp.dst.proto_ipv4', 'arp.opcode', 'arp.hw.size', 'arp.src.proto_ipv4', 'icmp.checksum', 'icmp.seq_le', 'icmp.transmit_timestamp', 'icmp.unused', 'http.file_data', 'http.content_length', 'http.request.uri.query', 'http.request.method', 'http.referer', 'http.request.full_uri', 'http.request.version', 'http.response', 'http.tls_port', 'tcp.ack', 'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags', 'tcp.flags.ack', 'tcp.len', 'tcp.options', 'tcp.payload', 'tcp.seq', 'tcp.srcport', 'udp.port', 'udp.stream', 'udp.time_delta', 'dns.qry.name', 'dns.qry.name.len', 'dns.qry.qu', 'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in', 'mqtt.conack.flags', 'mqtt.conflag.cleansess', 'mqtt.conflags', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as', 'mqtt.msg', 'mqtt.msgtype', 'mqtt.proto_len', 'mqtt.protoname', 'mqtt.topic', 'mqtt.topic_len', 'mqtt.ver', 'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id', 'Attack_label', 'Attack_type']\n",
      "Number of columns: 63\n",
      "\n",
      "Checking if all dataframes have the same headers...\n",
      "df_normal_Flame_sensor:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "df_normal_Heart_Rate:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "df_normal_IR_Receiver:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "df_normal_Modbus:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "df_normal_phValue:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "df_normal_Soil_moisture:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "df_normal_Sound_Sensor:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "df_normal_Temperature_and_Humidity:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "df_normal_Water_level:\n",
      "  Same column names as reference: ✓\n",
      "  Same number of columns as reference: ✓\n",
      "\n",
      "✅ All dataframes have identical headers.\n",
      "Shape of merged dataframe: (150000, 63)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List of all dataframes\n",
    "dataframes = [\n",
    "    df_normal_Distanc,\n",
    "    df_normal_Flame_sensor,\n",
    "    df_normal_Heart_Rate,\n",
    "    df_normal_IR_Receiver,\n",
    "    df_normal_Modbus,\n",
    "    df_normal_phValue,\n",
    "    df_normal_Soil_moisture,\n",
    "    df_normal_Sound_Sensor,\n",
    "    df_normal_Temperature_and_Humidity,\n",
    "    df_normal_Water_level\n",
    "]\n",
    "\n",
    "# Dataframe names for reference\n",
    "df_names = [\n",
    "    \"df_normal_Distanc\",\n",
    "    \"df_normal_Flame_sensor\",\n",
    "    \"df_normal_Heart_Rate\",\n",
    "    \"df_normal_IR_Receiver\",\n",
    "    \"df_normal_Modbus\",\n",
    "    \"df_normal_phValue\",\n",
    "    \"df_normal_Soil_moisture\",\n",
    "    \"df_normal_Sound_Sensor\",\n",
    "    \"df_normal_Temperature_and_Humidity\",\n",
    "    \"df_normal_Water_level\"\n",
    "]\n",
    "\n",
    "# Check if all dataframes have the same headers\n",
    "reference_columns = dataframes[0].columns.tolist()\n",
    "reference_df_name = df_names[0]\n",
    "\n",
    "print(f\"Reference columns from {reference_df_name}: {reference_columns}\")\n",
    "print(f\"Number of columns: {len(reference_columns)}\")\n",
    "print(\"\\nChecking if all dataframes have the same headers...\")\n",
    "\n",
    "all_headers_match = True\n",
    "for i, (df, name) in enumerate(zip(dataframes[1:], df_names[1:]), 1):\n",
    "    df_columns = df.columns.tolist()\n",
    "    \n",
    "    # Check if column names match\n",
    "    headers_match = (reference_columns == df_columns)\n",
    "    \n",
    "    # Check if column count matches (even if names might be different)\n",
    "    column_count_matches = (len(reference_columns) == len(df_columns))\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Same column names as reference: {'✓' if headers_match else '✗'}\")\n",
    "    print(f\"  Same number of columns as reference: {'✓' if column_count_matches else '✗'}\")\n",
    "    \n",
    "    if not headers_match:\n",
    "        all_headers_match = False\n",
    "        print(f\"  Columns from {name}: {df_columns}\")\n",
    "        \n",
    "        # Find which columns are different\n",
    "        if column_count_matches:\n",
    "            differences = [(i, ref, df_col) for i, (ref, df_col) in enumerate(zip(reference_columns, df_columns)) if ref != df_col]\n",
    "            print(f\"  Different columns (index, reference, current):\")\n",
    "            for diff in differences:\n",
    "                print(f\"    {diff}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "if all_headers_match:\n",
    "    print(\"✅ All dataframes have identical headers.\")\n",
    "else:\n",
    "    print(\"⚠️ Not all dataframes have identical headers. Review the differences above.\")\n",
    "    print(\"You may need to adjust your merge strategy accordingly.\")\n",
    "\n",
    "# Continue with merge only if all headers match or you decide to proceed anyway\n",
    "proceed_with_merge = True  # Set this to False if you want to stop if headers don't match\n",
    "\n",
    "if proceed_with_merge:\n",
    "    # Extract 15000 random rows from each dataframe and merge as before\n",
    "    samples = []\n",
    "    \n",
    "    for df in dataframes:\n",
    "        if df.shape[0] >= 15000:\n",
    "            sample = df.sample(n=15000, random_state=42)\n",
    "        else:\n",
    "            sample = df.sample(n=15000, replace=True, random_state=42)\n",
    "        \n",
    "        if len(samples) > 0:\n",
    "            sample_values = sample.values\n",
    "            samples.append(sample_values)\n",
    "        else:\n",
    "            samples.append(sample)\n",
    "    \n",
    "    merged_df = samples[0]\n",
    "    \n",
    "    for sample_array in samples[1:]:\n",
    "        temp_df = pd.DataFrame(sample_array, columns=merged_df.columns)\n",
    "        merged_df = pd.concat([merged_df, temp_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"Shape of merged dataframe: {merged_df.shape}\")\n",
    "    \n",
    "    # Optional: Shuffle the merged dataframe\n",
    "    merged_df = merged_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    print(\"Merge operation aborted due to header inconsistencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'frame.time': 14992 duplicate values\n",
      "Column 'ip.src_host': 149973 duplicate values\n",
      "Column 'ip.dst_host': 149973 duplicate values\n",
      "Column 'arp.dst.proto_ipv4': 149981 duplicate values\n",
      "Column 'arp.opcode': 149994 duplicate values\n",
      "Column 'arp.hw.size': 149991 duplicate values\n",
      "Column 'arp.src.proto_ipv4': 149986 duplicate values\n",
      "Column 'icmp.checksum': 149998 duplicate values\n",
      "Column 'icmp.seq_le': 149999 duplicate values\n",
      "Column 'icmp.transmit_timestamp': 149998 duplicate values\n",
      "Column 'icmp.unused': 149999 duplicate values\n",
      "Column 'http.file_data': 149999 duplicate values\n",
      "Column 'http.content_length': 149999 duplicate values\n",
      "Column 'http.request.uri.query': 149999 duplicate values\n",
      "Column 'http.request.method': 149999 duplicate values\n",
      "Column 'http.referer': 149999 duplicate values\n",
      "Column 'http.request.full_uri': 149999 duplicate values\n",
      "Column 'http.request.version': 149999 duplicate values\n",
      "Column 'http.response': 149999 duplicate values\n",
      "Column 'http.tls_port': 136334 duplicate values\n",
      "Column 'tcp.ack': 133706 duplicate values\n",
      "Column 'tcp.ack_raw': 29811 duplicate values\n",
      "Column 'tcp.checksum': 92916 duplicate values\n",
      "Column 'tcp.connection.fin': 149998 duplicate values\n",
      "Column 'tcp.connection.rst': 149998 duplicate values\n",
      "Column 'tcp.connection.syn': 149998 duplicate values\n",
      "Column 'tcp.connection.synack': 149930 duplicate values\n",
      "Column 'tcp.dstport': 133973 duplicate values\n",
      "Column 'tcp.flags': 149990 duplicate values\n",
      "Column 'tcp.flags.ack': 149895 duplicate values\n",
      "Column 'tcp.len': 149763 duplicate values\n",
      "Column 'tcp.options': 140098 duplicate values\n",
      "Column 'tcp.payload': 134006 duplicate values\n",
      "Column 'tcp.seq': 147462 duplicate values\n",
      "Column 'tcp.srcport': 133795 duplicate values\n",
      "Column 'udp.port': 149884 duplicate values\n",
      "Column 'udp.stream': 149770 duplicate values\n",
      "Column 'udp.time_delta': 149800 duplicate values\n",
      "Column 'dns.qry.name': 149977 duplicate values\n",
      "Column 'dns.qry.name.len': 149991 duplicate values\n",
      "Column 'dns.qry.qu': 149996 duplicate values\n",
      "Column 'dns.qry.type': 149997 duplicate values\n",
      "Column 'dns.retransmission': 149997 duplicate values\n",
      "Column 'dns.retransmit_request': 149998 duplicate values\n",
      "Column 'dns.retransmit_request_in': 149997 duplicate values\n",
      "Column 'mqtt.conack.flags': 149997 duplicate values\n",
      "Column 'mqtt.conflag.cleansess': 149998 duplicate values\n",
      "Column 'mqtt.conflags': 149997 duplicate values\n",
      "Column 'mqtt.hdrflags': 149994 duplicate values\n",
      "Column 'mqtt.len': 149974 duplicate values\n",
      "Column 'mqtt.msg_decoded_as': 149999 duplicate values\n",
      "Column 'mqtt.msg': 149643 duplicate values\n",
      "Column 'mqtt.msgtype': 149995 duplicate values\n",
      "Column 'mqtt.proto_len': 149996 duplicate values\n",
      "Column 'mqtt.protoname': 149997 duplicate values\n",
      "Column 'mqtt.topic': 149989 duplicate values\n",
      "Column 'mqtt.topic_len': 149992 duplicate values\n",
      "Column 'mqtt.ver': 149985 duplicate values\n",
      "Column 'mbtcp.len': 141260 duplicate values\n",
      "Column 'mbtcp.trans_id': 149991 duplicate values\n",
      "Column 'mbtcp.unit_id': 149999 duplicate values\n",
      "Column 'Attack_label': 149999 duplicate values\n",
      "Column 'Attack_type': 149999 duplicate values\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in each column\n",
    "for column in merged_df.columns:\n",
    "    duplicate_count = merged_df[column].duplicated().sum()\n",
    "    print(f\"Column '{column}': {duplicate_count} duplicate values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing df_normal_Distanc with 1143540 rows...\n",
      "  After deduplication: 1143540 rows\n",
      "  Sampling 15000 from 1143540 deduplicated rows\n",
      "Processing df_normal_Flame_sensor with 1070196 rows...\n",
      "  After deduplication: 1070196 rows\n",
      "  Sampling 15000 from 1070196 deduplicated rows\n",
      "Processing df_normal_Heart_Rate with 165319 rows...\n",
      "  After deduplication: 165319 rows\n",
      "  Sampling 15000 from 165319 deduplicated rows\n",
      "Processing df_normal_IR_Receiver with 1307778 rows...\n",
      "  After deduplication: 1307778 rows\n",
      "  Sampling 15000 from 1307778 deduplicated rows\n",
      "Processing df_normal_Modbus with 159502 rows...\n",
      "  After deduplication: 158217 rows\n",
      "  Sampling 15000 from 158217 deduplicated rows\n",
      "Processing df_normal_phValue with 746908 rows...\n",
      "  After deduplication: 746908 rows\n",
      "  Sampling 15000 from 746908 deduplicated rows\n",
      "Processing df_normal_Soil_moisture with 1192777 rows...\n",
      "  After deduplication: 1192777 rows\n",
      "  Sampling 15000 from 1192777 deduplicated rows\n",
      "Processing df_normal_Sound_Sensor with 1512883 rows...\n",
      "  After deduplication: 1512883 rows\n",
      "  Sampling 15000 from 1512883 deduplicated rows\n",
      "Processing df_normal_Temperature_and_Humidity with 1615722 rows...\n",
      "  After deduplication: 1615722 rows\n",
      "  Sampling 15000 from 1615722 deduplicated rows\n",
      "Processing df_normal_Water_level with 2295288 rows...\n",
      "  After deduplication: 2295288 rows\n",
      "  Sampling 15000 from 2295288 deduplicated rows\n",
      "\n",
      "Before final deduplication: 150000 rows\n",
      "After final deduplication: 150000 rows\n",
      "Removed 0 duplicate rows\n",
      "\n",
      "Duplicate values percentage per column (after final merge):\n",
      "Column 'ip.src_host': 149974 duplicate values (100.0%) - Consider dropping\n",
      "Column 'ip.dst_host': 149973 duplicate values (100.0%) - Consider dropping\n",
      "Column 'arp.dst.proto_ipv4': 149981 duplicate values (100.0%) - Consider dropping\n",
      "Column 'arp.opcode': 149994 duplicate values (100.0%) - Consider dropping\n",
      "Column 'arp.hw.size': 149995 duplicate values (100.0%) - Consider dropping\n",
      "Column 'arp.src.proto_ipv4': 149986 duplicate values (100.0%) - Consider dropping\n",
      "Column 'icmp.checksum': 149998 duplicate values (100.0%) - Consider dropping\n",
      "Column 'icmp.seq_le': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'icmp.transmit_timestamp': 149998 duplicate values (100.0%) - Consider dropping\n",
      "Column 'icmp.unused': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.file_data': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.content_length': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.request.uri.query': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.request.method': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.referer': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.request.full_uri': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.request.version': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.response': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'http.tls_port': 136285 duplicate values (90.9%) - Consider dropping\n",
      "Column 'tcp.ack': 133666 duplicate values (89.1%)\n",
      "Column 'tcp.checksum': 92916 duplicate values (61.9%)\n",
      "Column 'tcp.connection.fin': 149998 duplicate values (100.0%) - Consider dropping\n",
      "Column 'tcp.connection.rst': 149998 duplicate values (100.0%) - Consider dropping\n",
      "Column 'tcp.connection.syn': 149998 duplicate values (100.0%) - Consider dropping\n",
      "Column 'tcp.connection.synack': 149933 duplicate values (100.0%) - Consider dropping\n",
      "Column 'tcp.dstport': 133973 duplicate values (89.3%)\n",
      "Column 'tcp.flags': 149990 duplicate values (100.0%) - Consider dropping\n",
      "Column 'tcp.flags.ack': 149889 duplicate values (99.9%) - Consider dropping\n",
      "Column 'tcp.len': 149763 duplicate values (99.8%) - Consider dropping\n",
      "Column 'tcp.options': 140092 duplicate values (93.4%) - Consider dropping\n",
      "Column 'tcp.payload': 133930 duplicate values (89.3%)\n",
      "Column 'tcp.seq': 147476 duplicate values (98.3%) - Consider dropping\n",
      "Column 'tcp.srcport': 133794 duplicate values (89.2%)\n",
      "Column 'udp.port': 149878 duplicate values (99.9%) - Consider dropping\n",
      "Column 'udp.stream': 149763 duplicate values (99.8%) - Consider dropping\n",
      "Column 'udp.time_delta': 149774 duplicate values (99.8%) - Consider dropping\n",
      "Column 'dns.qry.name': 149977 duplicate values (100.0%) - Consider dropping\n",
      "Column 'dns.qry.name.len': 149991 duplicate values (100.0%) - Consider dropping\n",
      "Column 'dns.qry.qu': 149996 duplicate values (100.0%) - Consider dropping\n",
      "Column 'dns.qry.type': 149997 duplicate values (100.0%) - Consider dropping\n",
      "Column 'dns.retransmission': 149997 duplicate values (100.0%) - Consider dropping\n",
      "Column 'dns.retransmit_request': 149998 duplicate values (100.0%) - Consider dropping\n",
      "Column 'dns.retransmit_request_in': 149998 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.conack.flags': 149997 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.conflag.cleansess': 149998 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.conflags': 149997 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.hdrflags': 149994 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.len': 149974 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.msg_decoded_as': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.msg': 149643 duplicate values (99.8%) - Consider dropping\n",
      "Column 'mqtt.msgtype': 149995 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.proto_len': 149996 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.protoname': 149997 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.topic': 149989 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.topic_len': 149992 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mqtt.ver': 149986 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mbtcp.len': 141228 duplicate values (94.2%) - Consider dropping\n",
      "Column 'mbtcp.trans_id': 149991 duplicate values (100.0%) - Consider dropping\n",
      "Column 'mbtcp.unit_id': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'Attack_label': 149999 duplicate values (100.0%) - Consider dropping\n",
      "Column 'Attack_type': 149999 duplicate values (100.0%) - Consider dropping\n",
      "\n",
      "Final merged dataframe shape: (150000, 63)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# List of all dataframes\n",
    "dataframes = [\n",
    "    df_normal_Distanc,\n",
    "    df_normal_Flame_sensor,\n",
    "    df_normal_Heart_Rate,\n",
    "    df_normal_IR_Receiver,\n",
    "    df_normal_Modbus,\n",
    "    df_normal_phValue,\n",
    "    df_normal_Soil_moisture,\n",
    "    df_normal_Sound_Sensor,\n",
    "    df_normal_Temperature_and_Humidity,\n",
    "    df_normal_Water_level\n",
    "]\n",
    "\n",
    "# Dataframe names for reference\n",
    "df_names = [\n",
    "    \"df_normal_Distanc\",\n",
    "    \"df_normal_Flame_sensor\",\n",
    "    \"df_normal_Heart_Rate\",\n",
    "    \"df_normal_IR_Receiver\",\n",
    "    \"df_normal_Modbus\",\n",
    "    \"df_normal_phValue\",\n",
    "    \"df_normal_Soil_moisture\",\n",
    "    \"df_normal_Sound_Sensor\",\n",
    "    \"df_normal_Temperature_and_Humidity\",\n",
    "    \"df_normal_Water_level\"\n",
    "]\n",
    "\n",
    "# Function to intelligently sample from a dataframe to reduce duplicates\n",
    "def smart_sample(df, n_samples=15000, name=\"\"):\n",
    "    print(f\"Processing {name} with {df.shape[0]} rows...\")\n",
    "    \n",
    "    # Check if dataframe has enough rows\n",
    "    if df.shape[0] < n_samples:\n",
    "        print(f\"  Warning: {name} has fewer than {n_samples} rows, will use replacement sampling\")\n",
    "        return df.sample(n=n_samples, replace=True, random_state=42)\n",
    "    \n",
    "    # Strategy 1: Try to drop exact duplicates first\n",
    "    df_deduplicated = df.drop_duplicates()\n",
    "    print(f\"  After deduplication: {df_deduplicated.shape[0]} rows\")\n",
    "    \n",
    "    # If we still have enough rows after deduplication, sample from deduplicated set\n",
    "    if df_deduplicated.shape[0] >= n_samples:\n",
    "        print(f\"  Sampling {n_samples} from {df_deduplicated.shape[0]} deduplicated rows\")\n",
    "        return df_deduplicated.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    # Strategy 2: If we don't have enough rows after deduplication,\n",
    "    # take all unique rows and then sample the rest with stratification if possible\n",
    "    unique_rows = df_deduplicated.shape[0]\n",
    "    remaining_needed = n_samples - unique_rows\n",
    "    print(f\"  Taking all {unique_rows} unique rows and sampling {remaining_needed} more\")\n",
    "    \n",
    "    # Try to stratify by some meaningful column to increase diversity\n",
    "    stratify_by = None\n",
    "    categorical_columns = []\n",
    "    \n",
    "    # Find a good column to stratify by (one with moderate cardinality)\n",
    "    for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        n_values = df[col].nunique()\n",
    "        if 2 <= n_values <= 50:  # A reasonable number of categories\n",
    "            categorical_columns.append((col, n_values))\n",
    "    \n",
    "    if categorical_columns:\n",
    "        # Choose the column with the most balanced distribution\n",
    "        stratify_by = sorted(categorical_columns, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        print(f\"  Stratifying by column: {stratify_by} with {df[stratify_by].nunique()} unique values\")\n",
    "        \n",
    "        # Get remaining samples with stratification\n",
    "        remaining_df = df[~df.index.isin(df_deduplicated.index)]\n",
    "        \n",
    "        try:\n",
    "            remaining_samples = remaining_df.sample(\n",
    "                n=remaining_needed, \n",
    "                replace=True, \n",
    "                stratify=remaining_df[stratify_by],\n",
    "                random_state=42\n",
    "            )\n",
    "        except ValueError:\n",
    "            # Fall back to regular sampling if stratification fails\n",
    "            print(\"  Stratification failed, using regular sampling\")\n",
    "            remaining_samples = remaining_df.sample(n=remaining_needed, replace=True, random_state=42)\n",
    "    else:\n",
    "        # No good stratification column found, use regular sampling\n",
    "        remaining_df = df[~df.index.isin(df_deduplicated.index)]\n",
    "        remaining_samples = remaining_df.sample(n=remaining_needed, replace=True, random_state=42)\n",
    "    \n",
    "    # Combine unique rows with additional sampled rows\n",
    "    final_sample = pd.concat([df_deduplicated, remaining_samples])\n",
    "    \n",
    "    # Final shuffle\n",
    "    return final_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Sample from each dataframe using our smart sampling approach\n",
    "samples = []\n",
    "for df, name in zip(dataframes, df_names):\n",
    "    sample = smart_sample(df, n_samples=15000, name=name)\n",
    "    \n",
    "    # For all dataframes except the first one, we'll only keep the data (not headers)\n",
    "    if len(samples) > 0:\n",
    "        sample_values = sample.values\n",
    "        samples.append(sample_values)\n",
    "    else:\n",
    "        # For the first dataframe, we keep the full dataframe with headers\n",
    "        samples.append(sample)\n",
    "\n",
    "# Combine all samples\n",
    "merged_df = samples[0]\n",
    "\n",
    "# Append other samples as rows\n",
    "for sample_array in samples[1:]:\n",
    "    temp_df = pd.DataFrame(sample_array, columns=merged_df.columns)\n",
    "    merged_df = pd.concat([merged_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Final deduplication on the merged dataframe\n",
    "print(f\"\\nBefore final deduplication: {merged_df.shape[0]} rows\")\n",
    "merged_df_deduplicated = merged_df.drop_duplicates()\n",
    "print(f\"After final deduplication: {merged_df_deduplicated.shape[0]} rows\")\n",
    "print(f\"Removed {merged_df.shape[0] - merged_df_deduplicated.shape[0]} duplicate rows\")\n",
    "\n",
    "# Calculate duplicate percentage per column to identify problematic fields\n",
    "print(\"\\nDuplicate values percentage per column (after final merge):\")\n",
    "total_rows = merged_df_deduplicated.shape[0]\n",
    "for column in merged_df_deduplicated.columns:\n",
    "    duplicates = merged_df_deduplicated[column].duplicated().sum()\n",
    "    dup_percentage = (duplicates / total_rows) * 100\n",
    "    if dup_percentage > 90:\n",
    "        print(f\"Column '{column}': {duplicates} duplicate values ({dup_percentage:.1f}%) - Consider dropping\")\n",
    "    elif dup_percentage > 50:\n",
    "        print(f\"Column '{column}': {duplicates} duplicate values ({dup_percentage:.1f}%)\")\n",
    "\n",
    "# If too many duplicates remain, consider dropping highly-duplicated columns\n",
    "high_dup_columns = []\n",
    "for column in merged_df_deduplicated.columns:\n",
    "    dup_percentage = (merged_df_deduplicated[column].duplicated().sum() / total_rows) * 100\n",
    "    if dup_percentage > 99:  # Extreme duplication\n",
    "        high_dup_columns.append(column)\n",
    "\n",
    "if high_dup_columns and len(high_dup_columns) < len(merged_df_deduplicated.columns) // 2:\n",
    "    print(f\"\\nConsidering dropping {len(high_dup_columns)} columns with >99% duplication\")\n",
    "    # Uncomment to actually drop the columns:\n",
    "    # merged_df_deduplicated = merged_df_deduplicated.drop(columns=high_dup_columns)\n",
    "    # print(f\"After dropping highly duplicated columns: {merged_df_deduplicated.shape}\")\n",
    "\n",
    "# Final shuffle to ensure rows from different sources are well-mixed\n",
    "merged_df_final = merged_df_deduplicated.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"\\nFinal merged dataframe shape: {merged_df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_normal_Distanc: Found 3 informative columns out of 63\n",
      "df_normal_Flame_sensor: Found 3 informative columns out of 63\n",
      "df_normal_Heart_Rate: Found 3 informative columns out of 63\n",
      "df_normal_IR_Receiver: Found 3 informative columns out of 63\n",
      "df_normal_Modbus: Found 2 informative columns out of 63\n",
      "df_normal_phValue: Found 3 informative columns out of 63\n",
      "df_normal_Soil_moisture: Found 3 informative columns out of 63\n",
      "df_normal_Sound_Sensor: Found 3 informative columns out of 63\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "could not convert string to float: ' 2021 11:44:10.081753000 '",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:85\u001b[39m, in \u001b[36mdisallow.__call__.<locals>._f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:1007\u001b[39m, in \u001b[36mnanvar\u001b[39m\u001b[34m(values, axis, skipna, ddof, mask)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# xref GH10242\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[38;5;66;03m# Compute variance via two-pass algorithm, which is stable against\u001b[39;00m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# cancellation errors and relatively accurate for small numbers of\u001b[39;00m\n\u001b[32m   1004\u001b[39m \u001b[38;5;66;03m# observations.\u001b[39;00m\n\u001b[32m   1005\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[38;5;66;03m# See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m avg = _ensure_numeric(\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m) / count\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:49\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     48\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: ' 2021 11:44:10.081753000 '",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m all_informative_columns = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dataframes, df_names):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     informative_cols = \u001b[43midentify_informative_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.98\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(informative_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m informative columns out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m     all_informative_columns.update(informative_cols)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36midentify_informative_columns\u001b[39m\u001b[34m(df, threshold)\u001b[39m\n\u001b[32m     52\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Calculate variance for each column\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m variances = \u001b[43mdf_numeric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Normalize variance to 0-1 scale\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m variances.empty \u001b[38;5;129;01mand\u001b[39;00m variances.max() > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11376\u001b[39m, in \u001b[36mDataFrame.var\u001b[39m\u001b[34m(self, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11367\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mvar\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m2\u001b[39m))\n\u001b[32m  11368\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvar\u001b[39m(\n\u001b[32m  11369\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11374\u001b[39m     **kwargs,\n\u001b[32m  11375\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m11376\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[32m  11378\u001b[39m         result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mvar\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:11918\u001b[39m, in \u001b[36mNDFrame.var\u001b[39m\u001b[34m(self, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11910\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvar\u001b[39m(\n\u001b[32m  11911\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  11912\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11916\u001b[39m     **kwargs,\n\u001b[32m  11917\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m11918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function_ddof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  11919\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  11920\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:11894\u001b[39m, in \u001b[36mNDFrame._stat_function_ddof\u001b[39m\u001b[34m(self, name, func, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11891\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m  11892\u001b[39m     axis = \u001b[32m0\u001b[39m\n\u001b[32m> \u001b[39m\u001b[32m11894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  11895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddof\u001b[49m\n\u001b[32m  11896\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11204\u001b[39m, in \u001b[36mDataFrame._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m  11200\u001b[39m     df = df.T\n\u001b[32m  11202\u001b[39m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[32m  11203\u001b[39m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11204\u001b[39m res = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11205\u001b[39m out = df._constructor_from_mgr(res, axes=res.axes).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m  11206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out.dtype != \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:1459\u001b[39m, in \u001b[36mBlockManager.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m   1457\u001b[39m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1459\u001b[39m     nbs = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1460\u001b[39m     res_blocks.extend(nbs)\n\u001b[32m   1462\u001b[39m index = Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:377\u001b[39m, in \u001b[36mBlock.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    373\u001b[39m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    380\u001b[39m         res_values = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11136\u001b[39m, in \u001b[36mDataFrame._reduce.<locals>.blk_func\u001b[39m\u001b[34m(values, axis)\u001b[39m\n\u001b[32m  11134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array([result])\n\u001b[32m  11135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m> \u001b[39m\u001b[32m11136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:92\u001b[39m, in \u001b[36mdisallow.__call__.<locals>._f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[32m0\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: could not convert string to float: ' 2021 11:44:10.081753000 '"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# List of all dataframes\n",
    "dataframes = [\n",
    "    df_normal_Distanc,\n",
    "    df_normal_Flame_sensor,\n",
    "    df_normal_Heart_Rate,\n",
    "    df_normal_IR_Receiver,\n",
    "    df_normal_Modbus,\n",
    "    df_normal_phValue,\n",
    "    df_normal_Soil_moisture,\n",
    "    df_normal_Sound_Sensor,\n",
    "    df_normal_Temperature_and_Humidity,\n",
    "    df_normal_Water_level\n",
    "]\n",
    "\n",
    "# Dataframe names for reference\n",
    "df_names = [\n",
    "    \"df_normal_Distanc\",\n",
    "    \"df_normal_Flame_sensor\",\n",
    "    \"df_normal_Heart_Rate\",\n",
    "    \"df_normal_IR_Receiver\",\n",
    "    \"df_normal_Modbus\",\n",
    "    \"df_normal_phValue\",\n",
    "    \"df_normal_Soil_moisture\",\n",
    "    \"df_normal_Sound_Sensor\",\n",
    "    \"df_normal_Temperature_and_Humidity\",\n",
    "    \"df_normal_Water_level\"\n",
    "]\n",
    "\n",
    "# Step 1: Identify high-information columns before merging\n",
    "def identify_informative_columns(df, threshold=0.99):\n",
    "    \"\"\"\n",
    "    Identify columns that have good variance (not mostly the same value)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        threshold: Threshold for variance (0-1), higher means more strict filtering\n",
    "    \n",
    "    Returns:\n",
    "        List of column names with good variance\n",
    "    \"\"\"\n",
    "    # Convert to numeric where possible for variance calculation\n",
    "    df_numeric = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                df_numeric[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Calculate variance for each column\n",
    "    variances = df_numeric.var()\n",
    "    \n",
    "    # Normalize variance to 0-1 scale\n",
    "    if not variances.empty and variances.max() > 0:\n",
    "        normalized_variances = variances / variances.max()\n",
    "    else:\n",
    "        normalized_variances = variances\n",
    "    \n",
    "    # For categorical columns, calculate entropy/uniqueness\n",
    "    categorical_scores = {}\n",
    "    for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        # Calculate ratio of unique values\n",
    "        unique_ratio = df[col].nunique() / len(df)\n",
    "        categorical_scores[col] = unique_ratio\n",
    "    \n",
    "    # Combine numeric and categorical information\n",
    "    informative_columns = []\n",
    "    for col in df.columns:\n",
    "        if col in normalized_variances and normalized_variances[col] > (1 - threshold):\n",
    "            informative_columns.append(col)\n",
    "        elif col in categorical_scores and categorical_scores[col] > (1 - threshold):\n",
    "            informative_columns.append(col)\n",
    "    \n",
    "    return informative_columns\n",
    "\n",
    "# First, find columns with good variance across all dataframes\n",
    "all_informative_columns = set()\n",
    "for df, name in zip(dataframes, df_names):\n",
    "    informative_cols = identify_informative_columns(df, threshold=0.98)\n",
    "    print(f\"{name}: Found {len(informative_cols)} informative columns out of {df.shape[1]}\")\n",
    "    all_informative_columns.update(informative_cols)\n",
    "\n",
    "print(f\"\\nTotal unique informative columns across all dataframes: {len(all_informative_columns)}\")\n",
    "print(f\"Informative columns: {sorted(all_informative_columns)}\")\n",
    "\n",
    "# Sample rows from each dataframe, focusing on the informative columns\n",
    "samples = []\n",
    "for df, name in zip(dataframes, df_names):\n",
    "    # Get columns that exist in this dataframe\n",
    "    available_cols = [col for col in all_informative_columns if col in df.columns]\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No informative columns found in {name}, using all columns\")\n",
    "        available_cols = df.columns.tolist()\n",
    "    \n",
    "    print(f\"Sampling from {name} using {len(available_cols)} columns\")\n",
    "    \n",
    "    # Sample rows\n",
    "    if df.shape[0] >= 15000:\n",
    "        sample = df.sample(n=15000, random_state=42)\n",
    "    else:\n",
    "        sample = df.sample(n=15000, replace=True, random_state=42)\n",
    "    \n",
    "    # Keep only informative columns\n",
    "    sample = sample[available_cols]\n",
    "    \n",
    "    if len(samples) > 0:\n",
    "        sample_values = sample.values\n",
    "        samples.append((sample_values, available_cols))\n",
    "    else:\n",
    "        samples.append((sample, available_cols))\n",
    "\n",
    "# Combine samples, handling different column sets\n",
    "# First, create an empty DataFrame with all informative columns\n",
    "merged_df = pd.DataFrame(columns=sorted(all_informative_columns))\n",
    "\n",
    "# Add data from each sample\n",
    "for i, (sample_data, sample_cols) in enumerate(samples):\n",
    "    if i == 0:\n",
    "        # First dataframe\n",
    "        temp_df = sample_data if isinstance(sample_data, pd.DataFrame) else pd.DataFrame(sample_data, columns=sample_cols)\n",
    "    else:\n",
    "        # Other dataframes\n",
    "        temp_df = pd.DataFrame(sample_data, columns=sample_cols)\n",
    "    \n",
    "    # Add missing columns with NaN values\n",
    "    for col in all_informative_columns:\n",
    "        if col not in temp_df.columns:\n",
    "            temp_df[col] = np.nan\n",
    "    \n",
    "    # Append to merged dataframe\n",
    "    merged_df = pd.concat([merged_df, temp_df[sorted(all_informative_columns)]], ignore_index=True)\n",
    "\n",
    "# Final cleanup - remove columns that still have high duplication\n",
    "print(\"\\nAnalyzing final merged dataset for remaining duplicated columns...\")\n",
    "total_rows = merged_df.shape[0]\n",
    "columns_to_drop = []\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    if merged_df[column].dtype != 'object':  # Skip object columns for this check\n",
    "        # For numeric columns, check if most values are the same\n",
    "        value_counts = merged_df[column].value_counts(normalize=True)\n",
    "        most_common_pct = value_counts.iloc[0] if not value_counts.empty else 0\n",
    "        \n",
    "        if most_common_pct > 0.99:\n",
    "            columns_to_drop.append(column)\n",
    "            print(f\"Column '{column}': {most_common_pct*100:.1f}% same value - Dropping\")\n",
    "    else:\n",
    "        # For object columns, check duplicate percentage\n",
    "        duplicates = merged_df[column].duplicated().sum()\n",
    "        dup_percentage = (duplicates / total_rows) * 100\n",
    "        \n",
    "        if dup_percentage > 99:\n",
    "            columns_to_drop.append(column)\n",
    "            print(f\"Column '{column}': {dup_percentage:.1f}% duplicates - Dropping\")\n",
    "\n",
    "# Drop highly duplicated columns\n",
    "if columns_to_drop:\n",
    "    merged_df_cleaned = merged_df.drop(columns=columns_to_drop)\n",
    "    print(f\"\\nRemoved {len(columns_to_drop)} highly duplicated columns\")\n",
    "    print(f\"Final shape after cleaning: {merged_df_cleaned.shape}\")\n",
    "else:\n",
    "    merged_df_cleaned = merged_df\n",
    "    print(\"\\nNo additional columns needed to be dropped\")\n",
    "\n",
    "# Final shuffle\n",
    "merged_df_final = merged_df_cleaned.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print statistics about the final dataset\n",
    "print(\"\\nFinal Dataset Statistics:\")\n",
    "print(f\"Total rows: {merged_df_final.shape[0]}\")\n",
    "print(f\"Total columns: {merged_df_final.shape[1]}\")\n",
    "print(f\"Columns: {sorted(merged_df_final.columns.tolist())}\")\n",
    "\n",
    "# Calculate missing values\n",
    "missing_values = merged_df_final.isnull().sum()\n",
    "cols_with_missing = missing_values[missing_values > 0]\n",
    "if not cols_with_missing.empty:\n",
    "    print(f\"\\nColumns with missing values:\")\n",
    "    for col, count in cols_with_missing.items():\n",
    "        print(f\"  {col}: {count} missing values ({count/total_rows*100:.1f}%)\")\n",
    "\n",
    "# Now merged_df_final contains the cleaned dataset with only informative columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique columns across all dataframes: 63\n",
      "Processing df_normal_Distanc with 1143540 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "Processing df_normal_Flame_sensor with 1070196 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_Flame_sensor\n",
      "Processing df_normal_Heart_Rate with 165319 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_Heart_Rate\n",
      "Processing df_normal_IR_Receiver with 1307778 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_IR_Receiver\n",
      "Processing df_normal_Modbus with 159502 rows...\n",
      "  Using column 'tcp.ack' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_Modbus\n",
      "Processing df_normal_phValue with 746908 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_phValue\n",
      "Processing df_normal_Soil_moisture with 1192777 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_Soil_moisture\n",
      "Processing df_normal_Sound_Sensor with 1512883 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_Sound_Sensor\n",
      "Processing df_normal_Temperature_and_Humidity with 1615722 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_Temperature_and_Humidity\n",
      "Processing df_normal_Water_level with 2295288 rows...\n",
      "  Using column 'tcp.ack_raw' with highest variance for systematic sampling\n",
      "  Using 63 columns from df_normal_Water_level\n",
      "\n",
      "Duplicate values percentage per column (after final merge):\n",
      "Column 'frame.time': 14992 duplicate values (10.0%) \n",
      "Column 'ip.src_host': 149971 duplicate values (100.0%) VERY HIGH\n",
      "Column 'ip.dst_host': 149973 duplicate values (100.0%) VERY HIGH\n",
      "Column 'arp.dst.proto_ipv4': 149983 duplicate values (100.0%) VERY HIGH\n",
      "Column 'arp.opcode': 149995 duplicate values (100.0%) VERY HIGH\n",
      "Column 'arp.hw.size': 149991 duplicate values (100.0%) VERY HIGH\n",
      "Column 'arp.src.proto_ipv4': 149985 duplicate values (100.0%) VERY HIGH\n",
      "Column 'icmp.checksum': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'icmp.seq_le': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'icmp.transmit_timestamp': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'icmp.unused': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.file_data': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.content_length': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.request.uri.query': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.request.method': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.referer': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.request.full_uri': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.request.version': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.response': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'http.tls_port': 135912 duplicate values (90.6%) MODERATE\n",
      "Column 'tcp.ack': 133030 duplicate values (88.7%) \n",
      "Column 'tcp.ack_raw': 28807 duplicate values (19.2%) \n",
      "Column 'tcp.checksum': 92830 duplicate values (61.9%) \n",
      "Column 'tcp.connection.fin': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'tcp.connection.rst': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'tcp.connection.syn': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'tcp.connection.synack': 149922 duplicate values (99.9%) VERY HIGH\n",
      "Column 'tcp.dstport': 133879 duplicate values (89.3%) \n",
      "Column 'tcp.flags': 149990 duplicate values (100.0%) VERY HIGH\n",
      "Column 'tcp.flags.ack': 149896 duplicate values (99.9%) VERY HIGH\n",
      "Column 'tcp.len': 149768 duplicate values (99.8%) VERY HIGH\n",
      "Column 'tcp.options': 140387 duplicate values (93.6%) MODERATE\n",
      "Column 'tcp.payload': 133518 duplicate values (89.0%) \n",
      "Column 'tcp.seq': 147328 duplicate values (98.2%) HIGH\n",
      "Column 'tcp.srcport': 133727 duplicate values (89.2%) \n",
      "Column 'udp.port': 149874 duplicate values (99.9%) VERY HIGH\n",
      "Column 'udp.stream': 149719 duplicate values (99.8%) VERY HIGH\n",
      "Column 'udp.time_delta': 149797 duplicate values (99.9%) VERY HIGH\n",
      "Column 'dns.qry.name': 149978 duplicate values (100.0%) VERY HIGH\n",
      "Column 'dns.qry.name.len': 149991 duplicate values (100.0%) VERY HIGH\n",
      "Column 'dns.qry.qu': 149996 duplicate values (100.0%) VERY HIGH\n",
      "Column 'dns.qry.type': 149997 duplicate values (100.0%) VERY HIGH\n",
      "Column 'dns.retransmission': 149997 duplicate values (100.0%) VERY HIGH\n",
      "Column 'dns.retransmit_request': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'dns.retransmit_request_in': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.conack.flags': 149997 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.conflag.cleansess': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.conflags': 149997 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.hdrflags': 149994 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.len': 149973 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.msg_decoded_as': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.msg': 149637 duplicate values (99.8%) VERY HIGH\n",
      "Column 'mqtt.msgtype': 149995 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.proto_len': 149996 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.protoname': 149997 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.topic': 149989 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.topic_len': 149992 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mqtt.ver': 149986 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mbtcp.len': 141532 duplicate values (94.4%) MODERATE\n",
      "Column 'mbtcp.trans_id': 149987 duplicate values (100.0%) VERY HIGH\n",
      "Column 'mbtcp.unit_id': 149998 duplicate values (100.0%) VERY HIGH\n",
      "Column 'Attack_label': 149999 duplicate values (100.0%) VERY HIGH\n",
      "Column 'Attack_type': 149999 duplicate values (100.0%) VERY HIGH\n",
      "\n",
      "Final merged dataframe shape: (150000, 63)\n",
      "Number of columns: 63\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List of all dataframes\n",
    "dataframes = [\n",
    "    df_normal_Distanc,\n",
    "    df_normal_Flame_sensor,\n",
    "    df_normal_Heart_Rate,\n",
    "    df_normal_IR_Receiver,\n",
    "    df_normal_Modbus,\n",
    "    df_normal_phValue,\n",
    "    df_normal_Soil_moisture,\n",
    "    df_normal_Sound_Sensor,\n",
    "    df_normal_Temperature_and_Humidity,\n",
    "    df_normal_Water_level\n",
    "]\n",
    "\n",
    "# Dataframe names for reference\n",
    "df_names = [\n",
    "    \"df_normal_Distanc\",\n",
    "    \"df_normal_Flame_sensor\",\n",
    "    \"df_normal_Heart_Rate\",\n",
    "    \"df_normal_IR_Receiver\",\n",
    "    \"df_normal_Modbus\",\n",
    "    \"df_normal_phValue\",\n",
    "    \"df_normal_Soil_moisture\",\n",
    "    \"df_normal_Sound_Sensor\",\n",
    "    \"df_normal_Temperature_and_Humidity\",\n",
    "    \"df_normal_Water_level\"\n",
    "]\n",
    "\n",
    "# Function to sample with diverse selection strategy\n",
    "def diverse_sample(df, n_samples=15000, name=\"\"):\n",
    "    print(f\"Processing {name} with {df.shape[0]} rows...\")\n",
    "    \n",
    "    # Check if dataframe has enough rows\n",
    "    if df.shape[0] < n_samples:\n",
    "        print(f\"  Warning: {name} has fewer than {n_samples} rows, will use replacement sampling\")\n",
    "        return df.sample(n=n_samples, replace=True, random_state=42)\n",
    "    \n",
    "    # Try to get a diverse sample by:\n",
    "    # 1. Sorting by a column with high variance (if available)\n",
    "    # 2. Taking a systematic sample\n",
    "    \n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    if numeric_cols:\n",
    "        # Find the column with the highest variance\n",
    "        variances = df[numeric_cols].var()\n",
    "        if not variances.empty:\n",
    "            highest_var_col = variances.idxmax()\n",
    "            print(f\"  Using column '{highest_var_col}' with highest variance for systematic sampling\")\n",
    "            \n",
    "            # Sort by the highest variance column\n",
    "            df_sorted = df.sort_values(by=highest_var_col)\n",
    "            \n",
    "            # Take systematic sample\n",
    "            step = len(df_sorted) // n_samples\n",
    "            systematic_indices = np.arange(0, len(df_sorted), step)[:n_samples]\n",
    "            \n",
    "            # If we don't have enough indices, fill the rest randomly\n",
    "            if len(systematic_indices) < n_samples:\n",
    "                remaining = n_samples - len(systematic_indices)\n",
    "                all_indices = set(range(len(df_sorted)))\n",
    "                available_indices = list(all_indices - set(systematic_indices))\n",
    "                additional_indices = np.random.choice(available_indices, size=remaining, replace=False)\n",
    "                systematic_indices = np.concatenate([systematic_indices, additional_indices])\n",
    "            \n",
    "            return df_sorted.iloc[systematic_indices].sample(frac=1, random_state=42)\n",
    "    \n",
    "    # If no numeric columns or the above failed, try stratified sampling if possible\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    if categorical_cols:\n",
    "        # Find a column with a reasonable number of categories\n",
    "        for col in categorical_cols:\n",
    "            n_categories = df[col].nunique()\n",
    "            if 2 <= n_categories <= 50:\n",
    "                print(f\"  Using stratified sampling with column '{col}' ({n_categories} categories)\")\n",
    "                try:\n",
    "                    return df.groupby(col, group_keys=False).apply(\n",
    "                        lambda x: x.sample(\n",
    "                            min(len(x), int(np.ceil(n_samples / n_categories))),\n",
    "                            random_state=42\n",
    "                        )\n",
    "                    ).sample(n_samples, random_state=42, replace=True)\n",
    "                except:\n",
    "                    # If stratification fails, continue to next method\n",
    "                    pass\n",
    "    \n",
    "    # If all else fails, resort to random sampling with a twist\n",
    "    print(\"  Using enhanced random sampling\")\n",
    "    \n",
    "    # First try to get a chunk of ordered data to preserve some patterns\n",
    "    pattern_size = min(n_samples // 3, df.shape[0] // 3)\n",
    "    pattern_chunk = df.iloc[:pattern_size]\n",
    "    \n",
    "    # Get remaining rows through random sampling\n",
    "    remaining_needed = n_samples - pattern_size\n",
    "    random_chunk = df.iloc[pattern_size:].sample(n=remaining_needed, random_state=42)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    combined = pd.concat([pattern_chunk, random_chunk])\n",
    "    return combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Make sure all dataframes have the same columns by identifying the superset\n",
    "all_columns = set()\n",
    "for df in dataframes:\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "print(f\"Total unique columns across all dataframes: {len(all_columns)}\")\n",
    "\n",
    "# Sample rows from each dataframe using diverse sampling\n",
    "samples = []\n",
    "\n",
    "# First, get a reference dataframe (the first one)\n",
    "first_df = diverse_sample(dataframes[0], n_samples=15000, name=df_names[0])\n",
    "reference_columns = first_df.columns.tolist()\n",
    "samples.append(first_df)\n",
    "\n",
    "# Process the rest of the dataframes\n",
    "for i, (df, name) in enumerate(zip(dataframes[1:], df_names[1:]), 1):\n",
    "    sample = diverse_sample(df, n_samples=15000, name=name)\n",
    "    \n",
    "    # Keep only the columns that match the reference\n",
    "    matching_columns = [col for col in reference_columns if col in sample.columns]\n",
    "    \n",
    "    # Log what columns we're keeping\n",
    "    print(f\"  Using {len(matching_columns)} columns from {name}\")\n",
    "    \n",
    "    # Convert to values array to concatenate\n",
    "    sample_values = sample[matching_columns].values\n",
    "    samples.append(sample_values)\n",
    "\n",
    "# Combine all samples\n",
    "merged_df = samples[0]\n",
    "\n",
    "# Append other samples as rows\n",
    "for sample_array in samples[1:]:\n",
    "    temp_df = pd.DataFrame(sample_array, columns=matching_columns)\n",
    "    merged_df = pd.concat([merged_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Final shuffle\n",
    "merged_df_final = merged_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print statistics on duplicate values per column\n",
    "print(\"\\nDuplicate values percentage per column (after final merge):\")\n",
    "total_rows = merged_df_final.shape[0]\n",
    "dup_stats = {}\n",
    "\n",
    "for column in merged_df_final.columns:\n",
    "    duplicates = merged_df_final[column].duplicated().sum()\n",
    "    dup_percentage = (duplicates / total_rows) * 100\n",
    "    dup_stats[column] = (duplicates, dup_percentage)\n",
    "    \n",
    "    severity = \"\"\n",
    "    if dup_percentage > 99:\n",
    "        severity = \"VERY HIGH\"\n",
    "    elif dup_percentage > 95:\n",
    "        severity = \"HIGH\"\n",
    "    elif dup_percentage > 90:\n",
    "        severity = \"MODERATE\"\n",
    "    \n",
    "    print(f\"Column '{column}': {duplicates} duplicate values ({dup_percentage:.1f}%) {severity}\")\n",
    "\n",
    "# Final dataset info\n",
    "print(f\"\\nFinal merged dataframe shape: {merged_df_final.shape}\")\n",
    "print(f\"Number of columns: {merged_df_final.shape[1]}\")\n",
    "\n",
    "# Return the final dataframe\n",
    "merged_df = merged_df_final  # This assigns to the variable name you might be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv('Normal_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
