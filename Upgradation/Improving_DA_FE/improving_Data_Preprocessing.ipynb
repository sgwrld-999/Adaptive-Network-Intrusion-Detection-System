{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset with 71401 rows and 45 columns\n",
      "Class distribution before preprocessing:\n",
      "Attack_label\n",
      "1    85.994594\n",
      "0    14.005406\n",
      "Name: proportion, dtype: float64\n",
      "Selected 44 features: ['Unnamed: 0', 'arp.opcode', 'arp.hw.size', 'icmp.checksum', 'icmp.seq_le']...\n",
      "High skewness detected, using PowerTransformer.\n",
      "Saved scaler as power_transformer.pkl\n",
      "Removed 3570 outliers using Local Outlier Factor.\n",
      "Applying ADASYN to balance classes...\n",
      "Class distribution after ADASYN:\n",
      "Attack_label\n",
      "0    50.029598\n",
      "1    49.970402\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training Layer 1: Autoencoder...\n",
      "Epoch 1/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 1.4020 - val_loss: 0.3258 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.4458 - val_loss: 0.6394 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.2857 - val_loss: 1.4902 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.2087 - val_loss: 2.0472 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1672 - val_loss: 2.3929 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1435 - val_loss: 2.6584 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1284 - val_loss: 2.6715 - learning_rate: 5.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1226 - val_loss: 2.6822 - learning_rate: 5.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1172 - val_loss: 2.7744 - learning_rate: 5.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1126 - val_loss: 2.8628 - learning_rate: 5.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1452/1452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1092 - val_loss: 2.5589 - learning_rate: 5.0000e-05\n",
      "\u001b[1m2904/2904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360us/step\n",
      "Dynamic threshold set to: 0.255287352954945\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step\n",
      "Detected 1132 anomalies out of 13567 test samples.\n",
      "Anomaly detection rate: 8.34%\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Created 562 sequences with length 10.\n",
      "\n",
      "Training Layer 2: CNN-BiLSTM...\n",
      "Class weights: {0: 1.0345622119815667, 1: 0.9676724137931034}\n",
      "Epoch 1/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4668 - loss: 0.8772"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.4665 - loss: 0.8767 - val_accuracy: 0.4779 - val_loss: 0.7015 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5722 - loss: 0.7094 - val_accuracy: 0.4779 - val_loss: 0.7033 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6666 - loss: 0.6235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6631 - loss: 0.6252 - val_accuracy: 0.4867 - val_loss: 0.7030 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7282 - loss: 0.5730 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7257 - loss: 0.5754 - val_accuracy: 0.5133 - val_loss: 0.6965 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7592 - loss: 0.5086 - val_accuracy: 0.4956 - val_loss: 0.6932 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8463 - loss: 0.3499 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8463 - loss: 0.3507 - val_accuracy: 0.6372 - val_loss: 0.6648 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9335 - loss: 0.2412 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9257 - loss: 0.2445 - val_accuracy: 0.7257 - val_loss: 0.6325 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9573 - loss: 0.1662 - val_accuracy: 0.6814 - val_loss: 0.6047 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9754 - loss: 0.1391 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9688 - loss: 0.1406 - val_accuracy: 0.7434 - val_loss: 0.5624 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9868 - loss: 0.0831 - val_accuracy: 0.7168 - val_loss: 0.5904 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9741 - loss: 0.0858 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9745 - loss: 0.0887 - val_accuracy: 0.7522 - val_loss: 0.5293 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9860 - loss: 0.0667 - val_accuracy: 0.7168 - val_loss: 0.5886 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9812 - loss: 0.0764 - val_accuracy: 0.7080 - val_loss: 0.5478 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9918 - loss: 0.0495 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9864 - loss: 0.0573 - val_accuracy: 0.7965 - val_loss: 0.4667 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9861 - loss: 0.0552 - val_accuracy: 0.7876 - val_loss: 0.4215 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9785 - loss: 0.0669 - val_accuracy: 0.7345 - val_loss: 0.6264 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9987 - loss: 0.0444 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0424 - val_accuracy: 0.8053 - val_loss: 0.4265 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9926 - loss: 0.0338 - val_accuracy: 0.7965 - val_loss: 0.5202 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0225 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9989 - loss: 0.0251 - val_accuracy: 0.8142 - val_loss: 0.4782 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9969 - loss: 0.0215 - val_accuracy: 0.7965 - val_loss: 0.4982 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9979 - loss: 0.0230 - val_accuracy: 0.8142 - val_loss: 0.4448 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0191 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0242 - val_accuracy: 0.8407 - val_loss: 0.4723 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0173 - val_accuracy: 0.8407 - val_loss: 0.4707 - learning_rate: 2.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0149"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0150 - val_accuracy: 0.8584 - val_loss: 0.4582 - learning_rate: 2.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0198 - val_accuracy: 0.8584 - val_loss: 0.4490 - learning_rate: 2.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0148 - val_accuracy: 0.8584 - val_loss: 0.4513 - learning_rate: 2.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0131 - val_accuracy: 0.8584 - val_loss: 0.4588 - learning_rate: 2.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0135 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 0.8673 - val_loss: 0.4715 - learning_rate: 2.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 0.8673 - val_loss: 0.4781 - learning_rate: 2.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 0.8584 - val_loss: 0.4822 - learning_rate: 4.0000e-05\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8036 - loss: 0.4054 \n",
      "Test accuracy: 0.7876\n",
      "Test loss: 0.4215\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.77        55\n",
      "           1       0.77      0.83      0.80        58\n",
      "\n",
      "    accuracy                           0.79       113\n",
      "   macro avg       0.79      0.79      0.79       113\n",
      "weighted avg       0.79      0.79      0.79       113\n",
      "\n",
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n",
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n",
      "\n",
      "Total execution time: 263.75 seconds (4.40 minutes)\n",
      "Models saved with timestamp: 20250319-132835\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, MaxAbsScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import os\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "# Suppress TensorFlow logs for cleaner output\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# --- EnhancedAdaptiveNIDS: Layer 1 (Autoencoder for Anomaly Detection) ---\n",
    "class EnhancedAdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-4):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_autoencoder()\n",
    "        self.threshold = None\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        encoded = layers.Dense(self.latent_dim, activation='relu')(x)\n",
    "        x = layers.Dense(64, activation='relu')(encoded)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        decoded = layers.Dense(self.input_dim, activation='linear')(x)\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "                            loss='mean_squared_error')\n",
    "        return autoencoder\n",
    "\n",
    "    def train(self, X_train, X_val, epochs=50, batch_size=64):\n",
    "        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        history = self.model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size,\n",
    "                                 validation_data=(X_val, X_val), callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Autoencoder Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('autoencoder_loss.png')\n",
    "        plt.close()\n",
    "        self._set_dynamic_threshold(X_train)\n",
    "        return history\n",
    "\n",
    "    def _set_dynamic_threshold(self, X_data):\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        mse = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        self.threshold = np.percentile(mse, 95)\n",
    "        print(f\"Dynamic threshold set to: {self.threshold}\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(mse, bins=50)\n",
    "        plt.axvline(self.threshold, color='r', linestyle='--', label=f'Threshold: {self.threshold:.6f}')\n",
    "        plt.title('Reconstruction Error Distribution')\n",
    "        plt.xlabel('Mean Squared Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.savefig('error_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "    def detect_anomalies(self, X_data):\n",
    "        if self.threshold is None:\n",
    "            raise ValueError(\"Model hasn't been trained yet. Call train() first.\")\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.square(X_data - reconstructed), axis=1)\n",
    "        anomaly_indices = np.where(errors > self.threshold)[0]\n",
    "        confidence = errors / np.max(errors) if len(errors) > 0 else np.array([])\n",
    "        return X_data[anomaly_indices], anomaly_indices, errors, confidence\n",
    "\n",
    "    def get_encoded_features(self, X_data):\n",
    "        encoder = keras.Model(inputs=self.model.input, outputs=self.model.layers[6].output)\n",
    "        return encoder.predict(X_data)\n",
    "\n",
    "# --- AdaptiveNIDSLayer2: Layer 2 (CNN-BiLSTM for Classification) ---\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self._build_model()\n",
    "        self.class_weights = None\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim))\n",
    "        x = layers.Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-5))(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.Conv1D(128, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        shortcut = layers.Conv1D(128, 1)(inputs)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "        shortcut = layers.MaxPooling1D(4)(shortcut)\n",
    "        x = layers.add([x, shortcut])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, kernel_regularizer=regularizers.l2(1e-5)))(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(32, kernel_regularizer=regularizers.l2(1e-5)))(x)\n",
    "        context_vector = x\n",
    "        dense1 = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(context_vector)\n",
    "        dense1 = layers.BatchNormalization()(dense1)\n",
    "        dense1 = layers.Dropout(0.4)(dense1)\n",
    "        logits = layers.Dense(self.num_classes)(dense1)\n",
    "        temperature = 1.5\n",
    "        scaled_logits = layers.Lambda(lambda x: x / temperature)(logits)\n",
    "        outputs = layers.Activation('softmax')(scaled_logits)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def compute_class_weights(self, y_train):\n",
    "        y_train_int = y_train.astype(int)\n",
    "        unique_classes = np.unique(y_train_int)\n",
    "        class_counts = np.bincount(y_train_int)\n",
    "        total = len(y_train_int)\n",
    "        self.class_weights = {i: total / (len(unique_classes) * count) for i, count in enumerate(class_counts)}\n",
    "        print(\"Class weights:\", self.class_weights)\n",
    "        return self.class_weights\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=50, batch_size=64):\n",
    "        if self.class_weights is None:\n",
    "            self.compute_class_weights(y_train)\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-7),\n",
    "            keras.callbacks.ModelCheckpoint('best_layer2_model.h5', save_best_only=True, monitor='val_accuracy', mode='max'),\n",
    "            keras.callbacks.TensorBoard(log_dir=f'./logs/layer2_{time.strftime(\"%Y%m%d-%H%M%S\")}', histogram_freq=1)\n",
    "        ]\n",
    "        if X_val is not None and y_val is not None:\n",
    "            history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                                     validation_data=(X_val, y_val), callbacks=callbacks,\n",
    "                                     class_weight=self.class_weights)\n",
    "        else:\n",
    "            history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                                     validation_split=0.2, callbacks=callbacks, class_weight=self.class_weights)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        if 'lr' in history.history:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(history.history['lr'], label='Learning Rate')\n",
    "            plt.title('Learning Rate')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.yscale('log')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('layer2_training_metrics.png')\n",
    "        plt.close()\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=1)\n",
    "        print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Test loss: {test_loss:.4f}\")\n",
    "        y_pred_probs = self.model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "        return report, y_pred, y_pred_probs\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        self.model.save(filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return self.model\n",
    "def parallel_scaling(X_chunk, scaler):\n",
    "        return scaler.transform(X_chunk)\n",
    "\n",
    "# --- Improved Feature Engineering and Data Processing ---\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42, use_adasyn=True, top_k_features=50):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing with dynamic scaling, feature selection, and robust outlier handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "    \n",
    "    if df.empty or 'Attack_label' not in df.columns:\n",
    "        raise ValueError(\"Dataset is empty or missing 'Attack_label' column.\")\n",
    "    \n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    y = df['Attack_label'].astype(int)\n",
    "    \n",
    "    print(\"Class distribution before preprocessing:\")\n",
    "    print(y.value_counts(normalize=True) * 100)\n",
    "    \n",
    "    class_counts = y.value_counts()\n",
    "    if class_counts.min() / class_counts.max() < 0.01:\n",
    "        print(\"Warning: Extreme class imbalance detected. Consider adjusting resampling strategy.\")\n",
    "    \n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=min(top_k_features, X.shape[1]))\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    print(f\"Selected {len(selected_features)} features: {selected_features[:5]}...\")\n",
    "    joblib.dump(selector, 'feature_selector.pkl')\n",
    "    \n",
    "    if np.any(np.abs(stats.skew(X_selected)) > 1):\n",
    "        print(\"High skewness detected, using PowerTransformer.\")\n",
    "        scaler = PowerTransformer(method='yeo-johnson')\n",
    "        scaler_filename = 'power_transformer.pkl'\n",
    "    elif np.any(X_selected < 0):\n",
    "        print(\"Negative values detected, using QuantileTransformer.\")\n",
    "        scaler = QuantileTransformer(output_distribution='normal')\n",
    "        scaler_filename = 'quantile_transformer.pkl'\n",
    "    else:\n",
    "        print(\"Using MaxAbsScaler for non-negative data.\")\n",
    "        scaler = MaxAbsScaler()\n",
    "        scaler_filename = 'maxabs_scaler.pkl'\n",
    "    \n",
    "    # Replace multiprocessing with direct scaling\n",
    "    scaler.fit(X_selected)\n",
    "    X_scaled = scaler.transform(X_selected)\n",
    "    \n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"Saved scaler as {scaler_filename}\")\n",
    "    \n",
    "    # Use multiple CPU cores for LOF but avoid nested multiprocessing\n",
    "    n_cores = min(mp.cpu_count(), 4)\n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05, n_jobs=n_cores)\n",
    "    outlier_labels = lof.fit_predict(X_scaled)\n",
    "    outlier_mask = outlier_labels == 1\n",
    "    X_scaled = X_scaled[outlier_mask]\n",
    "    y = y[outlier_mask]\n",
    "    print(f\"Removed {np.sum(~outlier_mask)} outliers using Local Outlier Factor.\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=random_state,\n",
    "        stratify=y if len(np.unique(y)) > 1 else None\n",
    "    )\n",
    "    \n",
    "    if use_adasyn and len(np.unique(y_train)) > 1:\n",
    "        print(\"Applying ADASYN to balance classes...\")\n",
    "        adasyn = ADASYN(random_state=random_state)\n",
    "        X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "        print(\"Class distribution after ADASYN:\")\n",
    "        print(pd.Series(y_train).value_counts(normalize=True) * 100)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "def create_sequences(data, labels, seq_length=10, stride=2, min_seq_variance=0.01):\n",
    "    \"\"\"\n",
    "    Create sequences with dynamic length adjustment based on variance.\n",
    "    \"\"\"\n",
    "    if len(data) < seq_length:\n",
    "        raise ValueError(f\"Data length ({len(data)}) is less than sequence length ({seq_length}).\")\n",
    "    \n",
    "    data_variance = np.var(data, axis=0)\n",
    "    effective_seq_length = seq_length\n",
    "    if np.mean(data_variance) < min_seq_variance:\n",
    "        effective_seq_length = max(5, seq_length // 2)\n",
    "        print(f\"Low variance detected, adjusting sequence length to {effective_seq_length}.\")\n",
    "    \n",
    "    sequences, seq_labels = [], []\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    n_sequences = (len(data) - effective_seq_length) // stride + 1\n",
    "    if n_sequences <= 0:\n",
    "        print(\"Warning: No sequences generated. Using full data as a single sequence.\")\n",
    "        return np.array([data]), np.array([labels[-1]])\n",
    "    \n",
    "    for i in range(0, len(data) - effective_seq_length + 1, stride):\n",
    "        seq = data[i:i + effective_seq_length]\n",
    "        sequences.append(seq)\n",
    "        seq_labels.append(labels[i + effective_seq_length - 1])\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    seq_labels = np.array(seq_labels)\n",
    "    \n",
    "    if sequences.size == 0:\n",
    "        raise ValueError(\"No valid sequences created. Adjust seq_length or stride.\")\n",
    "    \n",
    "    print(f\"Created {len(sequences)} sequences with length {effective_seq_length}.\")\n",
    "    return sequences, seq_labels\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create logs and model directories\n",
    "    os.makedirs('./logs', exist_ok=True)\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Start time is already defined in the notebook\n",
    "    \n",
    "    # Dataset path (update this path as needed)\n",
    "    # Dataset path is already defined in the notebook\n",
    "    # Preprocess data with improved feature engineering\n",
    "    X_train, X_test, y_train, y_test, scaler = preprocess_data(\n",
    "        dataset_path, test_size=0.2, use_adasyn=True, top_k_features=50\n",
    "    )\n",
    "    \n",
    "    # Layer 1: Autoencoder for anomaly detection\n",
    "    print(\"\\nTraining Layer 1: Autoencoder...\")\n",
    "    layer1 = EnhancedAdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "    layer1.train(X_train, X_test, epochs=50, batch_size=64)\n",
    "    \n",
    "    # Detect anomalies using trained autoencoder\n",
    "    anomalies, anomaly_indices, errors, confidence = layer1.detect_anomalies(X_test)\n",
    "    \n",
    "    print(f\"Detected {len(anomaly_indices)} anomalies out of {len(X_test)} test samples.\")\n",
    "    print(f\"Anomaly detection rate: {len(anomaly_indices)/len(X_test)*100:.2f}%\")\n",
    "    \n",
    "    # Get encoded features for anomalies\n",
    "    encoded_features = layer1.get_encoded_features(anomalies)\n",
    "    \n",
    "    # Handle case where no anomalies are detected\n",
    "    if len(anomalies) == 0:\n",
    "        print(\"No anomalies detected. Using top 10% of highest error samples.\")\n",
    "        top_n = int(len(X_test) * 0.1)\n",
    "        sorted_indices = np.argsort(errors)[-top_n:]\n",
    "        anomalies = X_test[sorted_indices]\n",
    "        anomaly_indices = sorted_indices\n",
    "        encoded_features = layer1.get_encoded_features(anomalies)\n",
    "    \n",
    "    # Get original labels for anomalies\n",
    "    if isinstance(y_test, pd.Series):\n",
    "        y_anomalies = y_test.iloc[anomaly_indices]\n",
    "    else:\n",
    "        y_anomalies = y_test[anomaly_indices]\n",
    "    \n",
    "    # Create sequences for Layer 2 with improved sequence creation\n",
    "    X_layer2, y_layer2 = create_sequences(encoded_features, y_anomalies, seq_length=10, stride=2)\n",
    "    \n",
    "    # Split data for Layer 2\n",
    "    if len(np.unique(y_layer2)) > 1:\n",
    "        X_train_l2, X_test_l2, y_train_l2, y_test_l2 = train_test_split(\n",
    "            X_layer2, y_layer2, test_size=0.2, random_state=42, stratify=y_layer2\n",
    "        )\n",
    "    else:\n",
    "        X_train_l2, X_test_l2, y_train_l2, y_test_l2 = train_test_split(\n",
    "            X_layer2, y_layer2, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    # Layer 2: CNN-BiLSTM Classification\n",
    "    print(\"\\nTraining Layer 2: CNN-BiLSTM...\")\n",
    "    layer2 = AdaptiveNIDSLayer2(\n",
    "        input_dim=X_train_l2.shape[2],\n",
    "        num_classes=len(np.unique(y_train_l2)),\n",
    "        seq_length=10\n",
    "    )\n",
    "    layer2.train(X_train_l2, y_train_l2, X_test_l2, y_test_l2, epochs=50, batch_size=32)\n",
    "    \n",
    "    # Evaluate Layer 2\n",
    "    class_names = [f\"Class {i}\" for i in range(len(np.unique(y_train_l2)))]\n",
    "    report, y_pred, y_pred_probs = layer2.evaluate(X_test_l2, y_test_l2)\n",
    "    \n",
    "    # Save models with timestamp\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    layer1.model.save(f'models/autoencoder_model_{timestamp}.h5')\n",
    "    layer2.model.save(f'models/cnn_bilstm_model_{timestamp}.h5')\n",
    "    \n",
    "    # Save model architecture as image (optional)\n",
    "    try:\n",
    "        tf.keras.utils.plot_model(layer1.model, to_file='models/autoencoder_architecture.png',\n",
    "                                  show_shapes=True, show_layer_names=True)\n",
    "        tf.keras.utils.plot_model(layer2.model, to_file='models/cnn_bilstm_architecture.png',\n",
    "                                  show_shapes=True, show_layer_names=True)\n",
    "    except ImportError:\n",
    "        print(\"Couldn't save model architecture images. Install pydot and graphviz.\")\n",
    "    \n",
    "    # Save training configuration\n",
    "    config = {\n",
    "        'dataset_path': dataset_path,\n",
    "        'preprocessing': {\n",
    "            'use_adasyn': True,\n",
    "            'top_k_features': 50,\n",
    "            'test_size': 0.2\n",
    "        },\n",
    "        'layer1': {\n",
    "            'latent_dim': layer1.latent_dim,\n",
    "            'learning_rate': layer1.learning_rate,\n",
    "            'threshold': layer1.threshold\n",
    "        },\n",
    "        'layer2': {\n",
    "            'seq_length': layer2.seq_length,\n",
    "            'class_weights': layer2.class_weights\n",
    "        },\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    with open(f'models/training_config_{timestamp}.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTotal execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "    print(f\"Models saved with timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
