{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before preprocessing: Attack_label\n",
      "1    85.994594\n",
      "0    14.005406\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:111: UserWarning: Features [ 6  9 28 29 30 31 36 41 42 43] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 44 features: ['Unnamed: 0', 'mqtt.hdrflags', 'tcp.dstport', 'mqtt.msgtype', 'mqtt.len']...\n",
      "Removed 1597 outliers.\n",
      "Class distribution after ADASYN: Attack_label\n",
      "0    50.024538\n",
      "1    49.975462\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training Layer 1: Autoencoder...\n",
      "Epoch 1/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.3976 - val_loss: 0.1431 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1510 - val_loss: 0.1055 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1139 - val_loss: 0.0771 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0795 - val_loss: 0.0607 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0625 - val_loss: 0.0507 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0515 - val_loss: 0.0429 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0431 - val_loss: 0.0388 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0342 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0337 - val_loss: 0.0323 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0305 - val_loss: 0.0293 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0283 - val_loss: 0.0289 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0268 - val_loss: 0.0263 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0254 - val_loss: 0.0254 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0242 - val_loss: 0.0239 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0233 - val_loss: 0.0230 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0224 - val_loss: 0.0220 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0216 - val_loss: 0.0197 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0207 - val_loss: 0.0193 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0194 - val_loss: 0.0197 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0178 - val_loss: 0.0254 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0160 - val_loss: 0.0255 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0152 - val_loss: 0.0249 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0146 - val_loss: 0.0235 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0141 - val_loss: 0.0234 - learning_rate: 5.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0138 - val_loss: 0.0221 - learning_rate: 5.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0136 - val_loss: 0.0218 - learning_rate: 5.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0134 - val_loss: 0.0212 - learning_rate: 5.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0132 - val_loss: 0.0213 - learning_rate: 5.0000e-05\n",
      "\u001b[1m2993/2993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 532us/step\n",
      "Dynamic threshold set to: -0.6404252188096081\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step\n",
      "Detected 1150 anomalies out of 13961 samples.\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step\n",
      "Top 5 important features for anomalies:\n",
      "mqtt.topic_len: 1.2129\n",
      "mqtt.ver: 0.7226\n",
      "mqtt.conflags: 0.7213\n",
      "mqtt.proto_len: 0.7211\n",
      "mqtt.conflag.cleansess: 0.7039\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "9",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3790\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3792\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:152\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:181\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 9",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 356\u001b[39m\n\u001b[32m    354\u001b[39m encoded_features = layer1.get_encoded_features(anomalies)\n\u001b[32m    355\u001b[39m y_anomalies = y_test[anomaly_indices] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_test, pd.Series) \u001b[38;5;28;01melse\u001b[39;00m y_test.iloc[anomaly_indices]\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m X_layer2, y_layer2 = \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_anomalies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# Split data for Layer 2\u001b[39;00m\n\u001b[32m    359\u001b[39m X_train_l2, X_test_l2, y_train_l2, y_test_l2 = train_test_split(\n\u001b[32m    360\u001b[39m     X_layer2, y_layer2, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y_layer2 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np.unique(y_layer2)) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    361\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 328\u001b[39m, in \u001b[36mcreate_sequences\u001b[39m\u001b[34m(data, labels, seq_length, stride, min_seq_variance)\u001b[39m\n\u001b[32m    326\u001b[39m     seq = data[i:i + effective_seq_length]\n\u001b[32m    327\u001b[39m     sequences.append(seq)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     seq_labels.append(\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(sequences), np.array(seq_labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:1040\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1043\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:1156\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1155\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3793\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3794\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3795\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3796\u001b[39m     ):\n\u001b[32m   3797\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3798\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3800\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3801\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3802\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3803\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 9"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, MaxAbsScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2, f_classif\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import json\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Suppress TensorFlow logs for cleaner output\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "#### Residual Block for Autoencoder\n",
    "def residual_block(x, units):\n",
    "    \"\"\"Creates a residual block with two dense layers to improve gradient flow.\"\"\"\n",
    "    shortcut = x\n",
    "    x = layers.Dense(units)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(units)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "#### Focal Loss for Classification\n",
    "def focal_loss(gamma=2.0, alpha=None):\n",
    "    \"\"\"Focal loss function to address class imbalance in multi-class classification.\"\"\"\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32) if alpha is not None else None\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        p_t = tf.gather(y_pred, y_true, batch_dims=1)\n",
    "        ce = -tf.math.log(p_t)\n",
    "        alpha_t = tf.gather(alpha, y_true) if alpha is not None else 1.0\n",
    "        focal_weight = alpha_t * tf.math.pow((1 - p_t), gamma)\n",
    "        return focal_weight * ce\n",
    "    return loss\n",
    "\n",
    "#### Data Augmentation\n",
    "def augment_sequence(sequence, noise_level=0.01):\n",
    "    \"\"\"Applies Gaussian noise to a sequence for data augmentation.\"\"\"\n",
    "    noise = np.random.normal(0, noise_level, sequence.shape)\n",
    "    return sequence + noise\n",
    "\n",
    "#### Custom Sequence Generator\n",
    "class SequenceGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Custom sequence generator for efficient batch processing with optional augmentation.\"\"\"\n",
    "    def __init__(self, X, y, batch_size, augment=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_X = self.X[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        if self.augment:\n",
    "            batch_X = np.array([augment_sequence(seq) for seq in batch_X])\n",
    "        return batch_X, batch_y\n",
    "\n",
    "### Layer 1: Autoencoder for Anomaly Detection and Feature Extraction\n",
    "class EnhancedAdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=64, learning_rate=1e-4):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_autoencoder()\n",
    "        self.kde = None\n",
    "        self.threshold = None\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        \"\"\"Builds an Autoencoder with residual connections for stability.\"\"\"\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = layers.Dense(128, activation='relu')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = residual_block(x, 128)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = residual_block(x, 64)\n",
    "        encoded = layers.Dense(self.latent_dim, activation='relu')(x)\n",
    "        x = layers.Dense(64, activation='relu')(encoded)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = residual_block(x, 64)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = residual_block(x, 128)\n",
    "        decoded = layers.Dense(self.input_dim, activation='linear')(x)\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate), loss='huber')\n",
    "        return autoencoder\n",
    "\n",
    "    def train(self, X_train, X_val, epochs=50, batch_size=64):\n",
    "        \"\"\"Trains the Autoencoder and sets a dynamic anomaly threshold.\"\"\"\n",
    "        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        history = self.model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size,\n",
    "                                 validation_data=(X_val, X_val), callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "        self._set_dynamic_threshold(X_train)\n",
    "        return history\n",
    "\n",
    "    def _set_dynamic_threshold(self, X_data):\n",
    "        \"\"\"Sets a dynamic threshold using Kernel Density Estimation.\"\"\"\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.abs(X_data - reconstructed), axis=1)\n",
    "        self.kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(errors.reshape(-1, 1))\n",
    "        log_dens = self.kde.score_samples(errors.reshape(-1, 1))\n",
    "        self.threshold = np.percentile(-log_dens, 95)  # 95th percentile as anomaly threshold\n",
    "        print(f\"Dynamic threshold set to: {self.threshold}\")\n",
    "\n",
    "    def detect_anomalies(self, X_data):\n",
    "        \"\"\"Detects anomalies based on reconstruction errors.\"\"\"\n",
    "        if self.kde is None or self.threshold is None:\n",
    "            raise ValueError(\"Model hasn't been trained yet. Call train() first.\")\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.mean(np.abs(X_data - reconstructed), axis=1)\n",
    "        log_dens = self.kde.score_samples(errors.reshape(-1, 1))\n",
    "        anomaly_indices = np.where(-log_dens > self.threshold)[0]\n",
    "        confidence = -log_dens / np.max(-log_dens) if len(log_dens) > 0 else np.array([])\n",
    "        return X_data[anomaly_indices], anomaly_indices, errors, confidence\n",
    "\n",
    "    def get_encoded_features(self, X_data):\n",
    "        \"\"\"Extracts encoded features from the Autoencoder's latent layer.\"\"\"\n",
    "        encoder = keras.Model(inputs=self.model.input, outputs=self.model.layers[8].output)  # Latent layer index\n",
    "        return encoder.predict(X_data)\n",
    "\n",
    "    def get_feature_importance(self, X_data, anomaly_indices):\n",
    "        \"\"\"Computes feature importance based on reconstruction errors of anomalies.\"\"\"\n",
    "        reconstructed = self.model.predict(X_data)\n",
    "        errors = np.abs(X_data - reconstructed)\n",
    "        anomaly_errors = errors[anomaly_indices]\n",
    "        feature_importance = np.mean(anomaly_errors, axis=0)\n",
    "        return feature_importance\n",
    "\n",
    "### Layer 2: CNN-BiLSTM for Attack Classification\n",
    "class AdaptiveNIDSLayer2:\n",
    "    def __init__(self, input_dim, num_classes, seq_length=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.temperature = tf.Variable(1.0, trainable=True)\n",
    "        self.model = self._build_model()\n",
    "        self.class_weights = None\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Builds a CNN-BiLSTM model with attention mechanism.\"\"\"\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim))\n",
    "        x = layers.Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.Conv1D(128, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        shortcut = layers.Conv1D(128, 1, kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "        shortcut = layers.MaxPooling1D(4)(shortcut)\n",
    "        x = layers.add([x, shortcut])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, kernel_regularizer=regularizers.l2(1e-4)))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(32, return_sequences=True, kernel_regularizer=regularizers.l2(1e-4)))(x)\n",
    "        # Attention mechanism\n",
    "        attention_scores = layers.Dense(1, activation=None)(x)\n",
    "        attention_weights = layers.Softmax(axis=1)(attention_scores)\n",
    "        context_vector = layers.Lambda(lambda inputs: tf.reduce_sum(inputs[0] * inputs[1], axis=1))([x, attention_weights])\n",
    "        dense1 = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(context_vector)\n",
    "        dense1 = layers.BatchNormalization()(dense1)\n",
    "        dense1 = layers.Dropout(0.25)(dense1)\n",
    "        logits = layers.Dense(self.num_classes)(dense1)\n",
    "        scaled_logits = layers.Lambda(lambda x: x / self.temperature)(logits)\n",
    "        outputs = layers.Activation('softmax')(scaled_logits)\n",
    "        return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    def compute_class_weights(self, y_train):\n",
    "        \"\"\"Computes class weights to handle imbalance.\"\"\"\n",
    "        y_train_int = y_train.astype(int)\n",
    "        unique_classes = np.unique(y_train_int)\n",
    "        class_counts = np.bincount(y_train_int)\n",
    "        total = len(y_train_int)\n",
    "        self.class_weights = {i: total / (len(unique_classes) * count) for i, count in enumerate(class_counts)}\n",
    "        print(\"Class weights:\", self.class_weights)\n",
    "        return self.class_weights\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=50, batch_size=64):\n",
    "        \"\"\"Trains the CNN-BiLSTM model with focal loss and gradient clipping.\"\"\"\n",
    "        if self.class_weights is None:\n",
    "            self.compute_class_weights(y_train)\n",
    "        alpha = np.array([self.class_weights.get(i, 1.0) for i in range(self.num_classes)])\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=1e-3, clipvalue=1.0),\n",
    "            loss=focal_loss(gamma=2.0, alpha=alpha),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-7),\n",
    "            keras.callbacks.ModelCheckpoint('best_layer2_model.h5', save_best_only=True, monitor='val_accuracy', mode='max'),\n",
    "            keras.callbacks.TensorBoard(log_dir=f'./logs/layer2_{time.strftime(\"%Y%m%d-%H%M%S\")}', histogram_freq=1)\n",
    "        ]\n",
    "        train_generator = SequenceGenerator(X_train, y_train, batch_size, augment=True)\n",
    "        val_generator = SequenceGenerator(X_val, y_val, batch_size, augment=False) if X_val is not None else None\n",
    "        if val_generator:\n",
    "            history = self.model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=callbacks)\n",
    "        else:\n",
    "            history = self.model.fit(train_generator, epochs=epochs, validation_split=0.2, callbacks=callbacks)\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluates the model and generates performance metrics.\"\"\"\n",
    "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=1)\n",
    "        print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Test loss: {test_loss:.4f}\")\n",
    "        y_pred_probs = self.model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "        return report, y_pred, y_pred_probs\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Saves the trained model to a file.\"\"\"\n",
    "        self.model.save(filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "### Data Preprocessing\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42, use_adasyn=True, top_k_features=50):\n",
    "    \"\"\"Preprocesses data with ensemble feature selection, scaling, and outlier removal.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    if df.empty or 'Attack_label' not in df.columns:\n",
    "        raise ValueError(\"Dataset is empty or missing 'Attack_label' column.\")\n",
    "    X = df.drop(['Attack_label'], axis=1)\n",
    "    y = df['Attack_label'].astype(int)\n",
    "    print(\"Class distribution before preprocessing:\", y.value_counts(normalize=True) * 100)\n",
    "\n",
    "    # Ensemble feature selection\n",
    "    if np.any(X < 0):\n",
    "        score_funcs = [mutual_info_classif, f_classif]\n",
    "    else:\n",
    "        score_funcs = [mutual_info_classif, chi2, f_classif]\n",
    "    feature_scores = []\n",
    "    for score_func in score_funcs:\n",
    "        try:\n",
    "            if score_func == chi2:\n",
    "                scores, _ = score_func(X, y)\n",
    "            else:\n",
    "                scores = score_func(X, y)\n",
    "            scores = np.nan_to_num(scores, nan=0.0)\n",
    "            feature_scores.append(scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {score_func.__name__}: {e}\")\n",
    "            continue\n",
    "    if len(feature_scores) == 0:\n",
    "        raise ValueError(\"No valid feature scores computed.\")\n",
    "    expected_length = X.shape[1]\n",
    "    ranks = [np.argsort(np.argsort(-scores)) for scores in feature_scores if len(scores) == expected_length]\n",
    "    if len(ranks) == 0 or any(len(rank) != expected_length for rank in ranks):\n",
    "        raise ValueError(\"Inconsistent feature ranking lengths.\")\n",
    "    average_rank = np.mean(ranks, axis=0)\n",
    "    selected_indices = np.argsort(average_rank)[:min(top_k_features, X.shape[1])]\n",
    "    X_selected = X.iloc[:, selected_indices]\n",
    "    print(f\"Selected {len(selected_indices)} features: {X.columns[selected_indices][:5].tolist()}...\")\n",
    "\n",
    "    # Scaling\n",
    "    scaler = PowerTransformer(method='yeo-johnson') if np.any(np.abs(stats.skew(X_selected)) > 1) else MaxAbsScaler()\n",
    "    scaler.fit(X_selected)\n",
    "    X_scaled = scaler.transform(X_selected)\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "    # Outlier removal\n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination='auto', n_jobs=min(mp.cpu_count(), 4))\n",
    "    outlier_labels = lof.fit_predict(X_scaled)\n",
    "    outlier_mask = outlier_labels == 1\n",
    "    X_scaled = X_scaled[outlier_mask]\n",
    "    y = y[outlier_mask]\n",
    "    print(f\"Removed {np.sum(~outlier_mask)} outliers.\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=random_state, stratify=y if len(np.unique(y)) > 1 else None\n",
    "    )\n",
    "    if use_adasyn and len(np.unique(y_train)) > 1:\n",
    "        adasyn = ADASYN(random_state=random_state)\n",
    "        X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "        print(\"Class distribution after ADASYN:\", pd.Series(y_train).value_counts(normalize=True) * 100)\n",
    "    return X_train, X_test, y_train, y_test, scaler, X.columns[selected_indices].tolist()\n",
    "\n",
    "### Sequence Creation\n",
    "def create_sequences(data, labels, seq_length=10, stride=2, min_seq_variance=0.01):\n",
    "    \"\"\"Creates sequences from data with adaptive length based on variance.\"\"\"\n",
    "    if len(data) < seq_length:\n",
    "        raise ValueError(f\"Data length ({len(data)}) < sequence length ({seq_length}).\")\n",
    "    effective_seq_length = seq_length if np.mean(np.var(data, axis=0)) >= min_seq_variance else max(5, seq_length // 2)\n",
    "    sequences, seq_labels = [], []\n",
    "    for i in range(0, len(data) - effective_seq_length + 1, stride):\n",
    "        seq = data[i:i + effective_seq_length]\n",
    "        sequences.append(seq)\n",
    "        seq_labels.append(labels[i + effective_seq_length - 1])\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "### Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    os.makedirs('./logs', exist_ok=True)\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    dataset_path = \"training_dataset.csv\"  # Replace with your dataset path\n",
    "    X_train, X_test, y_train, y_test, scaler, selected_features = preprocess_data(dataset_path)\n",
    "\n",
    "    # Train Layer 1: Autoencoder\n",
    "    print(\"\\nTraining Layer 1: Autoencoder...\")\n",
    "    layer1 = EnhancedAdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "    layer1.train(X_train, X_test)\n",
    "    anomalies, anomaly_indices, errors, confidence = layer1.detect_anomalies(X_test)\n",
    "    print(f\"Detected {len(anomaly_indices)} anomalies out of {len(X_test)} samples.\")\n",
    "    feature_importance = layer1.get_feature_importance(X_test, anomaly_indices)\n",
    "    print(\"Top 5 important features for anomalies:\")\n",
    "    sorted_indices = np.argsort(-feature_importance)\n",
    "    for i in sorted_indices[:5]:\n",
    "        print(f\"{selected_features[i]}: {feature_importance[i]:.4f}\")\n",
    "    encoded_features = layer1.get_encoded_features(anomalies)\n",
    "    y_anomalies = y_test[anomaly_indices] if not isinstance(y_test, pd.Series) else y_test.iloc[anomaly_indices]\n",
    "    X_layer2, y_layer2 = create_sequences(encoded_features, y_anomalies)\n",
    "\n",
    "    # Split data for Layer 2\n",
    "    X_train_l2, X_test_l2, y_train_l2, y_test_l2 = train_test_split(\n",
    "        X_layer2, y_layer2, test_size=0.2, random_state=42, stratify=y_layer2 if len(np.unique(y_layer2)) > 1 else None\n",
    "    )\n",
    "\n",
    "    # Train Layer 2: CNN-BiLSTM\n",
    "    print(\"\\nTraining Layer 2: CNN-BiLSTM...\")\n",
    "    layer2 = AdaptiveNIDSLayer2(input_dim=X_train_l2.shape[2], num_classes=len(np.unique(y_train_l2)))\n",
    "    layer2.train(X_train_l2, y_train_l2, X_test_l2, y_test_l2)\n",
    "    report, y_pred, y_pred_probs = layer2.evaluate(X_test_l2, y_test_l2)\n",
    "\n",
    "    # Save models and configuration\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    layer1.model.save(f'models/autoencoder_model_{timestamp}.h5')\n",
    "    layer2.save_model(f'models/cnn_bilstm_model_{timestamp}.h5')\n",
    "    config = {\n",
    "        'dataset_path': dataset_path,\n",
    "        'layer1': {'latent_dim': layer1.latent_dim, 'threshold': layer1.threshold},\n",
    "        'layer2': {'seq_length': layer2.seq_length, 'class_weights': layer2.class_weights},\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    with open(f'models/config_{timestamp}.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    # Execution time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('training_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attack_label\n",
       "1.0    61401\n",
       "0.0    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp[\"Attack_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
