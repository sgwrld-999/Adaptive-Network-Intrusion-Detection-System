{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bb8c562-ff55-43aa-a11a-26badf31fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 38514476.0000 - val_loss: 38364216.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 38753532.0000 - val_loss: 38291340.0000 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 38246884.0000 - val_loss: 38218564.0000 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 38247920.0000 - val_loss: 38145996.0000 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 38397080.0000 - val_loss: 38073584.0000 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 38432724.0000 - val_loss: 38001292.0000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 38087148.0000 - val_loss: 37929000.0000 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 38204964.0000 - val_loss: 37856876.0000 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 38303040.0000 - val_loss: 37784852.0000 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 37993796.0000 - val_loss: 37712824.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37805048.0000 - val_loss: 37640948.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37870872.0000 - val_loss: 37569156.0000 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37805256.0000 - val_loss: 37497416.0000 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 37995712.0000 - val_loss: 37425876.0000 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37752808.0000 - val_loss: 37354328.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37580592.0000 - val_loss: 37282924.0000 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37607556.0000 - val_loss: 37211612.0000 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 37491752.0000 - val_loss: 37140328.0000 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37794592.0000 - val_loss: 37069312.0000 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37058024.0000 - val_loss: 36998160.0000 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37186984.0000 - val_loss: 36927224.0000 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37199648.0000 - val_loss: 36856424.0000 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 36991652.0000 - val_loss: 36785600.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36895596.0000 - val_loss: 36714932.0000 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37024756.0000 - val_loss: 36644336.0000 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36669236.0000 - val_loss: 36573800.0000 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36721252.0000 - val_loss: 36503492.0000 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36989100.0000 - val_loss: 36433228.0000 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36770588.0000 - val_loss: 36363028.0000 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36593536.0000 - val_loss: 36292836.0000 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36406244.0000 - val_loss: 36222804.0000 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36620064.0000 - val_loss: 36152940.0000 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36335952.0000 - val_loss: 36083032.0000 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36314968.0000 - val_loss: 36013360.0000 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36344112.0000 - val_loss: 35943656.0000 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36223200.0000 - val_loss: 35874132.0000 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36215132.0000 - val_loss: 35804676.0000 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35991020.0000 - val_loss: 35735276.0000 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35769736.0000 - val_loss: 35665912.0000 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36065844.0000 - val_loss: 35596872.0000 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36063028.0000 - val_loss: 35527792.0000 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35700016.0000 - val_loss: 35458760.0000 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 35811952.0000 - val_loss: 35389868.0000 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 35394800.0000 - val_loss: 35320972.0000 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35436056.0000 - val_loss: 35252296.0000 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35400128.0000 - val_loss: 35183652.0000 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 35254620.0000 - val_loss: 35115164.0000 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35412452.0000 - val_loss: 35046736.0000 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 35048972.0000 - val_loss: 34978276.0000 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35081528.0000 - val_loss: 34910100.0000 - learning_rate: 0.0010\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 97150638.76216795\n",
      "Model saved to autoencoder_lstm_model.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1033.3725581169128"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Autoencoder-LSTM with CNN Feature Enhancement\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model\n",
    "        \"\"\"\n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Reshape for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # CNN Feature Enhancement\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False\n",
    "        )(x)\n",
    "        \n",
    "        # Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu'\n",
    "        )(x)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=True\n",
    "        )(x)\n",
    "        \n",
    "        # Output Reconstruction\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Create Optimizer with direct learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compile with Adam and MSE Loss\n",
    "        autoencoder.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error threshold\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (float): Percentile for anomaly threshold\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b8c98ba-1a6a-4fc1-84f2-d2c924503165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 38681420.0000 - val_loss: 38291984.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 38536464.0000 - val_loss: 38147552.0000 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 38568508.0000 - val_loss: 38003676.0000 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 38124660.0000 - val_loss: 37859768.0000 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37992204.0000 - val_loss: 37716492.0000 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37917864.0000 - val_loss: 37573440.0000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37806572.0000 - val_loss: 37430924.0000 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37679812.0000 - val_loss: 37288636.0000 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 37540580.0000 - val_loss: 37146968.0000 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37299888.0000 - val_loss: 37005548.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37293312.0000 - val_loss: 36864316.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36971724.0000 - val_loss: 36723668.0000 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37015276.0000 - val_loss: 36583440.0000 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36353008.0000 - val_loss: 36443216.0000 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36679068.0000 - val_loss: 36303660.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36461936.0000 - val_loss: 36164524.0000 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36414984.0000 - val_loss: 36025848.0000 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36273944.0000 - val_loss: 35887408.0000 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 36041560.0000 - val_loss: 35749224.0000 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 36266380.0000 - val_loss: 35611608.0000 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 35858196.0000 - val_loss: 35474412.0000 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 35423640.0000 - val_loss: 35337092.0000 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 35323920.0000 - val_loss: 35200568.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 35320748.0000 - val_loss: 35064404.0000 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 35107124.0000 - val_loss: 34928636.0000 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 35101936.0000 - val_loss: 34793188.0000 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34861508.0000 - val_loss: 34657884.0000 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 34796176.0000 - val_loss: 34523180.0000 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34711284.0000 - val_loss: 34388900.0000 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 34733104.0000 - val_loss: 34255048.0000 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34764228.0000 - val_loss: 34121284.0000 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 11ms/step - loss: 34248676.0000 - val_loss: 33987964.0000 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 10ms/step - loss: 34097716.0000 - val_loss: 33855052.0000 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34199132.0000 - val_loss: 33722760.0000 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34115312.0000 - val_loss: 33590588.0000 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 33654624.0000 - val_loss: 33458542.0000 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 9ms/step - loss: 33703336.0000 - val_loss: 33327236.0000 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 9ms/step - loss: 33560936.0000 - val_loss: 33196336.0000 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 9ms/step - loss: 33149726.0000 - val_loss: 33065376.0000 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 33081868.0000 - val_loss: 32935224.0000 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 33365164.0000 - val_loss: 32805566.0000 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 33115918.0000 - val_loss: 32675798.0000 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 32621948.0000 - val_loss: 32546714.0000 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 32818874.0000 - val_loss: 32417958.0000 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 32802238.0000 - val_loss: 32289754.0000 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 32436474.0000 - val_loss: 32161524.0000 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 32436976.0000 - val_loss: 32034016.0000 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 32253524.0000 - val_loss: 31906604.0000 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 31987502.0000 - val_loss: 31779684.0000 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 31946680.0000 - val_loss: 31653050.0000 - learning_rate: 0.0010\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 90658422.49167582\n",
      "Model saved to autoencoder_lstm_model_1.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1509.0542860031128"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Autoencoder-LSTM with CNN Feature Enhancement\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model\n",
    "        \"\"\"\n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Reshape for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # CNN Feature Enhancement\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False\n",
    "        )(x)\n",
    "        \n",
    "        # Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu'\n",
    "        )(x)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=True\n",
    "        )(x)\n",
    "        \n",
    "        # Output Reconstruction\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Create Optimizer with direct learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compile with Adam and MSE Loss\n",
    "        autoencoder.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=16):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error threshold\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (float): Percentile for anomaly threshold\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model_1.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f0ca220-fa2e-4302-881c-eba665bbdde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - loss: 38557228.0000 - val_loss: 38400276.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38655144.0000 - val_loss: 38363304.0000 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38484036.0000 - val_loss: 38326648.0000 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38300188.0000 - val_loss: 38290116.0000 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38626332.0000 - val_loss: 38253680.0000 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38344596.0000 - val_loss: 38217256.0000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38502892.0000 - val_loss: 38180868.0000 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38246552.0000 - val_loss: 38144472.0000 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38115044.0000 - val_loss: 38108096.0000 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38579616.0000 - val_loss: 38071816.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38319060.0000 - val_loss: 38035516.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38226856.0000 - val_loss: 37999244.0000 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38215220.0000 - val_loss: 37962972.0000 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37919436.0000 - val_loss: 37926712.0000 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38319064.0000 - val_loss: 37890548.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 37962128.0000 - val_loss: 37854348.0000 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37975704.0000 - val_loss: 37818192.0000 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 38145016.0000 - val_loss: 37782072.0000 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 37944628.0000 - val_loss: 37745948.0000 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37883148.0000 - val_loss: 37709848.0000 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37795864.0000 - val_loss: 37673792.0000 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37770824.0000 - val_loss: 37637736.0000 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 38047064.0000 - val_loss: 37601752.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37850448.0000 - val_loss: 37565760.0000 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37846008.0000 - val_loss: 37529772.0000 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37638820.0000 - val_loss: 37493820.0000 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 38010412.0000 - val_loss: 37457928.0000 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37616900.0000 - val_loss: 37421984.0000 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37395276.0000 - val_loss: 37386108.0000 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37556636.0000 - val_loss: 37350256.0000 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37520416.0000 - val_loss: 37314436.0000 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37799088.0000 - val_loss: 37278644.0000 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37567600.0000 - val_loss: 37242840.0000 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37408240.0000 - val_loss: 37207088.0000 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37590668.0000 - val_loss: 37171364.0000 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37495496.0000 - val_loss: 37135628.0000 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37554624.0000 - val_loss: 37099952.0000 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37452076.0000 - val_loss: 37064292.0000 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - loss: 37156888.0000 - val_loss: 37028600.0000 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 37322800.0000 - val_loss: 36992972.0000 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 37304772.0000 - val_loss: 36957360.0000 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 37067308.0000 - val_loss: 36921788.0000 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37084748.0000 - val_loss: 36886260.0000 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37280544.0000 - val_loss: 36850732.0000 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37025448.0000 - val_loss: 36815220.0000 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 36701944.0000 - val_loss: 36779708.0000 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37153404.0000 - val_loss: 36744252.0000 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 36986600.0000 - val_loss: 36708820.0000 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 36888316.0000 - val_loss: 36673416.0000 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 36844304.0000 - val_loss: 36638032.0000 - learning_rate: 0.0010\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 100510342.91244602\n",
      "Model saved to autoencoder_lstm_model.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "757.7933580875397"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Autoencoder-LSTM with CNN Feature Enhancement\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model\n",
    "        \"\"\"\n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Reshape for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # CNN Feature Enhancement\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False\n",
    "        )(x)\n",
    "        \n",
    "        # Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu'\n",
    "        )(x)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=True\n",
    "        )(x)\n",
    "        \n",
    "        # Output Reconstruction\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Create Optimizer with direct learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compile with Adam and MSE Loss\n",
    "        autoencoder.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=64):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error threshold\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (float): Percentile for anomaly threshold\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95dbceb6-abb5-4095-bb04-a144baa6b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - loss: 38723436.0000 - val_loss: 38417708.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38521992.0000 - val_loss: 38398636.0000 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - loss: 38342388.0000 - val_loss: 38379996.0000 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38777364.0000 - val_loss: 38361532.0000 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38829800.0000 - val_loss: 38343148.0000 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38579472.0000 - val_loss: 38324776.0000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38579908.0000 - val_loss: 38306472.0000 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38442308.0000 - val_loss: 38288176.0000 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38190072.0000 - val_loss: 38269896.0000 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38375400.0000 - val_loss: 38251608.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38332872.0000 - val_loss: 38233344.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38476316.0000 - val_loss: 38215084.0000 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38553048.0000 - val_loss: 38196824.0000 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38389364.0000 - val_loss: 38178608.0000 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38371948.0000 - val_loss: 38160360.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38494400.0000 - val_loss: 38142136.0000 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38425152.0000 - val_loss: 38123920.0000 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38328868.0000 - val_loss: 38105700.0000 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38640704.0000 - val_loss: 38087500.0000 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38357772.0000 - val_loss: 38069300.0000 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38290732.0000 - val_loss: 38051100.0000 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38399188.0000 - val_loss: 38032912.0000 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - loss: 38028172.0000 - val_loss: 38014712.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 34ms/step - loss: 38094568.0000 - val_loss: 37996524.0000 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38353980.0000 - val_loss: 37978360.0000 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 31ms/step - loss: 38239508.0000 - val_loss: 37960196.0000 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37885492.0000 - val_loss: 37942024.0000 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38308656.0000 - val_loss: 37923880.0000 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 37987632.0000 - val_loss: 37905724.0000 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 37798352.0000 - val_loss: 37887560.0000 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37965748.0000 - val_loss: 37869440.0000 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38065548.0000 - val_loss: 37851300.0000 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38004768.0000 - val_loss: 37833164.0000 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38153768.0000 - val_loss: 37815068.0000 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - loss: 37824412.0000 - val_loss: 37796948.0000 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38129092.0000 - val_loss: 37778820.0000 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37833216.0000 - val_loss: 37760720.0000 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38004028.0000 - val_loss: 37742640.0000 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37652244.0000 - val_loss: 37724528.0000 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38093384.0000 - val_loss: 37706460.0000 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37993744.0000 - val_loss: 37688384.0000 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37857780.0000 - val_loss: 37670320.0000 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38028496.0000 - val_loss: 37652244.0000 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38022060.0000 - val_loss: 37634188.0000 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38020168.0000 - val_loss: 37616128.0000 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37801600.0000 - val_loss: 37598084.0000 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37689736.0000 - val_loss: 37580024.0000 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37752308.0000 - val_loss: 37561976.0000 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37842512.0000 - val_loss: 37543976.0000 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 37766464.0000 - val_loss: 37525936.0000 - learning_rate: 0.0010\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 102216439.90409833\n",
      "Model saved to autoencoder_lstm_model.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "658.4905989170074"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Autoencoder-LSTM with CNN Feature Enhancement\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model\n",
    "        \"\"\"\n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Reshape for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # CNN Feature Enhancement\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False\n",
    "        )(x)\n",
    "        \n",
    "        # Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu'\n",
    "        )(x)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=True\n",
    "        )(x)\n",
    "        \n",
    "        # Output Reconstruction\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Create Optimizer with direct learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compile with Adam and MSE Loss\n",
    "        autoencoder.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=128):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error threshold\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (float): Percentile for anomaly threshold\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150fdb5-a960-4f3c-91dd-92382d88480a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
