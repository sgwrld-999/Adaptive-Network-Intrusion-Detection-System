{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bb8c562-ff55-43aa-a11a-26badf31fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 38514476.0000 - val_loss: 38364216.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 38753532.0000 - val_loss: 38291340.0000 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 38246884.0000 - val_loss: 38218564.0000 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 38247920.0000 - val_loss: 38145996.0000 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 38397080.0000 - val_loss: 38073584.0000 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 38432724.0000 - val_loss: 38001292.0000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 38087148.0000 - val_loss: 37929000.0000 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 38204964.0000 - val_loss: 37856876.0000 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 38303040.0000 - val_loss: 37784852.0000 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 37993796.0000 - val_loss: 37712824.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37805048.0000 - val_loss: 37640948.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37870872.0000 - val_loss: 37569156.0000 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37805256.0000 - val_loss: 37497416.0000 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 37995712.0000 - val_loss: 37425876.0000 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37752808.0000 - val_loss: 37354328.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37580592.0000 - val_loss: 37282924.0000 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37607556.0000 - val_loss: 37211612.0000 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 37491752.0000 - val_loss: 37140328.0000 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37794592.0000 - val_loss: 37069312.0000 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37058024.0000 - val_loss: 36998160.0000 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37186984.0000 - val_loss: 36927224.0000 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37199648.0000 - val_loss: 36856424.0000 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 36991652.0000 - val_loss: 36785600.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36895596.0000 - val_loss: 36714932.0000 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 37024756.0000 - val_loss: 36644336.0000 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36669236.0000 - val_loss: 36573800.0000 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36721252.0000 - val_loss: 36503492.0000 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36989100.0000 - val_loss: 36433228.0000 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36770588.0000 - val_loss: 36363028.0000 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36593536.0000 - val_loss: 36292836.0000 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36406244.0000 - val_loss: 36222804.0000 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36620064.0000 - val_loss: 36152940.0000 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36335952.0000 - val_loss: 36083032.0000 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36314968.0000 - val_loss: 36013360.0000 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 36344112.0000 - val_loss: 35943656.0000 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36223200.0000 - val_loss: 35874132.0000 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36215132.0000 - val_loss: 35804676.0000 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35991020.0000 - val_loss: 35735276.0000 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35769736.0000 - val_loss: 35665912.0000 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36065844.0000 - val_loss: 35596872.0000 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 36063028.0000 - val_loss: 35527792.0000 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35700016.0000 - val_loss: 35458760.0000 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 35811952.0000 - val_loss: 35389868.0000 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 35394800.0000 - val_loss: 35320972.0000 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35436056.0000 - val_loss: 35252296.0000 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35400128.0000 - val_loss: 35183652.0000 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 35254620.0000 - val_loss: 35115164.0000 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35412452.0000 - val_loss: 35046736.0000 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 35048972.0000 - val_loss: 34978276.0000 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m1785/1785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 35081528.0000 - val_loss: 34910100.0000 - learning_rate: 0.0010\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 97150638.76216795\n",
      "Model saved to autoencoder_lstm_model.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1033.3725581169128"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Autoencoder-LSTM with CNN Feature Enhancement\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model\n",
    "        \"\"\"\n",
    "        # Add more diagnostic print statements\n",
    "        print(f\"Input Dimension: {self.input_dim}\")\n",
    "        print(f\"Latent Dimension: {self.latent_dim}\")\n",
    "        \n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Reshape for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # CNN Feature Enhancement\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False\n",
    "        )(x)\n",
    "        \n",
    "        # Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu'\n",
    "        )(x)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=s\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error threshold\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (float): Percentile for anomaly threshold\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b8c98ba-1a6a-4fc1-84f2-d2c924503165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 38681420.0000 - val_loss: 38291984.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 38536464.0000 - val_loss: 38147552.0000 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 38568508.0000 - val_loss: 38003676.0000 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 38124660.0000 - val_loss: 37859768.0000 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37992204.0000 - val_loss: 37716492.0000 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37917864.0000 - val_loss: 37573440.0000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37806572.0000 - val_loss: 37430924.0000 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37679812.0000 - val_loss: 37288636.0000 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 37540580.0000 - val_loss: 37146968.0000 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37299888.0000 - val_loss: 37005548.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37293312.0000 - val_loss: 36864316.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36971724.0000 - val_loss: 36723668.0000 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 37015276.0000 - val_loss: 36583440.0000 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36353008.0000 - val_loss: 36443216.0000 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36679068.0000 - val_loss: 36303660.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36461936.0000 - val_loss: 36164524.0000 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36414984.0000 - val_loss: 36025848.0000 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 36273944.0000 - val_loss: 35887408.0000 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 36041560.0000 - val_loss: 35749224.0000 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 36266380.0000 - val_loss: 35611608.0000 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 35858196.0000 - val_loss: 35474412.0000 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 35423640.0000 - val_loss: 35337092.0000 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 35323920.0000 - val_loss: 35200568.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 35320748.0000 - val_loss: 35064404.0000 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 35107124.0000 - val_loss: 34928636.0000 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 35101936.0000 - val_loss: 34793188.0000 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34861508.0000 - val_loss: 34657884.0000 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 34796176.0000 - val_loss: 34523180.0000 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34711284.0000 - val_loss: 34388900.0000 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 34733104.0000 - val_loss: 34255048.0000 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34764228.0000 - val_loss: 34121284.0000 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 11ms/step - loss: 34248676.0000 - val_loss: 33987964.0000 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 10ms/step - loss: 34097716.0000 - val_loss: 33855052.0000 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34199132.0000 - val_loss: 33722760.0000 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 34115312.0000 - val_loss: 33590588.0000 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 33654624.0000 - val_loss: 33458542.0000 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 9ms/step - loss: 33703336.0000 - val_loss: 33327236.0000 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 9ms/step - loss: 33560936.0000 - val_loss: 33196336.0000 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 9ms/step - loss: 33149726.0000 - val_loss: 33065376.0000 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 33081868.0000 - val_loss: 32935224.0000 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 33365164.0000 - val_loss: 32805566.0000 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 33115918.0000 - val_loss: 32675798.0000 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 32621948.0000 - val_loss: 32546714.0000 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 32818874.0000 - val_loss: 32417958.0000 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 32802238.0000 - val_loss: 32289754.0000 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 32436474.0000 - val_loss: 32161524.0000 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 32436976.0000 - val_loss: 32034016.0000 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 32253524.0000 - val_loss: 31906604.0000 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 31987502.0000 - val_loss: 31779684.0000 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m3570/3570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 31946680.0000 - val_loss: 31653050.0000 - learning_rate: 0.0010\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 90658422.49167582\n",
      "Model saved to autoencoder_lstm_model_1.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1509.0542860031128"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Autoencoder-LSTM with CNN Feature Enhancement\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model\n",
    "        \"\"\"\n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Reshape for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # CNN Feature Enhancement\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False\n",
    "        )(x)\n",
    "        \n",
    "        # Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu'\n",
    "        )(x)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=True\n",
    "        )(x)\n",
    "        \n",
    "        # Output Reconstruction\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Create Optimizer with direct learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compile with Adam and MSE Loss\n",
    "        autoencoder.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=16):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error threshold\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (float): Percentile for anomaly threshold\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model_1.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f0ca220-fa2e-4302-881c-eba665bbdde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - loss: 38557228.0000 - val_loss: 38400276.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38655144.0000 - val_loss: 38363304.0000 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38484036.0000 - val_loss: 38326648.0000 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38300188.0000 - val_loss: 38290116.0000 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38626332.0000 - val_loss: 38253680.0000 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38344596.0000 - val_loss: 38217256.0000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38502892.0000 - val_loss: 38180868.0000 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38246552.0000 - val_loss: 38144472.0000 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38115044.0000 - val_loss: 38108096.0000 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38579616.0000 - val_loss: 38071816.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38319060.0000 - val_loss: 38035516.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38226856.0000 - val_loss: 37999244.0000 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 38215220.0000 - val_loss: 37962972.0000 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37919436.0000 - val_loss: 37926712.0000 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 38319064.0000 - val_loss: 37890548.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 37962128.0000 - val_loss: 37854348.0000 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37975704.0000 - val_loss: 37818192.0000 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 38145016.0000 - val_loss: 37782072.0000 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 37944628.0000 - val_loss: 37745948.0000 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37883148.0000 - val_loss: 37709848.0000 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37795864.0000 - val_loss: 37673792.0000 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37770824.0000 - val_loss: 37637736.0000 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 38047064.0000 - val_loss: 37601752.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37850448.0000 - val_loss: 37565760.0000 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37846008.0000 - val_loss: 37529772.0000 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37638820.0000 - val_loss: 37493820.0000 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 38010412.0000 - val_loss: 37457928.0000 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37616900.0000 - val_loss: 37421984.0000 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37395276.0000 - val_loss: 37386108.0000 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37556636.0000 - val_loss: 37350256.0000 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37520416.0000 - val_loss: 37314436.0000 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37799088.0000 - val_loss: 37278644.0000 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37567600.0000 - val_loss: 37242840.0000 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37408240.0000 - val_loss: 37207088.0000 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37590668.0000 - val_loss: 37171364.0000 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37495496.0000 - val_loss: 37135628.0000 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37554624.0000 - val_loss: 37099952.0000 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37452076.0000 - val_loss: 37064292.0000 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - loss: 37156888.0000 - val_loss: 37028600.0000 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 37322800.0000 - val_loss: 36992972.0000 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 37304772.0000 - val_loss: 36957360.0000 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 37067308.0000 - val_loss: 36921788.0000 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37084748.0000 - val_loss: 36886260.0000 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 37280544.0000 - val_loss: 36850732.0000 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37025448.0000 - val_loss: 36815220.0000 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 36701944.0000 - val_loss: 36779708.0000 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 37153404.0000 - val_loss: 36744252.0000 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 36986600.0000 - val_loss: 36708820.0000 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - loss: 36888316.0000 - val_loss: 36673416.0000 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 36844304.0000 - val_loss: 36638032.0000 - learning_rate: 0.0010\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 100510342.91244602\n",
      "Model saved to autoencoder_lstm_model.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "757.7933580875397"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Autoencoder-LSTM with CNN Feature Enhancement\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model\n",
    "        \"\"\"\n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Reshape for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # CNN Feature Enhancement\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False\n",
    "        )(x)\n",
    "        \n",
    "        # Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu'\n",
    "        )(x)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=True\n",
    "        )(x)\n",
    "        \n",
    "        # Output Reconstruction\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Create Optimizer with direct learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compile with Adam and MSE Loss\n",
    "        autoencoder.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=64):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error threshold\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (float): Percentile for anomaly threshold\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95dbceb6-abb5-4095-bb04-a144baa6b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - loss: 38723436.0000 - val_loss: 38417708.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38521992.0000 - val_loss: 38398636.0000 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - loss: 38342388.0000 - val_loss: 38379996.0000 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38777364.0000 - val_loss: 38361532.0000 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38829800.0000 - val_loss: 38343148.0000 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38579472.0000 - val_loss: 38324776.0000 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38579908.0000 - val_loss: 38306472.0000 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38442308.0000 - val_loss: 38288176.0000 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38190072.0000 - val_loss: 38269896.0000 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38375400.0000 - val_loss: 38251608.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38332872.0000 - val_loss: 38233344.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38476316.0000 - val_loss: 38215084.0000 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38553048.0000 - val_loss: 38196824.0000 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38389364.0000 - val_loss: 38178608.0000 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38371948.0000 - val_loss: 38160360.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38494400.0000 - val_loss: 38142136.0000 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38425152.0000 - val_loss: 38123920.0000 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38328868.0000 - val_loss: 38105700.0000 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38640704.0000 - val_loss: 38087500.0000 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38357772.0000 - val_loss: 38069300.0000 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38290732.0000 - val_loss: 38051100.0000 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38399188.0000 - val_loss: 38032912.0000 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - loss: 38028172.0000 - val_loss: 38014712.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 34ms/step - loss: 38094568.0000 - val_loss: 37996524.0000 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38353980.0000 - val_loss: 37978360.0000 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 31ms/step - loss: 38239508.0000 - val_loss: 37960196.0000 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37885492.0000 - val_loss: 37942024.0000 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38308656.0000 - val_loss: 37923880.0000 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 37987632.0000 - val_loss: 37905724.0000 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 37798352.0000 - val_loss: 37887560.0000 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37965748.0000 - val_loss: 37869440.0000 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38065548.0000 - val_loss: 37851300.0000 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38004768.0000 - val_loss: 37833164.0000 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38153768.0000 - val_loss: 37815068.0000 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - loss: 37824412.0000 - val_loss: 37796948.0000 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38129092.0000 - val_loss: 37778820.0000 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37833216.0000 - val_loss: 37760720.0000 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38004028.0000 - val_loss: 37742640.0000 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37652244.0000 - val_loss: 37724528.0000 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38093384.0000 - val_loss: 37706460.0000 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37993744.0000 - val_loss: 37688384.0000 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37857780.0000 - val_loss: 37670320.0000 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 38028496.0000 - val_loss: 37652244.0000 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 38022060.0000 - val_loss: 37634188.0000 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 38020168.0000 - val_loss: 37616128.0000 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37801600.0000 - val_loss: 37598084.0000 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37689736.0000 - val_loss: 37580024.0000 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37752308.0000 - val_loss: 37561976.0000 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 37842512.0000 - val_loss: 37543976.0000 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 30ms/step - loss: 37766464.0000 - val_loss: 37525936.0000 - learning_rate: 0.0010\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 102216439.90409833\n",
      "Model saved to autoencoder_lstm_model.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "658.4905989170074"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Autoencoder-LSTM with CNN Feature Enhancement\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model\n",
    "        \"\"\"\n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Reshape for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(inputs)\n",
    "        \n",
    "        # CNN Feature Enhancement\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False\n",
    "        )(x)\n",
    "        \n",
    "        # Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu'\n",
    "        )(x)\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=True\n",
    "        )(x)\n",
    "        \n",
    "        # Output Reconstruction\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Create Optimizer with direct learning rate\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compile with Adam and MSE Loss\n",
    "        autoencoder.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=128):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error threshold\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (float): Percentile for anomaly threshold\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e41f05-c936-4d0b-a1fa-b846695e1e6e",
   "metadata": {},
   "source": [
    "Batch Size Impact \n",
    "\n",
    "| Batch Size | Anomaly Threshold | Final Loss | Observations |\n",
    "|-----------|-------------------|------------|--------------|\n",
    "| 16        | 90,658,422.49     | 1509.05    | Highest variance |\n",
    "| 32        | 97,150,638.76     | 1033.37    | Moderate variance |\n",
    "| 64        | 100,510,342.91    | 757.79     | Lower variance |\n",
    "| 128       | 102,216,439.90    | 658.49     | Lowest variance |\n",
    "\n",
    "### Insights of the outcome\n",
    "\n",
    "1. **Anomaly Threshold Trend w.r.t to Batch size**:\n",
    "   - Increasing with batch size\n",
    "   - 16 (Lowest): 90,658,422.49\n",
    "   - 128 (Highest): 102,216,439.90\n",
    "\n",
    "2. **Loss according to batch size**:\n",
    "   - Decreasing with increasing batch size\n",
    "   - 16 (Highest): 1509.05\n",
    "   - 128 (Lowest): 658.49\n",
    "\n",
    "### Interpretation of the outcomes\n",
    "\n",
    "1. **Extremely High Anomaly Thresholds**\n",
    "   - Values around 90-100 million are unusually large\n",
    "   - Indicates potential issues with:\n",
    "     - Data preprocessing \n",
    "     - Feature scaling\n",
    "     - Model architecture\n",
    "\n",
    "### Next steps to reduce anomaly thresholds\n",
    "\n",
    "1. Data Preprocessing\n",
    "2. Model Architecture Modification\n",
    "3. Threshold Calculation Enhancement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0b771f-9160-4def-9f49-34d9634b5da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "🔍 Model Configuration:\n",
      "Input Dimension: 44\n",
      "Latent Dimension: 32\n",
      "An error occurred during NIDS training: Input 0 of layer \"lstm_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18063688278198242"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Construct Adaptive Autoencoder-LSTM with Enhanced CNN Feature Extraction\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled Autoencoder model with advanced anomaly detection capabilities\n",
    "        \"\"\"\n",
    "        # Enhanced diagnostic print statements\n",
    "        print(f\"Model Configuration:\")\n",
    "        print(f\"Input Dimension: {self.input_dim}\")\n",
    "        print(f\"Latent Dimension: {self.latent_dim}\")\n",
    "    \n",
    "        # Input Layer with Robust Initialization\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Advanced Preprocessing Layer\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        \n",
    "        # Flexible Reshaping for 1D CNN\n",
    "        x = layers.Reshape((-1, 1))(x)\n",
    "        \n",
    "        # Enhanced CNN Feature Extraction\n",
    "        x = layers.Conv1D(\n",
    "            filters=32,  # Reduced from 64 for better generalization\n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001)  # L2 Regularization\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.4)(x)  # Increased dropout for better regularization\n",
    "    \n",
    "        # Additional Convolutional Layer for Deeper Feature Extraction\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        # Flatten before LSTM\n",
    "        x = layers.Flatten()(x)\n",
    "        \n",
    "        # Dense Layer for Feature Compression\n",
    "        x = layers.Dense(\n",
    "            units=128, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "        )(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # LSTM Encoder with Bidirectional Option\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=False,\n",
    "            recurrent_dropout=0.3,\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "        )(x)\n",
    "    \n",
    "        # Enhanced Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l1(0.001)\n",
    "        )(x)\n",
    "        \n",
    "        # Variational Autoencoder-like Sampling\n",
    "        encoded = layers.Dense(self.latent_dim)(encoded)\n",
    "        \n",
    "        # LSTM Decoder with Expanded Architecture\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim * 2,  # Increased units \n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=0.3\n",
    "        )(x)\n",
    "        \n",
    "        # Multi-Layer Reconstruction\n",
    "        x = layers.TimeDistributed(\n",
    "            layers.Dense(64, activation='relu')\n",
    "        )(x)\n",
    "        x = layers.TimeDistributed(\n",
    "            layers.Dense(32, activation='relu')\n",
    "        )(x)\n",
    "        \n",
    "        # Final Reconstruction Layer\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "    \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Advanced Optimizer Configuration\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=self.learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07\n",
    "        )\n",
    "        \n",
    "        # Compile with Custom Metrics\n",
    "        autoencoder.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_squared_error',\n",
    "            metrics=[\n",
    "                'mae',  # Mean Absolute Error\n",
    "                keras.metrics.MeanSquaredError()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Model Summary for Diagnostic Purposes\n",
    "        print(\"\\n🔬 Model Architecture Summary:\")\n",
    "        autoencoder.summary()\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=128):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        \n",
    "        # More comprehensive error analysis\n",
    "        print(\"Reconstruction Error Analysis:\")\n",
    "        print(f\"Mean Error: {np.mean(reconstruction_errors)}\")\n",
    "        print(f\"Median Error: {np.median(reconstruction_errors)}\")\n",
    "        print(f\"Error Standard Deviation: {np.std(reconstruction_errors)}\")\n",
    "        print(f\"Error Distribution Percentiles:\")\n",
    "        for p in [50, 75, 90, 95, 99]:\n",
    "            print(f\"{p}th Percentile: {np.percentile(reconstruction_errors, p)}\")\n",
    "        \n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model_after_updating_model_Arch_&_thres.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e3a9f0-6c01-4b83-a08e-a0b90edc2cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 82ms/step - loss: 37129980.0000 - mae: 855.3536 - mean_squared_error: 37129980.0000 - val_loss: 18835338.0000 - val_mae: 538.9406 - val_mean_squared_error: 18835338.0000 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 78ms/step - loss: 11610540.0000 - mae: 406.4425 - mean_squared_error: 11610539.0000 - val_loss: 1043190.9375 - val_mae: 109.8960 - val_mean_squared_error: 1043189.9375 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 82ms/step - loss: 833043.2500 - mae: 104.8816 - mean_squared_error: 833041.8125 - val_loss: 204533.2188 - val_mae: 59.0621 - val_mean_squared_error: 204532.1562 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 84ms/step - loss: 277633.6562 - mae: 62.9586 - mean_squared_error: 277632.5625 - val_loss: 79873.1641 - val_mae: 35.8848 - val_mean_squared_error: 79872.0078 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 85ms/step - loss: 206842.7344 - mae: 53.6897 - mean_squared_error: 206841.6250 - val_loss: 48723.3242 - val_mae: 23.5608 - val_mean_squared_error: 48722.1133 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 89ms/step - loss: 177013.3750 - mae: 49.3831 - mean_squared_error: 177012.1406 - val_loss: 47943.7656 - val_mae: 23.4761 - val_mean_squared_error: 47942.5117 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 87ms/step - loss: 170389.9062 - mae: 48.4879 - mean_squared_error: 170388.6719 - val_loss: 51088.3789 - val_mae: 24.9975 - val_mean_squared_error: 51087.0977 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 81ms/step - loss: 152077.2031 - mae: 45.4651 - mean_squared_error: 152075.8750 - val_loss: 47985.4883 - val_mae: 24.4147 - val_mean_squared_error: 47984.1875 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 82ms/step - loss: 126343.1719 - mae: 41.3250 - mean_squared_error: 126341.8281 - val_loss: 54261.2148 - val_mae: 28.1537 - val_mean_squared_error: 54259.9297 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 81ms/step - loss: 130600.8750 - mae: 41.4893 - mean_squared_error: 130599.5156 - val_loss: 51225.0078 - val_mae: 24.9954 - val_mean_squared_error: 51223.6562 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 87ms/step - loss: 117123.7266 - mae: 39.8249 - mean_squared_error: 117122.3594 - val_loss: 45990.2422 - val_mae: 25.0983 - val_mean_squared_error: 45988.8633 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 84ms/step - loss: 111719.9766 - mae: 38.5277 - mean_squared_error: 111718.5859 - val_loss: 128390.1094 - val_mae: 43.0037 - val_mean_squared_error: 128388.7266 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 82ms/step - loss: 111260.9297 - mae: 38.7973 - mean_squared_error: 111259.5391 - val_loss: 63201.5781 - val_mae: 27.4380 - val_mean_squared_error: 63200.1406 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 80ms/step - loss: 111825.9531 - mae: 38.4385 - mean_squared_error: 111824.5234 - val_loss: 57440.5703 - val_mae: 26.9660 - val_mean_squared_error: 57439.1250 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 84ms/step - loss: 108416.8984 - mae: 38.1412 - mean_squared_error: 108415.3984 - val_loss: 50177.2109 - val_mae: 24.6689 - val_mean_squared_error: 50175.7148 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 82ms/step - loss: 90231.1250 - mae: 35.9081 - mean_squared_error: 90229.6562 - val_loss: 58095.8516 - val_mae: 25.7013 - val_mean_squared_error: 58094.3438 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 78ms/step - loss: 85678.0938 - mae: 33.6751 - mean_squared_error: 85676.5859 - val_loss: 63600.7344 - val_mae: 27.2629 - val_mean_squared_error: 63599.2109 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 79ms/step - loss: 81913.8359 - mae: 33.1804 - mean_squared_error: 81912.2891 - val_loss: 91411.7422 - val_mae: 31.6241 - val_mean_squared_error: 91410.2031 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 80ms/step - loss: 73898.2344 - mae: 31.6640 - mean_squared_error: 73896.6875 - val_loss: 79105.2344 - val_mae: 30.6611 - val_mean_squared_error: 79103.7188 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 79ms/step - loss: 74599.3438 - mae: 31.8135 - mean_squared_error: 74597.8047 - val_loss: 50776.6719 - val_mae: 24.4507 - val_mean_squared_error: 50775.1055 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 79ms/step - loss: 73957.7578 - mae: 31.5189 - mean_squared_error: 73956.1797 - val_loss: 74887.2422 - val_mae: 28.9017 - val_mean_squared_error: 74885.6719 - learning_rate: 5.0000e-04\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Error Analysis:\n",
      "Mean Error: 45988.85814415081\n",
      "Median Error: 16600.7712170693\n",
      "Error Standard Deviation: 111272.75706236661\n",
      "Error Distribution Percentiles:\n",
      "50th Percentile: 16600.7712170693\n",
      "75th Percentile: 53655.75553725298\n",
      "90th Percentile: 110902.7680407934\n",
      "95th Percentile: 162486.1893144254\n",
      "99th Percentile: 388332.8465355264\n",
      "Anomaly Threshold: 162486.1893144254\n",
      "Model saved to autoencoder_lstm_model_after_updating_model_Arch_&_thres.h5\n",
      "Threshold saved to anomaly_threshold.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "779.9328281879425"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=32, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_autoencoder_cnn_model()\n",
    "        \n",
    "    def _build_autoencoder_cnn_model(self):\n",
    "        # Input Layer with Robust Initialization\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Advanced Preprocessing Layer\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        \n",
    "        # Reshape to 3D input for LSTM\n",
    "        x = layers.Reshape((-1, 1))(x)  # Add time steps dimension\n",
    "        \n",
    "        # Enhanced CNN Feature Extraction\n",
    "        x = layers.Conv1D(\n",
    "            filters=32,  \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        # Additional Convolutional Layer\n",
    "        x = layers.Conv1D(\n",
    "            filters=64, \n",
    "            kernel_size=3, \n",
    "            activation='relu', \n",
    "            padding='same',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        # LSTM Encoder \n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim, \n",
    "            return_sequences=True,  # Change to return sequences\n",
    "            recurrent_dropout=0.3,\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "        )(x)\n",
    "        \n",
    "        # Global Average Pooling to reduce dimensions\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # Dense Layer for Feature Compression\n",
    "        x = layers.Dense(\n",
    "            units=128, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "        )(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # Enhanced Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l1(0.001)\n",
    "        )(x)\n",
    "        \n",
    "        # Repeat and Reconstruct\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim * 2,  \n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=0.3\n",
    "        )(x)\n",
    "        \n",
    "        # Multi-Layer Reconstruction\n",
    "        x = layers.TimeDistributed(\n",
    "            layers.Dense(64, activation='relu')\n",
    "        )(x)\n",
    "        x = layers.TimeDistributed(\n",
    "            layers.Dense(32, activation='relu')\n",
    "        )(x)\n",
    "        \n",
    "        # Final Reconstruction Layer\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Compile with Custom Metrics\n",
    "        autoencoder.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mae', keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=50, batch_size=128):\n",
    "        \"\"\"\n",
    "        Train the Autoencoder model\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.array): Training data\n",
    "            X_val (np.array, optional): Validation data\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \n",
    "        Returns:\n",
    "            history: Training history\n",
    "        \"\"\"\n",
    "        # Early Stopping to prevent overfitting\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder reconstructs input\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=95):\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        \n",
    "        # More comprehensive error analysis\n",
    "        print(\"Reconstruction Error Analysis:\")\n",
    "        print(f\"Mean Error: {np.mean(reconstruction_errors)}\")\n",
    "        print(f\"Median Error: {np.median(reconstruction_errors)}\")\n",
    "        print(f\"Error Standard Deviation: {np.std(reconstruction_errors)}\")\n",
    "        print(f\"Error Distribution Percentiles:\")\n",
    "        for p in [50, 75, 90, 95, 99]:\n",
    "            print(f\"{p}th Percentile: {np.percentile(reconstruction_errors, p)}\")\n",
    "        \n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies in network traffic\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        return mse > threshold\n",
    "    \n",
    "    def save_model(self, model_path='autoencoder_lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save model\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to preprocessed scaled dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load preprocessed scaled dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'label' is the target column)\n",
    "        X = df.drop(['Attack_label'], axis=1).values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'autoencoder_lstm_model_after_updating_model_Arch_&_thres.h5'\n",
    "    threshold_save_path = 'anomaly_threshold.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes for verification\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        \n",
    "        # Save threshold for inference\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        print(f\"Threshold saved to {threshold_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "ex_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9d5e6-08ef-447b-98fb-cc02c2a10aa4",
   "metadata": {},
   "source": [
    "### Error Distribution Characteristics\n",
    "\n",
    "- **Mean Error**: 45,988.86\n",
    "- **Median Error**: 16,600.77\n",
    "- **Standard Deviation**: 111,272.76\n",
    "\n",
    "## Error Percentile Breakdown\n",
    "\n",
    "| Percentile | Error Value | Significance |\n",
    "|-----------|-------------|--------------|\n",
    "| 50th (Median) | 16,600.77 | Central tendency of reconstruction errors |\n",
    "| 75th | 53,655.76 | Indicates spread of typical reconstruction errors |\n",
    "| 90th | 110,902.77 | Identifies high-variance error regions |\n",
    "| 95th | 162,486.19 | **Anomaly Detection Threshold** |\n",
    "| 99th | 388,332.85 | Extreme error cases |\n",
    "\n",
    "## Observations\n",
    "\n",
    "### 1. Error Distribution Anomalies\n",
    "- Significant disparity between mean and median errors\n",
    "- Indicates a highly skewed error distribution\n",
    "- Suggests presence of substantial outliers in reconstruction process\n",
    "\n",
    "### 2. Anomaly Detection Implications\n",
    "- Current threshold captures top 5% of most divergent samples\n",
    "- Potential risk of missing subtle network intrusion patterns\n",
    "- High variability may compromise detection accuracy\n",
    "\n",
    "###Improvement Strategies\n",
    "\n",
    "#### Data Preprocessing\n",
    "- Enhance feature scaling techniques\n",
    "- Investigate and handle dataset outliers\n",
    "- Implement more robust normalization methods\n",
    "\n",
    "#### Model Architecture Refinement\n",
    "- Reduce model complexity\n",
    "- Increase regularization mechanisms\n",
    "- Experiment with alternative latent space configurations\n",
    "\n",
    "#### Anomaly Threshold Optimization\n",
    "- Consider lowering percentile threshold\n",
    "- Implement adaptive thresholding mechanism\n",
    "- Develop multi-tier anomaly classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb784557-ee68-4cce-9945-3cb47ffdd72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57120, 44)\n",
      "Validation data shape: (14281, 44)\n",
      "Epoch 1/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 27ms/step - loss: 12580389888.0000 - mae: 3839.6240 - mean_squared_error: 12580389888.0000 - val_loss: 15294824448.0000 - val_mae: 3986.7656 - val_mean_squared_error: 15294824448.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 2/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12570474496.0000 - mae: 3916.2473 - mean_squared_error: 12570474496.0000 - val_loss: 15290957824.0000 - val_mae: 4012.1270 - val_mean_squared_error: 15290957824.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 3/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12328400896.0000 - mae: 3850.2791 - mean_squared_error: 12328400896.0000 - val_loss: 15285043200.0000 - val_mae: 4046.2610 - val_mean_squared_error: 15285043200.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 4/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12254767104.0000 - mae: 3927.1460 - mean_squared_error: 12254767104.0000 - val_loss: 15277291520.0000 - val_mae: 4107.8389 - val_mean_squared_error: 15277291520.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 5/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 12386510848.0000 - mae: 3989.7773 - mean_squared_error: 12386510848.0000 - val_loss: 15267611648.0000 - val_mae: 4180.3135 - val_mean_squared_error: 15267611648.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 6/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12590083072.0000 - mae: 4136.0044 - mean_squared_error: 12590083072.0000 - val_loss: 15256384512.0000 - val_mae: 4261.0586 - val_mean_squared_error: 15256384512.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 7/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - loss: 12483979264.0000 - mae: 4167.0786 - mean_squared_error: 12483979264.0000 - val_loss: 15243488256.0000 - val_mae: 4357.1152 - val_mean_squared_error: 15243488256.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 8/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - loss: 12794526720.0000 - mae: 4382.6157 - mean_squared_error: 12794526720.0000 - val_loss: 15228956672.0000 - val_mae: 4439.9707 - val_mean_squared_error: 15228956672.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 9/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - loss: 12785451008.0000 - mae: 4484.4644 - mean_squared_error: 12785451008.0000 - val_loss: 15213345792.0000 - val_mae: 4566.2085 - val_mean_squared_error: 15213345792.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 10/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 13009156096.0000 - mae: 4632.0884 - mean_squared_error: 13009156096.0000 - val_loss: 15192557568.0000 - val_mae: 4517.1899 - val_mean_squared_error: 15192557568.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 11/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 11966883840.0000 - mae: 4087.6511 - mean_squared_error: 11966883840.0000 - val_loss: 15167306752.0000 - val_mae: 4128.0684 - val_mean_squared_error: 15167306752.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 12/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 12236376064.0000 - mae: 3975.0085 - mean_squared_error: 12236376064.0000 - val_loss: 15143561216.0000 - val_mae: 4124.7925 - val_mean_squared_error: 15143561216.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 13/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 12529388544.0000 - mae: 4097.3828 - mean_squared_error: 12529388544.0000 - val_loss: 15117665280.0000 - val_mae: 4140.3203 - val_mean_squared_error: 15117665280.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 14/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 12003283968.0000 - mae: 3969.7854 - mean_squared_error: 12003283968.0000 - val_loss: 15090014208.0000 - val_mae: 4120.5103 - val_mean_squared_error: 15090014208.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 15/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12135880704.0000 - mae: 3995.2390 - mean_squared_error: 12135880704.0000 - val_loss: 15060113408.0000 - val_mae: 4171.2700 - val_mean_squared_error: 15060113408.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 16/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12391705600.0000 - mae: 4090.1855 - mean_squared_error: 12391705600.0000 - val_loss: 15028260864.0000 - val_mae: 4102.0996 - val_mean_squared_error: 15028260864.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 17/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - loss: 12069037056.0000 - mae: 3994.3440 - mean_squared_error: 12069037056.0000 - val_loss: 14994593792.0000 - val_mae: 4103.0283 - val_mean_squared_error: 14994593792.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 18/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 29ms/step - loss: 12509287424.0000 - mae: 4128.2632 - mean_squared_error: 12509287424.0000 - val_loss: 14958623744.0000 - val_mae: 4097.1953 - val_mean_squared_error: 14958623744.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 19/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 31ms/step - loss: 12239539200.0000 - mae: 4018.0183 - mean_squared_error: 12239539200.0000 - val_loss: 14920534016.0000 - val_mae: 4093.4463 - val_mean_squared_error: 14920534016.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 20/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - loss: 12284828672.0000 - mae: 4074.3655 - mean_squared_error: 12284828672.0000 - val_loss: 14880135168.0000 - val_mae: 4098.6924 - val_mean_squared_error: 14880135168.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 21/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 12035818496.0000 - mae: 4028.3484 - mean_squared_error: 12035818496.0000 - val_loss: 14837981184.0000 - val_mae: 4149.4448 - val_mean_squared_error: 14837981184.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 22/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 11649188864.0000 - mae: 3922.8049 - mean_squared_error: 11649188864.0000 - val_loss: 14793847808.0000 - val_mae: 4061.0391 - val_mean_squared_error: 14793847808.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 23/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12154304512.0000 - mae: 4036.3699 - mean_squared_error: 12154304512.0000 - val_loss: 14747693056.0000 - val_mae: 4035.9370 - val_mean_squared_error: 14747693056.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 24/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 12107940864.0000 - mae: 4010.0273 - mean_squared_error: 12107940864.0000 - val_loss: 14699943936.0000 - val_mae: 4014.9084 - val_mean_squared_error: 14699943936.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 25/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - loss: 12256706560.0000 - mae: 4023.1360 - mean_squared_error: 12256706560.0000 - val_loss: 14649751552.0000 - val_mae: 3997.4856 - val_mean_squared_error: 14649751552.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 26/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12004528128.0000 - mae: 4013.9543 - mean_squared_error: 12004528128.0000 - val_loss: 14598296576.0000 - val_mae: 4004.2625 - val_mean_squared_error: 14598296576.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 27/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 11820478464.0000 - mae: 3925.9519 - mean_squared_error: 11820478464.0000 - val_loss: 14544550912.0000 - val_mae: 4008.4368 - val_mean_squared_error: 14544550912.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 28/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 12077614080.0000 - mae: 4007.4438 - mean_squared_error: 12077614080.0000 - val_loss: 14490382336.0000 - val_mae: 3973.2454 - val_mean_squared_error: 14490382336.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 29/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - loss: 11572648960.0000 - mae: 3872.7202 - mean_squared_error: 11572648960.0000 - val_loss: 14434256896.0000 - val_mae: 3961.7053 - val_mean_squared_error: 14434256896.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 30/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - loss: 11819781120.0000 - mae: 3946.4207 - mean_squared_error: 11819781120.0000 - val_loss: 14374520832.0000 - val_mae: 3943.0925 - val_mean_squared_error: 14374520832.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 31/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 11802406912.0000 - mae: 3968.5688 - mean_squared_error: 11802406912.0000 - val_loss: 14314054656.0000 - val_mae: 3920.6482 - val_mean_squared_error: 14314054656.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 32/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 11578004480.0000 - mae: 3885.6052 - mean_squared_error: 11578004480.0000 - val_loss: 14253142016.0000 - val_mae: 3882.2173 - val_mean_squared_error: 14253142016.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 33/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 12202127360.0000 - mae: 4016.8445 - mean_squared_error: 12202127360.0000 - val_loss: 14190306304.0000 - val_mae: 3869.5059 - val_mean_squared_error: 14190306304.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 34/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 11221357568.0000 - mae: 3747.2461 - mean_squared_error: 11221357568.0000 - val_loss: 14124407808.0000 - val_mae: 3866.0002 - val_mean_squared_error: 14124407808.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 35/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 11341995008.0000 - mae: 3849.0286 - mean_squared_error: 11341995008.0000 - val_loss: 14058103808.0000 - val_mae: 3860.9172 - val_mean_squared_error: 14058103808.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 36/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 11560526848.0000 - mae: 3872.4170 - mean_squared_error: 11560526848.0000 - val_loss: 13988914176.0000 - val_mae: 3853.1282 - val_mean_squared_error: 13988914176.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 37/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - loss: 11515925504.0000 - mae: 3886.4194 - mean_squared_error: 11515925504.0000 - val_loss: 13919650816.0000 - val_mae: 3828.4353 - val_mean_squared_error: 13919650816.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 38/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - loss: 11466970112.0000 - mae: 3867.7542 - mean_squared_error: 11466970112.0000 - val_loss: 13849244672.0000 - val_mae: 3831.2524 - val_mean_squared_error: 13849244672.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 39/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - loss: 11519481856.0000 - mae: 3908.9968 - mean_squared_error: 11519481856.0000 - val_loss: 13777764352.0000 - val_mae: 3835.7144 - val_mean_squared_error: 13777764352.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 40/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 10958663680.0000 - mae: 3769.8479 - mean_squared_error: 10958663680.0000 - val_loss: 13701275648.0000 - val_mae: 3795.0493 - val_mean_squared_error: 13701275648.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 41/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 11615866880.0000 - mae: 3950.5977 - mean_squared_error: 11615866880.0000 - val_loss: 13627019264.0000 - val_mae: 3801.8181 - val_mean_squared_error: 13627019264.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 42/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 11313769472.0000 - mae: 3879.5386 - mean_squared_error: 11313769472.0000 - val_loss: 13551107072.0000 - val_mae: 3804.3481 - val_mean_squared_error: 13551107072.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 43/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 10798276608.0000 - mae: 3767.6365 - mean_squared_error: 10798276608.0000 - val_loss: 13472349184.0000 - val_mae: 3770.4170 - val_mean_squared_error: 13472349184.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 44/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 10905687040.0000 - mae: 3819.0369 - mean_squared_error: 10905687040.0000 - val_loss: 13392795648.0000 - val_mae: 3734.4614 - val_mean_squared_error: 13392795648.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 45/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 26ms/step - loss: 10526118912.0000 - mae: 3695.9258 - mean_squared_error: 10526118912.0000 - val_loss: 13310915584.0000 - val_mae: 3730.8569 - val_mean_squared_error: 13310915584.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 46/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - loss: 10688172032.0000 - mae: 3760.4578 - mean_squared_error: 10688172032.0000 - val_loss: 13226729472.0000 - val_mae: 3726.7219 - val_mean_squared_error: 13226729472.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 47/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 10875764736.0000 - mae: 3812.0828 - mean_squared_error: 10875764736.0000 - val_loss: 13146370048.0000 - val_mae: 3707.7375 - val_mean_squared_error: 13146370048.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 48/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 10136715264.0000 - mae: 3661.8716 - mean_squared_error: 10136715264.0000 - val_loss: 13062150144.0000 - val_mae: 3685.3274 - val_mean_squared_error: 13062150144.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 49/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - loss: 10322808832.0000 - mae: 3675.0078 - mean_squared_error: 10322808832.0000 - val_loss: 12976762880.0000 - val_mae: 3658.9380 - val_mean_squared_error: 12976762880.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 50/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - loss: 10979810304.0000 - mae: 4371.5024 - mean_squared_error: 10979810304.0000 - val_loss: 14685224960.0000 - val_mae: 6943.0269 - val_mean_squared_error: 14685224960.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 51/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - loss: 11882635264.0000 - mae: 6202.2388 - mean_squared_error: 11882635264.0000 - val_loss: 14129557504.0000 - val_mae: 5511.8369 - val_mean_squared_error: 14129557504.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 52/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 11786988544.0000 - mae: 5751.4341 - mean_squared_error: 11786988544.0000 - val_loss: 14035283968.0000 - val_mae: 5025.8481 - val_mean_squared_error: 14035283968.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 53/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 10983363584.0000 - mae: 4583.8047 - mean_squared_error: 10983363584.0000 - val_loss: 12736629760.0000 - val_mae: 3704.0796 - val_mean_squared_error: 12736629760.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 54/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 10276526080.0000 - mae: 3729.2427 - mean_squared_error: 10276526080.0000 - val_loss: 12646505472.0000 - val_mae: 3634.3970 - val_mean_squared_error: 12646505472.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 55/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 9882333184.0000 - mae: 3648.8896 - mean_squared_error: 9882333184.0000 - val_loss: 12557477888.0000 - val_mae: 3619.1423 - val_mean_squared_error: 12557477888.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 56/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 9897228288.0000 - mae: 3582.4868 - mean_squared_error: 9897228288.0000 - val_loss: 12462050304.0000 - val_mae: 3578.5979 - val_mean_squared_error: 12462050304.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 57/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 10009151488.0000 - mae: 3631.3513 - mean_squared_error: 10009151488.0000 - val_loss: 12368224256.0000 - val_mae: 3545.9370 - val_mean_squared_error: 12368224256.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 58/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 9901022208.0000 - mae: 3629.3928 - mean_squared_error: 9901022208.0000 - val_loss: 12274537472.0000 - val_mae: 3533.6189 - val_mean_squared_error: 12274537472.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 59/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 9691849728.0000 - mae: 3571.8281 - mean_squared_error: 9691849728.0000 - val_loss: 12181405696.0000 - val_mae: 3571.4124 - val_mean_squared_error: 12181405696.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 60/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 9681581056.0000 - mae: 3630.7610 - mean_squared_error: 9681581056.0000 - val_loss: 12099708928.0000 - val_mae: 3514.9077 - val_mean_squared_error: 12099708928.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 61/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 9767528448.0000 - mae: 3664.8877 - mean_squared_error: 9767528448.0000 - val_loss: 12004364288.0000 - val_mae: 3531.5481 - val_mean_squared_error: 12004364288.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 62/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 9283494912.0000 - mae: 3505.2830 - mean_squared_error: 9283494912.0000 - val_loss: 11913945088.0000 - val_mae: 3548.4238 - val_mean_squared_error: 11913945088.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 63/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 9216053248.0000 - mae: 3512.7109 - mean_squared_error: 9216053248.0000 - val_loss: 11810492416.0000 - val_mae: 3487.0942 - val_mean_squared_error: 11810492416.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 64/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 9284657152.0000 - mae: 3514.7268 - mean_squared_error: 9284657152.0000 - val_loss: 11712036864.0000 - val_mae: 3503.7771 - val_mean_squared_error: 11712036864.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 65/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 9392913408.0000 - mae: 3613.5591 - mean_squared_error: 9392913408.0000 - val_loss: 11604800512.0000 - val_mae: 3442.6521 - val_mean_squared_error: 11604800512.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 66/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 9228438528.0000 - mae: 3566.5081 - mean_squared_error: 9228438528.0000 - val_loss: 11523511296.0000 - val_mae: 3462.6472 - val_mean_squared_error: 11523511296.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 67/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 8922383360.0000 - mae: 3457.8689 - mean_squared_error: 8922383360.0000 - val_loss: 11403450368.0000 - val_mae: 3406.2266 - val_mean_squared_error: 11403450368.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 68/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 8631825408.0000 - mae: 3390.1909 - mean_squared_error: 8631825408.0000 - val_loss: 11302589440.0000 - val_mae: 3375.5413 - val_mean_squared_error: 11302589440.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 69/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 8869741568.0000 - mae: 3495.9343 - mean_squared_error: 8869741568.0000 - val_loss: 14563519488.0000 - val_mae: 6700.3530 - val_mean_squared_error: 14563519488.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 70/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 11746396160.0000 - mae: 6645.3491 - mean_squared_error: 11746396160.0000 - val_loss: 13652012032.0000 - val_mae: 5939.0601 - val_mean_squared_error: 13652012032.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 71/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 11129127936.0000 - mae: 6165.5601 - mean_squared_error: 11129127936.0000 - val_loss: 13208987648.0000 - val_mae: 6135.7676 - val_mean_squared_error: 13208987648.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 72/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 11219996672.0000 - mae: 6266.0420 - mean_squared_error: 11219996672.0000 - val_loss: 13538723840.0000 - val_mae: 5887.0488 - val_mean_squared_error: 13538723840.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 73/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 11340605440.0000 - mae: 6256.0913 - mean_squared_error: 11340605440.0000 - val_loss: 13112599552.0000 - val_mae: 5651.8574 - val_mean_squared_error: 13112599552.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 74/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - loss: 10648890368.0000 - mae: 6036.2720 - mean_squared_error: 10648890368.0000 - val_loss: 13327361024.0000 - val_mae: 5911.2310 - val_mean_squared_error: 13327361024.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 75/75\n",
      "\u001b[1m893/893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - loss: 10612930560.0000 - mae: 6051.5967 - mean_squared_error: 10612930560.0000 - val_loss: 12897187840.0000 - val_mae: 5553.9292 - val_mean_squared_error: 12897187840.0000 - learning_rate: 5.0000e-04\n",
      "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\n",
      " Reconstruction Error Analysis:\n",
      "Mean Error: 11302590453.45\n",
      "Median Error: 1036.86\n",
      "Error Standard Deviation: 304160698957.95\n",
      "50th Percentile: 1036.86\n",
      "75th Percentile: 3273.03\n",
      "90th Percentile: 249647.44\n",
      "95th Percentile: 11947726959.52\n",
      "99th Percentile: 294574851969.34\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Threshold: 249647.44291945396\n",
      "Model saved to advanced_nids_model_first_layer_iteration_7.h5\n",
      "Training and saving completed successfully!\n",
      "🕒 Execution Time: 1543.40 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class AdaptiveNIDS:\n",
    "    def __init__(self, input_dim, latent_dim=16, learning_rate=5e-4):\n",
    "        \"\"\"\n",
    "        Initialize Adaptive Network Intrusion Detection System with enhanced configuration\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            latent_dim (int): Dimensionality of the latent space\n",
    "            learning_rate (float): Initial learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build model components\n",
    "        self.model = self._build_improved_autoencoder()\n",
    "        \n",
    "    def _build_improved_autoencoder(self):\n",
    "        \"\"\"\n",
    "        Build an improved autoencoder with:\n",
    "        - More aggressive regularization\n",
    "        - Simplified architecture\n",
    "        - Enhanced feature extraction\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Advanced Preprocessing with Robust Scaling\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        \n",
    "        # Reshape to 2D input for 1D Convolutions\n",
    "        x = layers.Reshape((-1, 1))(x)\n",
    "        \n",
    "        # Enhanced Feature Extraction with Reduced Complexity\n",
    "        x = layers.Conv1D(\n",
    "            filters=16,  # Reduced from 32\n",
    "            kernel_size=3, \n",
    "            activation='elu',  # Changed from ReLU\n",
    "            padding='same',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.0005)  # Adjusted regularization\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Dropout(0.3)(x)  # Reduced dropout rate\n",
    "        \n",
    "        # Simplified Feature Extraction\n",
    "        x = layers.Conv1D(\n",
    "            filters=32,  # Reduced from 64\n",
    "            kernel_size=3, \n",
    "            activation='elu', \n",
    "            padding='same',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.0005)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # More Compact Latent Space Representation\n",
    "        x = layers.Dense(\n",
    "            units=64,  # Reduced complexity\n",
    "            activation='elu',\n",
    "            kernel_regularizer=keras.regularizers.l1_l2(l1=0.0005, l2=0.0005)\n",
    "        )(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Compressed Latent Representation\n",
    "        encoded = layers.Dense(\n",
    "            self.latent_dim, \n",
    "            activation='linear',  # Changed to linear\n",
    "            kernel_regularizer=keras.regularizers.l1(0.0005)\n",
    "        )(x)\n",
    "        \n",
    "        # Decoder with Simplified Architecture\n",
    "        x = layers.RepeatVector(self.input_dim)(encoded)\n",
    "        x = layers.LSTM(\n",
    "            units=self.latent_dim * 2,  \n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=0.2  # Reduced dropout\n",
    "        )(x)\n",
    "        \n",
    "        # Streamlined Reconstruction Layers\n",
    "        x = layers.TimeDistributed(\n",
    "            layers.Dense(32, activation='elu')\n",
    "        )(x)\n",
    "        \n",
    "        # Final Reconstruction Layer\n",
    "        decoded = layers.TimeDistributed(\n",
    "            layers.Dense(1, activation='linear')\n",
    "        )(x)\n",
    "        \n",
    "        # Flatten for proper shape\n",
    "        decoded = layers.Flatten()(decoded)\n",
    "        \n",
    "        # Create Autoencoder Model\n",
    "        autoencoder = keras.Model(inputs=inputs, outputs=decoded)\n",
    "        \n",
    "        # Compile with Advanced Optimization\n",
    "        autoencoder.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                learning_rate=self.learning_rate, \n",
    "                clipnorm=1.0  # Added gradient clipping\n",
    "            ),\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mae', keras.metrics.MeanSquaredError()]\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def train(self, X_train, X_val=None, epochs=75, batch_size=64):\n",
    "        \"\"\"\n",
    "        Enhanced training method with more sophisticated callbacks\n",
    "        \"\"\"\n",
    "        # Advanced Early Stopping\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=15,  # Increased patience\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.0001  # Smaller improvement threshold\n",
    "        )\n",
    "        \n",
    "        # Adaptive Learning Rate\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.3,  # More aggressive reduction\n",
    "            patience=7,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        # Training with enhanced flexibility\n",
    "        history = self.model.fit(\n",
    "            X_train, X_train,  \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def calculate_threshold(self, X_val, percentile=90):\n",
    "        \"\"\"\n",
    "        Enhanced threshold calculation with more comprehensive analysis\n",
    "        \n",
    "        Args:\n",
    "            X_val (np.array): Validation data\n",
    "            percentile (int): Percentile for threshold calculation\n",
    "        \n",
    "        Returns:\n",
    "            float: Anomaly detection threshold\n",
    "        \"\"\"\n",
    "        # Predict and calculate reconstruction errors\n",
    "        reconstructions = self.model.predict(X_val)\n",
    "        reconstruction_errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
    "        \n",
    "        # Comprehensive Error Analysis\n",
    "        print(\"\\n Reconstruction Error Analysis:\")\n",
    "        print(f\"Mean Error: {np.mean(reconstruction_errors):.2f}\")\n",
    "        print(f\"Median Error: {np.median(reconstruction_errors):.2f}\")\n",
    "        print(f\"Error Standard Deviation: {np.std(reconstruction_errors):.2f}\")\n",
    "        \n",
    "        # Percentile Analysis\n",
    "        percentiles = [50, 75, 90, 95, 99]\n",
    "        for p in percentiles:\n",
    "            print(f\"{p}th Percentile: {np.percentile(reconstruction_errors, p):.2f}\")\n",
    "        \n",
    "        # Additional Visualization\n",
    "        self.visualize_reconstruction_errors(reconstruction_errors)\n",
    "        \n",
    "        # Return threshold based on specified percentile\n",
    "        return np.percentile(reconstruction_errors, percentile)\n",
    "    \n",
    "    def visualize_reconstruction_errors(self, reconstruction_errors):\n",
    "        \"\"\"\n",
    "        Visualize reconstruction error distribution\n",
    "        \n",
    "        Args:\n",
    "            reconstruction_errors (np.array): Array of reconstruction errors\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(reconstruction_errors, bins=50, edgecolor='black')\n",
    "        plt.title('Reconstruction Error Distribution')\n",
    "        plt.xlabel('Reconstruction Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(reconstruction_errors)\n",
    "        plt.title('Reconstruction Error Boxplot')\n",
    "        plt.ylabel('Error Magnitude')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def detect_anomalies(self, X_test, threshold):\n",
    "        \"\"\"\n",
    "        Detect anomalies with enhanced error calculation\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.array): Test data\n",
    "            threshold (float): Anomaly detection threshold\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Boolean mask of anomalies\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X_test)\n",
    "        mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "        \n",
    "        # Optional: Log detailed anomaly statistics\n",
    "        print(\"\\nAnomaly Detection Summary:\")\n",
    "        print(f\"Total Samples: {len(X_test)}\")\n",
    "        anomalies = mse > threshold\n",
    "        print(f\"Detected Anomalies: {np.sum(anomalies)} ({np.mean(anomalies)*100:.2f}%)\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def save_model(self, model_path='advanced_autoencoder_nids.h5'):\n",
    "        \"\"\"\n",
    "        Save trained model with additional metadata\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def preprocess_data(file_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Enhanced data preprocessing with advanced scaling\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to dataset\n",
    "        test_size (float): Proportion of validation data\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of preprocessed training and validation datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Separate features (assuming 'Attack_label' is the target)\n",
    "        X = df.drop(['Attack_label'], axis=1)\n",
    "        \n",
    "        # Apply RobustScaler for handling outliers\n",
    "        scaler = RobustScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = train_test_split(\n",
    "            X_scaled, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Optional: Save scaler for future use\n",
    "        joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "        \n",
    "        return X_train, X_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    dataset_path = 'training_dataset.csv'\n",
    "    model_save_path = 'advanced_nids_model_first_layer_iteration_7.h5'\n",
    "    threshold_save_path = 'anomaly_threshold_iteration_7.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data\n",
    "        X_train, X_val = preprocess_data(dataset_path)\n",
    "        \n",
    "        # Print data shapes\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Initialize and train NIDS\n",
    "        nids = AdaptiveNIDS(input_dim=X_train.shape[1])\n",
    "        history = nids.train(X_train, X_val)\n",
    "        \n",
    "        # Calculate anomaly threshold\n",
    "        threshold = nids.calculate_threshold(X_val)\n",
    "        print(f\"Anomaly Threshold: {threshold}\")\n",
    "        \n",
    "        # Save model and threshold\n",
    "        nids.save_model(model_save_path)\n",
    "        joblib.dump({'threshold': threshold}, threshold_save_path)\n",
    "        \n",
    "        print(\"Training and saving completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during NIDS training: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "print(f\"🕒 Execution Time: {ex_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d4f08-1cb2-4670-96c4-35d55ef0e544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
